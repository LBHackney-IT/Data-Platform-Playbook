"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[3952],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return c}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),h=p(n),c=o,m=h["".concat(l,".").concat(c)]||h[c]||d[c]||r;return n?a.createElement(m,i(i({ref:t},u),{},{components:n})):a.createElement(m,i({ref:t},u))}));function c(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var p=2;p<r;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},6280:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return p},toc:function(){return u},default:function(){return h}});var a=n(7462),o=n(3366),r=(n(7294),n(3905)),i=["components"],s={title:"Sagemaker",description:"Investigating whether using AWS sagemaker with AWS glue development endpoints is a viable notebooking tool",tags:["tech-spike"],layout:"layout"},l="Spike: Sagemaker",p={unversionedId:"spikes/sagemaker",id:"spikes/sagemaker",isDocsHomePage:!1,title:"Sagemaker",description:"Investigating whether using AWS sagemaker with AWS glue development endpoints is a viable notebooking tool",source:"@site/docs/spikes/sagemaker.md",sourceDirName:"spikes",slug:"/spikes/sagemaker",permalink:"/Data-Platform-Playbook/spikes/sagemaker",editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/spikes/sagemaker.md",tags:[{label:"tech-spike",permalink:"/Data-Platform-Playbook/tags/tech-spike"}],version:"current",frontMatter:{title:"Sagemaker",description:"Investigating whether using AWS sagemaker with AWS glue development endpoints is a viable notebooking tool",tags:["tech-spike"],layout:"layout"},sidebar:"docs",previous:{title:"Qlik Integration",permalink:"/Data-Platform-Playbook/spikes/qlik-integration"},next:{title:"Glossary",permalink:"/Data-Platform-Playbook/glossary"}},u=[{value:"Invesitgate the use of sagemaker with glue development endpoints as a notebooking tool",id:"invesitgate-the-use-of-sagemaker-with-glue-development-endpoints-as-a-notebooking-tool",children:[]},{value:"Objective &amp; Considerations",id:"objective--considerations",children:[]},{value:"Findings",id:"findings",children:[{value:"The Setup",id:"the-setup",children:[]},{value:"Github integration",id:"github-integration",children:[]},{value:"Running spark SQL",id:"running-spark-sql",children:[]},{value:"Costing",id:"costing",children:[]},{value:"Shutting down in evenings and weekends",id:"shutting-down-in-evenings-and-weekends",children:[]}]},{value:"Helpful Resources/Documentation",id:"helpful-resourcesdocumentation",children:[]}],d={toc:u};function h(e){var t=e.components,n=(0,o.Z)(e,i);return(0,r.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"spike-sagemaker"},"Spike: Sagemaker"),(0,r.kt)("h3",{id:"invesitgate-the-use-of-sagemaker-with-glue-development-endpoints-as-a-notebooking-tool"},"Invesitgate the use of sagemaker with glue development endpoints as a notebooking tool"),(0,r.kt)("h2",{id:"objective--considerations"},"Objective & Considerations"),(0,r.kt)("p",null,"Our current notebooking solution is a docker container that closes emulates the environment that a glue job run in that can be run locally.\nWe are looking to replace this solution due to the following issues:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"The image we are using in unstable on the new Mac M1 laptops "),(0,r.kt)("li",{parentName:"ol"},"It requires a lot of installation which is too high a barrier of entry for some users, e.g. cloning the repository, installing docker desktop, using the terminal "),(0,r.kt)("li",{parentName:"ol"},"Users who have chromebooks can't use it."),(0,r.kt)("li",{parentName:"ol"},"It doesn't have access to the glue datacatalog, preinstalled helpers module that we use in a lot of glue jobs.\nThis means that after prototyping in the notebook there are quite a few changes to the script that need to be made to test it as a glue job.")),(0,r.kt)("p",null,"We have the following user stories for what we would like from a new notebooking solution."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"As a BI Analyst I want to develop a SQL script in the same syntax it will run in so that I can reduce development time and time to insight."),(0,r.kt)("li",{parentName:"ol"},"As a Python Analyst I want to develop a Python script to transform data in an environment where I can easily debug the script so that I can reduce development time and time to insight"),(0,r.kt)("li",{parentName:"ol"},"As a Python Analyst/BI Analyst I want a development environment that doesn\u2019t require me to install something and works on my Chrome Device so that I can develop SQL/Python scripts")),(0,r.kt)("p",null,"An additional consideration not mentioned in these user stories is the cost of the tool."),(0,r.kt)("h2",{id:"findings"},"Findings"),(0,r.kt)("h3",{id:"the-setup"},"The Setup"),(0,r.kt)("p",null,"AWS sagemaker provides is a managed notebook instance, we can provide startup scripts that run when the instance is created and everytime it is started.\nIn this startup script (",(0,r.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/blob/sagemaker/modules/sagemaker/scripts/notebook-start-up.sh"},"example"),") we can install a spark magic and setup the connection to a glue development endpoint so that when we run scripts in the pyspark kernel they will run against the development endpoint.\nThis gives access to the full glue catalogue & runs in the same environment that glue jobs run in."),(0,r.kt)("p",null,"When creating the glue devleopment endpoint you provide an IAM role, which controls the access to any data.\nHence we would need a separate endpoint for each department that needs one, to control permissions."),(0,r.kt)("p",null,"You can also provide the develpoment endpoint with any extra python libraries and jars that are stored in S3, similar to a glue job.\nThis means we can access the helpers module & data quality testing in the same we that we do in glue jobs."),(0,r.kt)("h3",{id:"github-integration"},"Github integration"),(0,r.kt)("p",null,"Each notebook instance can be linked to a GitHub repository (",(0,r.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform-Notebooks"},"example"),").\nThe repository will appear as a folder in jupyterlab and users can use the UI to push to GitHub, as long as they have been added to the repository.\nAs two factor authentication in enforced in the Hackney Github, users who wish to push changes will have to generate a PAT(Personal Access Token) code to use as a password."),(0,r.kt)("h3",{id:"running-spark-sql"},"Running spark SQL"),(0,r.kt)("p",null,'Spark SQL can be run with after the data has first been loaded using PySpark.\nIn this following code block the mtfh_tenureinformation table, which exists in the glue catalog, is being "loaded" for use in Spark SQL as the table "tenures".'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from awsglue.context import GlueContext\n\nglue_context = GlueContext(spark)\n\ndf = glue_context.create_data_frame.from_catalog( \n    database="dataplatform-emmacorbett-landing-zone-database", \n    table_name="mtfh_tenureinformation", \n    transformation_ctx="tenures_source")\n\ndf.createOrReplaceTempView("tenures")\n')),(0,r.kt)("p",null,"After preparing any tables needed for the SQL query there are two ways to run spark SQL, using the spark magic or written in pyspark."),(0,r.kt)("h4",{id:"using-sparkmagic"},"Using sparkmagic"),(0,r.kt)("p",null,"This gets the first 10 rows in the tenures table and save it to a pandas dataframe called tenures."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"%%sql -o tenures -q\nSELECT * FROM tenures limit 10\n")),(0,r.kt)("p",null,"This will print out the first 10 rows of the tenures pandas dataframe."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"%%local\ntenures\n")),(0,r.kt)("h4",{id:"using-pyspark-functions"},"Using PySpark functions"),(0,r.kt)("p",null,"This gets the first 10 rows in the tenures table and saves it to a dataframe."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'tenuresDF = spark.sql("""\nSELECT * FROM tenures limit 10\n""")\n')),(0,r.kt)("p",null,"This will print out the first 10 rows of the dataframe."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"sqlDF.show()\n")),(0,r.kt)("h3",{id:"costing"},"Costing"),(0,r.kt)("p",null,"I've worked out a few scenarios for the cost of running both development endpoints & notebook instances for 4 weeks."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Scenario"),(0,r.kt)("th",{parentName:"tr",align:null},"Running continuously"),(0,r.kt)("th",{parentName:"tr",align:null},"Running 10 hrs/day, 5 days/week"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2 nbs @ 2dpu each"),(0,r.kt)("td",{parentName:"tr",align:null},"$1259.328"),(0,r.kt)("td",{parentName:"tr",align:null},"$374.8")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2 nbs @ 4dpu each"),(0,r.kt)("td",{parentName:"tr",align:null},"$2442.04"),(0,r.kt)("td",{parentName:"tr",align:null},"$726.8")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1 nb @ 2dpu"),(0,r.kt)("td",{parentName:"tr",align:null},"$629.664"),(0,r.kt)("td",{parentName:"tr",align:null},"$187.4")))),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"nbs"),": Notebook Instance (inc development endpoint)"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"dpu"),": data processing unit. Pricing for running glue, it depended on worker type and number of workers.\nThe worker types are:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Standard (2dpu/hour): 16 GB memory, 4 vCPUs of compute capacity, and 50 GB of attached EBS storage (2 Executors / worker)"),(0,r.kt)("li",{parentName:"ul"},"G.1X (4dpu/hour): 16 GB memory, 4 vCPUs, and 64 GB of attached EBS storage (1 Executor / worker)"),(0,r.kt)("li",{parentName:"ul"},"G.2X (8dpu/hour): 32 GB memory, 8vCPUs, 128 GB of EBS (1 Executor / worker)")),(0,r.kt)("h3",{id:"shutting-down-in-evenings-and-weekends"},"Shutting down in evenings and weekends"),(0,r.kt)("p",null,"Due to the cost of keeping a development endpoint running continuously I would recommend deleting the endpoints at the end of the working day and recreating it at the beginning.\nTo achieve this we could partially managed these resources in terraform but the creation and deletion of the dev endpoint would have to managed by a scheduled task."),(0,r.kt)("h4",{id:"managed-in-terraform-a-module-that-can-be-used-for-each-notebook-instance"},"Managed in terraform, a module that can be used for each notebook instance"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"A notebook instance (",(0,r.kt)("a",{parentName:"li",href:"https://github.com/LBHackney-IT/Data-Platform/blob/sagemaker/modules/sagemaker/11-notebook.tf"},"example"),")"),(0,r.kt)("li",{parentName:"ol"},"A private/ public key pair. With the private key saved to SSM."),(0,r.kt)("li",{parentName:"ol"},"An SSM parameter with a JSON object holding the configuration for the development endpoint. For example,")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "endpoint_name": "sagemaker-development-endpoint-parking",\n    "extra_python_libs_s3_path": "s3://dataplatform-glue-scripts/python-modules/data_platform_glue_job_helpers-1.0-py3-none-any.whl,s3://dataplatform-glue-scripts/python-modules/pydeequ-1.0.1.zip",\n    "extra_jars_s3_path": "s3://dataplatform-glue-scripts/jars/java-lib-1.0-SNAPSHOT-jar-with-dependencies.jar",\n    "public_keys": ["ssh-rsa AAAAAAHHHHSBDFJGWHIEWGHI example@hackney.gov.uk"],\n    "worker_type": "Standard",\n    "number_of_workers": "2",\n    "role_arn": "arn:aws:iam::482356789234:role/glue-role-parking"\n}\n')),(0,r.kt)("h4",{id:"a-task-to-run-just-before-the-beginning-of-the-working-day-to-create-development-endpoints"},"A task to run just before the beginning of the working day to create development endpoints"),(0,r.kt)("p",null,"This task would first find all the SSM parameters relating to glue development endpoint configurations.\nThen it would create all of these development endpoints as per the configuration specified."),(0,r.kt)("h4",{id:"a-task-to-run-at-the-end-of-the-working-day"},"A task to run at the end of the working day"),(0,r.kt)("p",null,"This task would both stop all notebook instances that are still running and then delete all development endpoints in the account."),(0,r.kt)("p",null,"Note: If following this approach then the startup script for the notebok instamces would need to altered so that it found and connected to the development endpoint everytime it starts.\nCurrently it caches whether it has previous checked the connection and then doesn't run again."),(0,r.kt)("h2",{id:"helpful-resourcesdocumentation"},"Helpful Resources/Documentation"),(0,r.kt)("p",null,"The terraform module for the aforementioned notebook instance is on ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/tree/sagemaker/modules/sagemaker"},"this branch"),"."))}h.isMDXComponent=!0}}]);