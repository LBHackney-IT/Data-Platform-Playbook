"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[3088],{8206:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var s=n(4848),i=n(8453);const r={id:"using-pytest-for-verifying-pyspark-transformations",title:"Using pytest for verifying PySpark transformations - ADR 010",description:"",tags:["adr"],number:"10","date-issued":"2021-08-25",status:"Accepted"},a=void 0,o={id:"architecture-decisions/records/using-pytest-for-verifying-pyspark-transformations",title:"Using pytest for verifying PySpark transformations - ADR 010",description:"",source:"@site/docs/architecture-decisions/records/010-using-pytest-for-verifying-pyspark-transformations.md",sourceDirName:"architecture-decisions/records",slug:"/architecture-decisions/records/using-pytest-for-verifying-pyspark-transformations",permalink:"/Data-Platform-Playbook/architecture-decisions/records/using-pytest-for-verifying-pyspark-transformations",draft:!1,unlisted:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/architecture-decisions/records/010-using-pytest-for-verifying-pyspark-transformations.md",tags:[{inline:!0,label:"adr",permalink:"/Data-Platform-Playbook/tags/adr"}],version:"current",sidebarPosition:10,frontMatter:{id:"using-pytest-for-verifying-pyspark-transformations",title:"Using pytest for verifying PySpark transformations - ADR 010",description:"",tags:["adr"],number:"10","date-issued":"2021-08-25",status:"Accepted"},sidebar:"docs",previous:{title:"Ingesting data from APIs - ADR 009",permalink:"/Data-Platform-Playbook/architecture-decisions/records/ingesting-data-from-apis"},next:{title:"Using DataHub as a Data Catalogue - ADR 011",permalink:"/Data-Platform-Playbook/architecture-decisions/records/using-datahub-as-a-data-catalogue"}},c={},d=[{value:"Context",id:"context",level:2},{value:"Decision",id:"decision",level:2},{value:"Consequences",id:"consequences",level:2}];function l(e){const t={a:"a",h2:"h2",li:"li",p:"p",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h2,{id:"context",children:"Context"}),"\n",(0,s.jsx)(t.p,{children:"The Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform."}),"\n",(0,s.jsx)(t.p,{children:"Examples include:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Address matching"}),"\n",(0,s.jsx)(t.li,{children:"Address cleaning"}),"\n",(0,s.jsx)(t.li,{children:"Repairs sheets data cleaning"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"These jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform."}),"\n",(0,s.jsx)(t.p,{children:"By introducing testing practices, frameworks and tools we hope to:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Improve the speed at which PySpark scripts can be developed"}),"\n",(0,s.jsx)(t.li,{children:"Provide documentation for each script with example data they expect, and what results they output"}),"\n",(0,s.jsx)(t.li,{children:"Increase the proportion of defects found before they reach staging environment"}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"decision",children:"Decision"}),"\n",(0,s.jsx)(t.p,{children:"We will:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Use a Python testing framework, ",(0,s.jsx)(t.a,{href:"https://docs.pytest.org",children:"pytest"})]}),"\n",(0,s.jsxs)(t.li,{children:["Use the same ",(0,s.jsx)(t.a,{href:"https://hub.docker.com/r/amazon/aws-glue-libs",children:"Docker container"})," we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally."]}),"\n",(0,s.jsx)(t.li,{children:"Integrate that framework into Apache Spark, and provide example test code"}),"\n",(0,s.jsx)(t.li,{children:"Create documentation and guidance around how to productively test PySpark scripts"}),"\n",(0,s.jsx)(t.li,{children:"Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging"}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"consequences",children:"Consequences"}),"\n",(0,s.jsx)(t.p,{children:"Building and maintaining PySpark scripts should become easier and faster as a result."}),"\n",(0,s.jsx)(t.p,{children:"Writing PySpark scripts will require some additional learning, if you haven't used unit testing practices before."})]})}function p(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var s=n(6540);const i={},r=s.createContext(i);function a(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(r.Provider,{value:t},e.children)}}}]);