"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[7853],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return u}});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var p=r.createContext({}),s=function(e){var t=r.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=s(e.components);return r.createElement(p.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,p=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=s(n),u=o,f=m["".concat(p,".").concat(u)]||m[u]||d[u]||a;return n?r.createElement(f,i(i({ref:t},c),{},{components:n})):r.createElement(f,i({ref:t},c))}));function u(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=m;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l.mdxType="string"==typeof e?e:o,i[1]=l;for(var s=2;s<a;s++)i[s]=n[s];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3654:function(e,t,n){n.r(t),n.d(t,{assets:function(){return c},contentTitle:function(){return p},default:function(){return u},frontMatter:function(){return l},metadata:function(){return s},toc:function(){return d}});var r=n(3117),o=n(102),a=(n(7294),n(3905)),i=["components"],l={title:"CD Process",description:"Explaination of the CD process for the Data Platform",tags:["playbook"],layout:"layout"},p=void 0,s={unversionedId:"docs/CD-process",id:"docs/CD-process",title:"CD Process",description:"Explaination of the CD process for the Data Platform",source:"@site/docs/docs/CD-process.md",sourceDirName:"docs",slug:"/docs/CD-process",permalink:"/Data-Platform-Playbook/docs/CD-process",draft:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/docs/CD-process.md",tags:[{label:"playbook",permalink:"/Data-Platform-Playbook/tags/playbook"}],version:"current",frontMatter:{title:"CD Process",description:"Explaination of the CD process for the Data Platform",tags:["playbook"],layout:"layout"},sidebar:"docs",previous:{title:"Module 2 - Developing a simple dashboard",permalink:"/Data-Platform-Playbook/training-modules/Qlik/qlik-module-2"},next:{title:"CI Process",permalink:"/Data-Platform-Playbook/docs/CI-process"}},c={},d=[{value:"Current Process",id:"current-process",level:3},{value:"Staging Deployment",id:"staging-deployment",level:4},{value:"Production Deployment",id:"production-deployment",level:4},{value:"To Be Process",id:"to-be-process",level:3},{value:"Staging and Production Deployment",id:"staging-and-production-deployment",level:4}],m={toc:d};function u(e){var t=e.components,l=(0,o.Z)(e,i);return(0,a.kt)("wrapper",(0,r.Z)({},m,l,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"CD Process",src:n(1944).Z,width:"781",height:"1191"})),(0,a.kt)("h3",{id:"current-process"},"Current Process"),(0,a.kt)("h4",{id:"staging-deployment"},"Staging Deployment"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"The staging deployment pipeline is triggered on merge of a pull request or commit of code to the ",(0,a.kt)("inlineCode",{parentName:"li"},"main")," code branch of the Data Platform repository"),(0,a.kt)("li",{parentName:"ol"},"Terraform and python scripts are then tested in parallel. For Python the unit tests are executed and for Terraform the source code is linted and then validated using ",(0,a.kt)("inlineCode",{parentName:"li"},"tf lint")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform validate")," commands"),(0,a.kt)("li",{parentName:"ol"},"If the previous steps are successful then a ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform plan")," followed by a ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform apply")," is executed against the environment")),(0,a.kt)("h4",{id:"production-deployment"},"Production Deployment"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"The production deployment pipeline is triggered on manual creation of a release in the Data Platform repository"),(0,a.kt)("li",{parentName:"ol"},"Terraform and python scripts are then tested in parallel. For Python the unit tests are executed and for Terraform the source code is linted and then validated using ",(0,a.kt)("inlineCode",{parentName:"li"},"tf lint")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform validate")," commands"),(0,a.kt)("li",{parentName:"ol"},"If the previous steps are successful then a ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform plan")," followed by a ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform apply")," is executed against the environment")),(0,a.kt)("h3",{id:"to-be-process"},"To Be Process"),(0,a.kt)("h4",{id:"staging-and-production-deployment"},"Staging and Production Deployment"),(0,a.kt)("p",null,"In the near future there will be only 1 pipeline for each area of terraform that completes the staging and production deployments together instead of 2 different pipelines. The below steps document the intended process"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Pipeline is triggered on merge of a pull request or commit of code to the ",(0,a.kt)("inlineCode",{parentName:"li"},"main")," code branch of the Data Platform repository"),(0,a.kt)("li",{parentName:"ol"},"Terraform and python scripts are then tested in parallel. For Python the unit tests are executed and for Terraform the source code is linted and then validated using ",(0,a.kt)("inlineCode",{parentName:"li"},"tf lint")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform validate")," commands "),(0,a.kt)("li",{parentName:"ol"},"If the previous steps are successful then a ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform plan")," followed by Terraform compliance checks and then finally a ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform apply")," is executed against the Staging environment"),(0,a.kt)("li",{parentName:"ol"},"The pipeline then pauses for a manual approval."),(0,a.kt)("li",{parentName:"ol"},"If the manual approval step is approved then a ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform plan")," followed by Terraform compliance checks and then finally a ",(0,a.kt)("inlineCode",{parentName:"li"},"terraform apply")," is executed against the Production environment")))}u.isMDXComponent=!0},1944:function(e,t,n){t.Z=n.p+"assets/images/CD-cbc2a861c072d69ee2b5448a6e48a44e.png"}}]);