"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[6859],{3905:(e,t,r)=>{r.d(t,{Zo:()=>p,kt:()=>m});var n=r(7294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},i=Object.keys(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var l=n.createContext({}),c=function(e){var t=n.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},f=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(r),f=a,m=u["".concat(l,".").concat(f)]||u[f]||d[f]||i;return r?n.createElement(m,o(o({ref:t},p),{},{components:r})):n.createElement(m,o({ref:t},p))}));function m(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=r.length,o=new Array(i);o[0]=f;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:a,o[1]=s;for(var c=2;c<i;c++)o[c]=r[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,r)}f.displayName="MDXCreateElement"},1048:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var n=r(7462),a=(r(7294),r(3905));const i={id:"using-pytest-for-verifying-pyspark-transformations",title:"Using pytest for verifying PySpark transformations - ADR 010",description:"",tags:["adr"],number:"10","date-issued":"2021-08-25",status:"Accepted"},o=void 0,s={unversionedId:"architecture-decisions/records/using-pytest-for-verifying-pyspark-transformations",id:"architecture-decisions/records/using-pytest-for-verifying-pyspark-transformations",title:"Using pytest for verifying PySpark transformations - ADR 010",description:"",source:"@site/docs/architecture-decisions/records/010-using-pytest-for-verifying-pyspark-transformations.md",sourceDirName:"architecture-decisions/records",slug:"/architecture-decisions/records/using-pytest-for-verifying-pyspark-transformations",permalink:"/Data-Platform-Playbook/architecture-decisions/records/using-pytest-for-verifying-pyspark-transformations",draft:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/architecture-decisions/records/010-using-pytest-for-verifying-pyspark-transformations.md",tags:[{label:"adr",permalink:"/Data-Platform-Playbook/tags/adr"}],version:"current",sidebarPosition:10,frontMatter:{id:"using-pytest-for-verifying-pyspark-transformations",title:"Using pytest for verifying PySpark transformations - ADR 010",description:"",tags:["adr"],number:"10","date-issued":"2021-08-25",status:"Accepted"},sidebar:"docs",previous:{title:"Ingesting data from APIs - ADR 009",permalink:"/Data-Platform-Playbook/architecture-decisions/records/ingesting-data-from-apis"},next:{title:"Using DataHub as a Data Catalogue - ADR 011",permalink:"/Data-Platform-Playbook/architecture-decisions/records/using-datahub-as-a-data-catalogue"}},l={},c=[{value:"Context",id:"context",level:2},{value:"Decision",id:"decision",level:2},{value:"Consequences",id:"consequences",level:2}],p={toc:c},u="wrapper";function d(e){let{components:t,...r}=e;return(0,a.kt)(u,(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"context"},"Context"),(0,a.kt)("p",null,"The Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform."),(0,a.kt)("p",null,"Examples include:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Address matching"),(0,a.kt)("li",{parentName:"ul"},"Address cleaning"),(0,a.kt)("li",{parentName:"ul"},"Repairs sheets data cleaning")),(0,a.kt)("p",null,"These jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform."),(0,a.kt)("p",null,"By introducing testing practices, frameworks and tools we hope to:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Improve the speed at which PySpark scripts can be developed"),(0,a.kt)("li",{parentName:"ul"},"Provide documentation for each script with example data they expect, and what results they output"),(0,a.kt)("li",{parentName:"ul"},"Increase the proportion of defects found before they reach staging environment")),(0,a.kt)("h2",{id:"decision"},"Decision"),(0,a.kt)("p",null,"We will:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Use a Python testing framework, ",(0,a.kt)("a",{parentName:"li",href:"https://docs.pytest.org"},"pytest")),(0,a.kt)("li",{parentName:"ul"},"Use the same ",(0,a.kt)("a",{parentName:"li",href:"https://hub.docker.com/r/amazon/aws-glue-libs"},"Docker container")," we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally."),(0,a.kt)("li",{parentName:"ul"},"Integrate that framework into Apache Spark, and provide example test code"),(0,a.kt)("li",{parentName:"ul"},"Create documentation and guidance around how to productively test PySpark scripts"),(0,a.kt)("li",{parentName:"ul"},"Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging")),(0,a.kt)("h2",{id:"consequences"},"Consequences"),(0,a.kt)("p",null,"Building and maintaining PySpark scripts should become easier and faster as a result."),(0,a.kt)("p",null,"Writing PySpark scripts will require some additional learning, if you haven't used unit testing practices before."))}d.isMDXComponent=!0}}]);