"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[5127],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return h}});var o=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,o,r=function(e,t){if(null==e)return{};var a,o,r={},n=Object.keys(e);for(o=0;o<n.length;o++)a=n[o],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(o=0;o<n.length;o++)a=n[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var u=o.createContext({}),s=function(e){var t=o.useContext(u),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},p=function(e){var t=s(e.components);return o.createElement(u.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},m=o.forwardRef((function(e,t){var a=e.components,r=e.mdxType,n=e.originalType,u=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),m=s(a),h=r,c=m["".concat(u,".").concat(h)]||m[h]||d[h]||n;return a?o.createElement(c,l(l({ref:t},p),{},{components:a})):o.createElement(c,l({ref:t},p))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var n=a.length,l=new Array(n);l[0]=m;var i={};for(var u in t)hasOwnProperty.call(t,u)&&(i[u]=t[u]);i.originalType=e,i.mdxType="string"==typeof e?e:r,l[1]=i;for(var s=2;s<n;s++)l[s]=a[s];return o.createElement.apply(null,l)}return o.createElement.apply(null,a)}m.displayName="MDXCreateElement"},8040:function(e,t,a){a.r(t),a.d(t,{assets:function(){return p},contentTitle:function(){return u},default:function(){return h},frontMatter:function(){return i},metadata:function(){return s},toc:function(){return d}});var o=a(3117),r=a(102),n=(a(7294),a(3905)),l=["components"],i={},u=void 0,s={unversionedId:"training-modules/module-3",id:"training-modules/module-3",title:"module-3",description:"---",source:"@site/docs/training-modules/module-3.md",sourceDirName:"training-modules",slug:"/training-modules/module-3",permalink:"/Data-Platform-Playbook/training-modules/module-3",draft:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/training-modules/module-3.md",tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Module 2 - Transforming data to refined zone",permalink:"/Data-Platform-Playbook/training-modules/module-2"},next:{title:"Module 1 - Introduction to Qlik",permalink:"/Data-Platform-Playbook/training-modules/Qlik/qlik-module-1"}},p={},d=[{value:"tags: training",id:"tags-training",level:2},{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Part 1. Prototyping the job in the AWS glue console",id:"part-1-prototyping-the-job-in-the-aws-glue-console",level:2},{value:"Detailed steps for part 1:",id:"detailed-steps-for-part-1",level:3},{value:"2. Part 2: Deploying your job via Terraform",id:"2-part-2-deploying-your-job-via-terraform",level:2},{value:"Overview of the steps for part 2:",id:"overview-of-the-steps-for-part-2",level:3},{value:"Detailed steps for part 2:",id:"detailed-steps-for-part-2",level:3}],m={toc:d};function h(e){var t=e.components,i=(0,r.Z)(e,l);return(0,n.kt)("wrapper",(0,o.Z)({},m,i,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("hr",null),(0,n.kt)("p",null,'title: Module 3 - Deploying a job in Glue\ndescription: "Training Module 3"\nlayout: playbook_js'),(0,n.kt)("h2",{id:"tags-training"},"tags: ","[training]"),(0,n.kt)("h1",{id:"module-3---deploying-a-job-in-glue"},"Module 3 - Deploying a job in Glue"),(0,n.kt)("p",null,"In this module, you will turn the transformation script you created in ",(0,n.kt)("a",{parentName:"p",href:"/Data-Platform-Playbook/training-modules/module-2"},"module 2")," into a Glue job and \u2018productionize\u2019 it. At the end of the module, you will have a Glue job (and Crawler) that is managed by the Data Platform infrastructure and code repository which you can use to carry out your tasks."),(0,n.kt)("h2",{id:"overview"},"Overview"),(0,n.kt)("p",null,"First, you will be carrying over your transformation script from Sagemaker into the Glue console, where it will become a Glue job. You'll test this job and check your data like you did in Module 1. "),(0,n.kt)("p",null,"Then, you will deploy the same job following the Data Platform standard process. This involves two main changes:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Your Glue job will be coded in Terraform. When deployed, Terraform will generate all the necessary infrastructure in the AWS environment (job, crawler, scheduler). This will replace the elements you\u2019ve created manually in the AWS console in the second part of Module 2. This is \u2018Infrastructure as Code\u2019 (IaC) and brings a lot of advantages to the Data Platform:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Quality assurance: we use Terraform templates so all elements of infrastructure follow the same standards"),(0,n.kt)("li",{parentName:"ul"},"Repeatability: your job can be recreated by a simple redeployment if anything goes wrong. It can also be replicated to another environment, for instance from Staging to Production"))),(0,n.kt)("li",{parentName:"ul"},"Both the PySpark script and the job details in Terraform will become part of the Data Platform code base and saved in Github. This also brings advantages:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Safety: Your code cannot be accidentally lost and is version-controlled"),(0,n.kt)("li",{parentName:"ul"},"Quality assurance: Your code will be reviewed by at least 2 engineers. Github will also perform automated checks which must pass before merging it into the repository")))),(0,n.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,n.kt)("p",null,"Please ensure that both ",(0,n.kt)("a",{parentName:"p",href:"/Data-Platform-Playbook/training-modules/module-0"},"Module 0"),", ",(0,n.kt)("a",{parentName:"p",href:"/Data-Platform-Playbook/training-modules/module-1"},"Module 1")," and ",(0,n.kt)("a",{parentName:"p",href:"/Data-Platform-Playbook/training-modules/module-2"},"Module 2")," are completed before proceeding."),(0,n.kt)("h2",{id:"part-1-prototyping-the-job-in-the-aws-glue-console"},"Part 1. Prototyping the job in the AWS glue console"),(0,n.kt)("p",null,"You\u2019ll take the following steps before productionising the script you wrote in Module 2, to check it runs smoothly as a job in the AWS glue environment (it should, because the notebook environment you used in Sagemaker runs against glue).\nIt will be an opportunity for you to try logging.\nAs we\u2019re just testing, we won\u2019t write any Terraform (we'll do this in the second half of this module) and we won\u2019t schedule the job.\nWe\u2019ll also delete our job at the end.\nIf you need more detailed instructions at any point checkout ",(0,n.kt)("a",{parentName:"p",href:"/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/using-glue-studio"},"the guide to set up an ETL job"),"."),(0,n.kt)("h3",{id:"detailed-steps-for-part-1"},"Detailed steps for part 1:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Login to AWS pre-production acccount via the ",(0,n.kt)("a",{parentName:"p",href:"https://hackney.awsapps.com/start#/"},"Hackney SSO"),".")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Set up a new job in Glue Studio. You can start from a template."),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Go to ",(0,n.kt)("a",{parentName:"li",href:"https://eu-west-2.console.aws.amazon.com/gluestudio/home?region=eu-west-2#/jobs"},"AWS Glue Studio"),' and open the Template job, called "stg job_template".'),(0,n.kt)("li",{parentName:"ul"},'Clone the job (from Actions) and rename it with your name and remove "stg" from the prefix, for example "jane doe template".'),(0,n.kt)("li",{parentName:"ul"},"Open the job and familiarise yourself with the steps (reading, transforming and writing data) and note the differences compared to the Notebook template.")),(0,n.kt)("p",{parentName:"li"},(0,n.kt)("img",{alt:"Cloning a glue job",src:a(9408).Z,width:"512",height:"223"}))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Write your job."),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"In a different tab, open Sagemaker and navigate to the notebook you created in Module 2.")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"Paste your code from the notebook into the template, ignoring the first cell (Cells can be merged in Jupyter by holding ",(0,n.kt)("inlineCode",{parentName:"p"},"Shift")," and selecting the cells, then pressing ",(0,n.kt)("inlineCode",{parentName:"p"},"Shift+M")," on your keyboard).")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"For the variables defined in the first cell of your notebook, you will use the ",(0,n.kt)("inlineCode",{parentName:"p"},"Job details")," tab instead of writing them in the script.\nYou should define them in the ",(0,n.kt)("strong",{parentName:"p"},"Job parameters")," panel which can be found under ",(0,n.kt)("inlineCode",{parentName:"p"},"Advanced properties"),".\nYou should also update the S3 path in the ",(0,n.kt)("strong",{parentName:"p"},"Script path")," field to ",(0,n.kt)("inlineCode",{parentName:"p"},"s3://dataplatform-stg-glue-scripts/custom/"),". "))),(0,n.kt)("p",{parentName:"li"},(0,n.kt)("img",{alt:"job parameters",src:a(9493).Z,width:"512",height:"342"}))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Run your job.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Review the run result logs.\nSee the ",(0,n.kt)("a",{parentName:"p",href:"/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/using-glue-studio#monitoring-a-glue-job-run"},"monitoring section")," of the using Glue Studio guide for an explanation on how to do this.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Once your job status has changed from ",(0,n.kt)("inlineCode",{parentName:"p"},"Running")," to ",(0,n.kt)("inlineCode",{parentName:"p"},"Succeeded")," you should check the data wrote correctly into ",(0,n.kt)("a",{parentName:"p",href:"https://s3.console.aws.amazon.com/s3/home?region=eu-west-2"},"S3"),".\nTo do this, follow the same steps in the ",(0,n.kt)("a",{parentName:"p",href:"/Data-Platform-Playbook/training-modules/module-1#6-crawling-the-ingested-data-to-make-it-available-in-the-glue-catalogue"},"last section of Module 1"),".")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://eu-west-2.console.aws.amazon.com/glue/home?region=eu-west-2#catalog:tab=crawlers"},"Crawl")," the results. (Using the crawler named \u201csandbox-refined-zone\u201d).")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the data in ",(0,n.kt)("a",{parentName:"p",href:"https://eu-west-2.console.aws.amazon.com/athena/home?region=eu-west-2#/query-editor/"},"Athena")," - (",(0,n.kt)("a",{parentName:"p",href:"/Data-Platform-Playbook/playbook/querying-and-analysing-data/querying-data-using-sql"},"playbook"),")."))),(0,n.kt)("p",null,"Congratulations, you've tested that your transformation is working in Glue. You can move to the next part of Module 3."),(0,n.kt)("h2",{id:"2-part-2-deploying-your-job-via-terraform"},"2. Part 2: Deploying your job via Terraform"),(0,n.kt)("p",null,"In this second part of Module 3, you will \u2018productionize\u2019 the job you\u2019ve tested in the previous part. To do this, you will write a Terraform module in the Data Platform GitHub repository and you'll deploy it. The entirety of this module will be carried out in the GitHub User Interface, but you could also work in your IDE if you prefer. "),(0,n.kt)("h3",{id:"overview-of-the-steps-for-part-2"},"Overview of the steps for part 2:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"First, you\u2019ll delete the data you've created in part 1 as they will get created again in this part."),(0,n.kt)("li",{parentName:"ul"},"Then, you will save your PySpark script at the relevant location of the DP repository. You already have the code ready.  "),(0,n.kt)("li",{parentName:"ul"},"Then, you will write a Module in Terraform that encapsulates all the resources your Glue job requires."),(0,n.kt)("li",{parentName:"ul"},"You will then create a Pull Request and merge your code with the Pre-Production DP repository"),(0,n.kt)("li",{parentName:"ul"},"At this point, Github will attempt to deploy your code, and your job will get created in AWS. "),(0,n.kt)("li",{parentName:"ul"},"You\u2019ll be able to retrieve your Glue job in the AWS console, to run it, and check the results in Athena. ")),(0,n.kt)("h3",{id:"detailed-steps-for-part-2"},"Detailed steps for part 2:"),(0,n.kt)("ol",{start:9},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Delete the data generated in Part 1 of this module\nThis is to avoid getting confused at the end of this Module, as you will be running the newly deployed job, generating the same data again. To delete your data, navigate to it in S3 (Refined zone bucket, Sandbox department) and permanently delete the 2 full folders with your name (there should be one folder for Locations and one for Vaccinations). ",(0,n.kt)("em",{parentName:"p"},"Do NOT")," delete your data from the Raw zone, otherwise you would have to ingest it again by running the ingestion job created in Module 1!")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Create your PySpark script in the repository\nThis step can be done in Github in the browser, or in your IDE (in this case make sure you have pulled the last version of the DP repository).\nFollow the steps from the ",(0,n.kt)("a",{parentName:"p",href:"https://lbhackney-it.github.io/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/deploy-glue-jobs/#1-add-your-script-to-the-data-platform-project-using-the-github-ui"},"Using GitHub Playbook article"),", part 1.\nAdd your name at the end of the file name. An name example is ",(0,n.kt)("em",{parentName:"p"},"covid_vaccinations_arda.py"),".\nWhen you validate your new file in GitHub, do not create a Pull Request straight away. Look at the name of the new branch you are creating (you can aben customise it). You'll use the same branch in the next step and raise one PR at the end.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Define your job in a Terraform module\nA module is a block of Terraform script that creates a set of related resources in AWS. The DP team has created a module template for Glue jobs. An ","[example from the Playbook][job module in playbook]"," is reproduced below. As you can see, it includes:"))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"The job name and department"),(0,n.kt)("li",{parentName:"ul"},"The location of the pySpark script"),(0,n.kt)("li",{parentName:"ul"},"The schedule of the job"),(0,n.kt)("li",{parentName:"ul"},"The job parameters (everything you defined manually in \u2018job details\u2019 in the Glue console)"),(0,n.kt)("li",{parentName:"ul"},"A crawler to crawl the results of your job as soon as it has completed")),(0,n.kt)("p",null,"You\u2019ll create your module in the existing file: ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/terraform/25-aws-glue-job-sandbox.tf"},"terraform/25-aws-glue-job-sandbox.tf"),". This link takes you to the main branch of the repository, but you should make sure you are in the branch you've created in the step below."),(0,n.kt)("p",null,"To create your module, follow the steps from the ","[Playbook, Part 2][job module in playbook]",".\nPlease ",(0,n.kt)("em",{parentName:"p"},"add your name")," at the end of the module name and the job name.\nPlease ",(0,n.kt)("em",{parentName:"p"},"DON\u2019T")," add a schedule to your module, you\u2019ll just run it manually later.\n",(0,n.kt)("em",{parentName:"p"},"TIP"),": the example above doesn\u2019t contain all the mandatory fields\u2026 make sure you read the Playbook carefully so you don\u2019t forget any! If you do, GitHub won't let you commit your work."),(0,n.kt)("ol",{start:12},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Have your code reviewed\nCommit your code to your current branch. Then, open a Pull Request. It contains your new script and your new module.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Deploy to the Data Platform\nWhen you have received 2 approvals from reviewers, go back to GitHub. You will now be able to merge your branch (the merge button is green). "))),(0,n.kt)("p",null,"The automated deployment will now start and take several minutes.\nTo check how it is going, you can navigate to the ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/actions"},"Actions tab")," and monitor the progress of your code deployment.\nA reason for deploy failure is when a resource declared in Terraform (for instance a pySpark script file) is missing from the AWS environment. If this is happening, make sure all the necessary resources have been deployed before re-trying to deploy yours."),(0,n.kt)("ol",{start:14},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check your new resources in the AWS console.\nIn the ",(0,n.kt)("a",{parentName:"p",href:"https://eu-west-2.console.aws.amazon.com/gluestudio/home?region=eu-west-2#/jobs"},"Glue Studio page"),", you should find your Glue job with the prefix \u201cstg-\u201d added. In the ",(0,n.kt)("a",{parentName:"p",href:"https://eu-west-2.console.aws.amazon.com/glue/home?region=eu-west-2#catalog:tab=crawlers"},"Glue Crawlers")," page, you should find the associated crawler. You can easily link the details of these new resources to the statements you wrote in the Terraform module.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Run the deployed job\nYou can run your job manually from Glue Studio. After it has completed, the job will trigger the crawler so you don\u2019t need to run it yourself. If you navigate to the Crawler page, you should see your crawler in the \u2018running\u2019 state.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the resulting data in Athena - the interface to view and query data from the Glue Catalogue."))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Open the ",(0,n.kt)("a",{parentName:"li",href:"https://eu-west-2.console.aws.amazon.com/athena/home?region=eu-west-2#/query-editor"},"Query editor")),(0,n.kt)("li",{parentName:"ul"},"Make sure workgroup is \u201csandbox\u201d and you\u2019re using the \u201csandbox-raw-zone\u201d database"),(0,n.kt)("li",{parentName:"ul"},"Run a simple query in Athena against your tables (created/ updated) by the Crawlers. A simple way to do this is to select the 3 vertical dots by the table name and select \u201cPreview Table\u201d to see the top 10 lines. (The dialect of SQL used in Athena is ",(0,n.kt)("a",{parentName:"li",href:"https://prestodb.io/"},"Presto SQL"),")")),(0,n.kt)("ol",{start:17},(0,n.kt)("li",{parentName:"ol"},"Delete the job prototype you've created manually in the console in the oart 1 of this module\nIf the deployed job has worked successfully, you can safely delete the one you created manually earlier - your code is now in the DP codebase!  ")),(0,n.kt)("p",null,"Congratulations, you've completed this module and deployed a job in the data platform! "))}h.isMDXComponent=!0},9408:function(e,t,a){t.Z=a.p+"assets/images/cloning_jobs-c2503e064e6e9ecf46dbfa06882c3695.png"},9493:function(e,t,a){t.Z=a.p+"assets/images/job_parameters-a3dbe41df15b36bdcb26ce05edb564cc.png"}}]);