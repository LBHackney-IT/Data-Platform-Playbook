"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[2040],{9905:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var a=t(4848),i=t(8453);const r={title:"Alloy data ingestion",description:"Description of the ingestion and refinement pipeline for Alloy Environmental Services data",layout:"playbook_js",tags:["playbook"]},s="This article describes the Alloy data ingestion process.",o={id:"docs/alloy-ingestion",title:"Alloy data ingestion",description:"Description of the ingestion and refinement pipeline for Alloy Environmental Services data",source:"@site/docs/docs/alloy-ingestion.md",sourceDirName:"docs",slug:"/docs/alloy-ingestion",permalink:"/Data-Platform-Playbook/docs/alloy-ingestion",draft:!1,unlisted:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/docs/alloy-ingestion.md",tags:[{inline:!0,label:"playbook",permalink:"/Data-Platform-Playbook/tags/playbook"}],version:"current",frontMatter:{title:"Alloy data ingestion",description:"Description of the ingestion and refinement pipeline for Alloy Environmental Services data",layout:"playbook_js",tags:["playbook"]},sidebar:"docs",previous:{title:"Ingesting Academy data onto the Data Platform",permalink:"/Data-Platform-Playbook/docs/academy-ingestion"},next:{title:"Auto-adjusting AWS Budget Alerts",permalink:"/Data-Platform-Playbook/docs/auto-adjusting-aws-budget"}},l={},d=[{value:"Overview of process",id:"overview-of-process",level:2},{value:"Retrieving a data extract from the Alloy API",id:"retrieving-a-data-extract-from-the-alloy-api",level:2},{value:"Creating the refined tables",id:"creating-the-refined-tables",level:2},{value:"Pipeline creation in Terraform",id:"pipeline-creation-in-terraform",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"this-article-describes-the-alloy-data-ingestion-process",children:"This article describes the Alloy data ingestion process."})}),"\n",(0,a.jsx)(n.h2,{id:"overview-of-process",children:"Overview of process"}),"\n",(0,a.jsx)(n.p,{children:"The following diagram describes the process steps that make up the pipeline for ingesting Alloy data:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Alloy data ingestion process",src:t(645).A+"",width:"1061",height:"717"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["An initial Glue job triggered on a schedule","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"requests the export described in AQS (Alloy query syntax) from the API"}),"\n",(0,a.jsxs)(n.li,{children:["extracts the csv tables from the returned zip file and saves them to the ",(0,a.jsx)(n.em,{children:"raw zone"})," in S3"]}),"\n",(0,a.jsx)(n.li,{children:"applies only a necessary amount of transformation to save a copy of the data as parquet files also to s3"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"On job completion, a crawler updates tables in the env-services-raw Glue Catalog with the new parquet files"}),"\n",(0,a.jsxs)(n.li,{children:["A second Glue job then creates Dynamic Frames for all unprocessed parquet files for each table in the raw zone","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"this job then checks for a column name dictionary in S3 then applies a mapping if one is found"}),"\n",(0,a.jsx)(n.li,{children:"the transformed data is then exported to the refined zone and saved to s3"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["This data is then crawled and made available in the ",(0,a.jsx)(n.em,{children:"refined zone"})," Catalog"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"retrieving-a-data-extract-from-the-alloy-api",children:"Retrieving a data extract from the Alloy API"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/jobs/env_services/alloy_api_ingestion.py",children:(0,a.jsx)(n.em,{children:"This Glue job"})})," sends queries to the Alloy API that request an extract of datasets described in ",(0,a.jsx)(n.a,{href:"https://help.alloyapp.io/alloy-query-syntax/alloy-query-syntax.html",children:"Alloy Query Syntax (AQS)"}),". These queries are stored in json format and uploadeded to ",(0,a.jsx)(n.a,{href:"https://github.com/LBHackney-IT/Data-Platform/tree/main/scripts/jobs/env_services/aqs",children:"this directory in the Data Platform GitHub repository"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Each query creates a new job for that dataset that runs independently of any other queries."}),"\n",(0,a.jsx)(n.p,{children:"A request is sent to the API and once the export is complete the data is downloaded and extracted as a zip file containing csvs that are saved in S3. These csvs then read from the S3 bucket to a spark data frame. Minimal transformation is applied to allow for conversion to parquet format and import datetime columns are added."}),"\n",(0,a.jsx)(n.h2,{id:"creating-the-refined-tables",children:"Creating the refined tables"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/jobs/env_services/alloy_raw_to_refined.py",children:(0,a.jsx)(n.em,{children:"The second Glue job"})})," looks for tables in the ",(0,a.jsx)(n.em,{children:"env-services-raw-zone"}),' Glue Catalog database with a given prefix of the form "parent table name_".']}),"\n",(0,a.jsxs)(n.p,{children:["It then iterates over each of the sub-tables to process any parquet files in the raw zone that have been created since the last run (identified using the ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html",children:"job bookmarking feature"}),")."]}),"\n",(0,a.jsx)(n.p,{children:"As part of the refining process, the job checks for a json dictionary stored in s3 under the raw bucket with a key formed like this:"}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:"env-services/alloy/mapping-files/parent table name_table_name.json"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The json is of the form:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n    "old_name1": "new_name1",\n    "old_name2": "new_name2",\n    ...\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:"If a key is not present in the table no action will be taken, and similarly if there is no key for a column that is present in the table it remains unchanged."}),"\n",(0,a.jsxs)(n.p,{children:["Column names are cleaned to conform to requirements for parquet formatted files. Refined date columns are added. Then the resultant DynamicFrames are written to the ",(0,a.jsx)(n.em,{children:"Refined Zone"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"pipeline-creation-in-terraform",children:"Pipeline creation in Terraform"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/terraform/etl/25-alloy-etl-env-services.tf",children:"The Terraform"})," will create a complete pipeline and triggers for each AQS json file added to the ",(0,a.jsx)(n.a,{href:"https://github.com/LBHackney-IT/Data-Platform/tree/main/scripts/jobs/env_services/aqs",children:"aqs directory in the Data-Platform repository"}),"."]}),"\n",(0,a.jsx)(n.p,{children:'The naming of these files dictates the names for the resources in AWS. Adding an AQS query in a file called "Parent Table.json" will create:'}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["A Glue trigger:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Alloy API Export Job Trigger ",(0,a.jsx)(n.strong,{children:"Parent Table"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["2 Glue jobs:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["alloy_api_export_",(0,a.jsx)(n.strong,{children:"Parent Table"}),"_env_services"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Parent Table"}),"_alloy_daily_raw_to_refined_env_services"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["2 Crawlers","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Alloy Export Crawler ",(0,a.jsx)(n.strong,{children:"Parent Table"})]}),"\n",(0,a.jsxs)(n.li,{children:["Alloy Refined Crawler ",(0,a.jsx)(n.strong,{children:"Parent Table"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Removing files from this location will result in the related AWS resources also being destroyed when the Terraform in applied."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},645:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/alloy_pipeline-7e59a4787d4203e9ea525f7287a91da6.png"},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var a=t(6540);const i={},r=a.createContext(i);function s(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);