"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[1530],{8186:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var a=t(4848),s=t(8453);const i={title:"Ingesting data from databases into the Data Platform",description:"Ingesting database tables into the Data Platform using a JDBC Connection",layout:"playbook_js",tags:["playbook"]},o=void 0,l={id:"playbook/ingesting-data/database-ingestion",title:"Ingesting data from databases into the Data Platform",description:"Ingesting database tables into the Data Platform using a JDBC Connection",source:"@site/docs/playbook/ingesting-data/006-database-ingestion.md",sourceDirName:"playbook/ingesting-data",slug:"/playbook/ingesting-data/database-ingestion",permalink:"/Data-Platform-Playbook/playbook/ingesting-data/database-ingestion",draft:!1,unlisted:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/playbook/ingesting-data/006-database-ingestion.md",tags:[{inline:!0,label:"playbook",permalink:"/Data-Platform-Playbook/tags/playbook"}],version:"current",sidebarPosition:6,frontMatter:{title:"Ingesting data from databases into the Data Platform",description:"Ingesting database tables into the Data Platform using a JDBC Connection",layout:"playbook_js",tags:["playbook"]},sidebar:"docs",previous:{title:"Ingesting RDS snapshot into the Data Platform Landing Zone",permalink:"/Data-Platform-Playbook/playbook/ingesting-data/ingesting-rds-snapshot-in-landing-zone"},next:{title:"Ingesting Dynamo DB tables into the Landing Zone",permalink:"/Data-Platform-Playbook/playbook/ingesting-data/ingesting-dynamo-db-tables"}},r={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Overview",id:"overview",level:2},{value:"Add the database credentials to the Data Platform project",id:"add-the-database-credentials-to-the-data-platform-project",level:3},{value:"Construct the JDBC URL",id:"construct-the-jdbc-url",level:3},{value:"Set up the Glue JDBC Connection",id:"set-up-the-glue-jdbc-connection",level:3},{value:"The following input variables are required:",id:"the-following-input-variables-are-required",level:4},{value:"Create a Glue job and Crawler to ingest all database tables",id:"create-a-glue-job-and-crawler-to-ingest-all-database-tables",level:3},{value:"Prototyping your Glue job",id:"prototyping-your-glue-job",level:4},{value:"Deploying your Glue job",id:"deploying-your-glue-job",level:4},{value:"Commit your changes and create a Pull Request for review by the Data Platform team",id:"commit-your-changes-and-create-a-pull-request-for-review-by-the-data-platform-team",level:3},{value:"Running the ingestion manually",id:"running-the-ingestion-manually",level:3},{value:"Complete example with both Glue JDBC Connection and Glue job modules",id:"complete-example-with-both-glue-jdbc-connection-and-glue-job-modules",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"This guide explains the process of ingesting data/tables from databases into the Data Platform using AWS Glue JDBC Connection."}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Check that your database type is supported by AWS Glue JDBC Connection (see ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/glue/latest/dg/connection-defining.html",children:"AWS Glue JDBC Connection Properties"})," section)"]}),"\n",(0,a.jsxs)(n.li,{children:["Ensure that your database allows user login/authentication, and you have a database user with login credentials","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"If you would like to restrict access to only a selection of tables in your database, then ensure the database's user permissions are updated to reflect this"}),"\n",(0,a.jsxs)(n.li,{children:["In addition to the database name and user login credentials, you will also need:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["the type of database you want to connect to e.g. ",(0,a.jsx)(n.code,{children:"mssql"})]}),"\n",(0,a.jsx)(n.li,{children:"the database host name/ endpoint"}),"\n",(0,a.jsx)(n.li,{children:"the database port number"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["These will be used to construct the ",(0,a.jsx)(n.a,{href:"#construct-the-jdbc-url",children:"JDBC URL"})," in a later section"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"In the following sections you will set up a connection from the Data Platform to your source database to ingest its data, which will involve:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Authenticating access to the source database from the Data Platform by adding the database credentials to AWS Secrets Manager"}),"\n",(0,a.jsx)(n.li,{children:"Establishing a connection to the respective database by creating a Glue connection using a JDBC URL which uses the source database's credentials stored in Secrets Manager"}),"\n",(0,a.jsx)(n.li,{children:"Populating a Glue Catalog database with the source database's table schemas and metadata by creating a Crawler and crawling the source database"}),"\n",(0,a.jsx)(n.li,{children:"Pulling in the data from the source database and writing to the Data Platform S3 storage by creating a Glue job which uses the Glue connection as well as the table schemas and metadata from the Glue Catalog database"}),"\n",(0,a.jsx)(n.li,{children:"Making the data available for querying in Athena and other Glue jobs by creating a Crawler to crawl the tables in S3 which will then populate a predetermined Glue Catalog database"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"add-the-database-credentials-to-the-data-platform-project",children:"Add the database credentials to the Data Platform project"}),"\n",(0,a.jsx)(n.p,{children:"The database credentials are retrieved from AWS Secrets Manager.\nThe credentials are used to allow the Data Platform to authenticate against the source database."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Contact an engineer from the Data Platform team to add the database credentials to AWS Secrets Manager."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["You will need to request that a ",(0,a.jsx)(n.strong,{children:"secret"})," (with an appropriate description) is created following the naming convention below:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"database-credentials/DATABASE_NAME-DATASET_NAME\n"})}),"\n",(0,a.jsxs)(n.p,{children:["e.g. ",(0,a.jsx)(n.code,{children:"database-credentials/geolive-permits"})," (",(0,a.jsx)(n.strong,{children:"Remember this name as you will need it later"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Request that the following key-value pairs be added to this secret:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"database_name"})," = ",(0,a.jsx)(n.code,{children:"Name of your database"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"username"})," = ",(0,a.jsx)(n.code,{children:"Your database user username"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"password"})," = ",(0,a.jsx)(n.code,{children:"Your database user password"})]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["You will be notified once the secret has been stored with the credentials.\nMake a note of the ",(0,a.jsx)(n.strong,{children:"secret"})," name as it will be needed in the ",(0,a.jsx)(n.a,{href:"#set-up-the-glue-jdbc-connection",children:"Set up the Glue JDBC Connection section"})," below to reference your stored credentials in the JDBC Connection."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"construct-the-jdbc-url",children:"Construct the JDBC URL"}),"\n",(0,a.jsx)(n.p,{children:"In this section, you will create the JDBC URL which will be used in the section below."}),"\n",(0,a.jsx)(n.p,{children:"Generally JDBC URLs for different types of databases are quite similar.\nHowever, some differ slightly. You will be using the following to construct the JDBC URL:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Database type"}),"\n",(0,a.jsx)(n.li,{children:"Database name"}),"\n",(0,a.jsxs)(n.li,{children:["Database host/ endpoint e.g. ",(0,a.jsx)(n.code,{children:"127.0.0.1"})]}),"\n",(0,a.jsxs)(n.li,{children:["Database port number e.g. ",(0,a.jsx)(n.code,{children:"1433"})]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Refer to ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/glue/latest/dg/connection-defining.html",children:"AWS Glue JDBC Connection Properties"})," for examples and guidance on how to construct your JDBC URL."]}),"\n",(0,a.jsx)(n.h3,{id:"set-up-the-glue-jdbc-connection",children:"Set up the Glue JDBC Connection"}),"\n",(0,a.jsx)(n.p,{children:"Here you will configure a module which will set up the connection to the source database, as well as a Crawler to crawl the source database which will retrieve the\nmetadata and schemas of the database tables to populate in a Glue Catalog database.\nThe module will also create a Glue Workflow which the Crawler will be added to.\nYou can then use this workflow to link your ingestion Glue job (and Crawler) which you will be creating in the following section.\nThis will help facilitate the automation of the entire database ingestion process."}),"\n",(0,a.jsx)(n.admonition,{type:"important",children:(0,a.jsxs)(n.p,{children:["The Crawler will run automatically every weekday at 6am, however, in order to ingest your data on the same day of deployment, you will need to run it manually in the AWS Console.\nIt will have the same name as the value you set for the ",(0,a.jsx)(n.strong,{children:"name"})," input variable in this section."]})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.em,{children:["For more technical details on the overall process, see: ",(0,a.jsx)(n.a,{href:"/Data-Platform-Playbook/spikes/mssql-ingestion",children:"Database Ingestion documentation"})]})}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Open the ",(0,a.jsx)(n.a,{href:"https://github.com/LBHackney-IT/Data-Platform/tree/main/terraform/core",children:"terraform/core directory"})," in the Data Platform Project in GitHub.","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["If you don't have the correct permissions, you'll get a '404' error (see ",(0,a.jsx)(n.a,{href:"/Data-Platform-Playbook/playbook/getting-set-up/",children:"Getting Set Up on the Platform"}),")."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["Note: If the data you're ingesting is for a specific department then it should be ingested into that department's ",(0,a.jsx)(n.code,{children:"raw zone"})," S3 bucket, otherwise it should go into the ",(0,a.jsx)(n.code,{children:"landing zone"})]})," S3 bucket."]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Create a new file ",(0,a.jsx)(n.code,{children:"29-<YOUR-DEPARTMENT-NAME>-<DATABASE-NAME>-database-ingestion.tf"})," if department specific, otherwise ",(0,a.jsx)(n.code,{children:"29-<DATABASE-NAME>-database-ingestion.tf"})]}),"\n",(0,a.jsx)(n.p,{children:"For example, for Academy (database), which is not department specific, the file name will be:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"29-academy-database-ingestion.tf\n"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsxs)(n.em,{children:["Refer to this ",(0,a.jsx)(n.a,{href:"#example-module-block",children:"example"})," to get started."]})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Copy the ",(0,a.jsx)(n.a,{href:"#example-module-block",children:"example module block"})," paste it in your file."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Update the ",(0,a.jsx)(n.code,{children:"module"})," name using the following name convention:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"<department_name>_<database_name>_database_ingestion\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"Note: the department name must be all lowercase and separated by underscores"})}),"\n",(0,a.jsx)(n.p,{children:"For example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'"academy_lbhatestrbviews_database_ingestion"\n'})}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Update or add your input variables."}),"\n"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.h4,{id:"the-following-input-variables-are-required",children:"The following input variables are required:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"source"})," (required): This will be ",(0,a.jsx)(n.code,{children:'"../modules/database-ingestion-via-jdbc-connection"'}),". It is the path to where the database ingestion module is saved within the repository"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsxs)(n.em,{children:[(0,a.jsx)(n.strong,{children:"Note"}),": If you've copied the example module block then you won\u2019t need to change the ",(0,a.jsx)(n.strong,{children:"source"})," variable"]})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"name"})," (required): Name of the dataset that will be ingested in all ",(0,a.jsx)(n.strong,{children:"lowercase letters"})," with ",(0,a.jsx)(n.strong,{children:"words separated by hyphens"}),". e.g. ",(0,a.jsx)(n.code,{children:'"revenue-benefits-and-council-tax"'})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"This will be the name of all your resources created as part of this step and will be needed to identify your resources in the AWS Console when populating the Glue job parameters to prototype your Glue job."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"jdbc_connection_url"})," (required): This will be in the format: ",(0,a.jsx)(n.code,{children:"jdbc:protocol://host:port/db_name"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Set this to the JDBC URL you constructed in the previous section.\nYou can refer to ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/glue/latest/dg/connection-defining.html",children:"AWS Glue JDBC Connection Properties"})," for more guidance on how to construct your JDBC URL."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"For example, a SQL Server database's JDBC URL will look like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'jdbc_connection_url = "jdbc:sqlserver://10.120.23.22:1433;databaseName=LBHATestRBViews"\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"jdbc_connection_description"})," (required): A description of the connection i.e. The type of connection, database and dataset that will be ingested\nFor example:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'"JDBC connection to Academy Production Insights LBHATestRBViews database to ingest Council Tax data"\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"jdbc_connection_subnet"})," (required): The subnet to deploy the connection to.\nSet this to ",(0,a.jsx)(n.code,{children:"data.aws_subnet.network[local.instance_subnet_id]"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"identifier_prefix"})," (required): Set this to ",(0,a.jsx)(n.code,{children:"local.short_identifier_prefix"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"database_secret_name"})," (required): Name of the secret in AWS Secrets Manager where your database credentials are being stored.\nThis would have been shared with you by a member of the Data Platform team.\nFor example:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'database_secret_name = "database-credentials/lbhatestrbviews-council-tax"\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"schema_name"})," (optional): Name of schema in the database containing tables to be ingested. e.g. ",(0,a.jsx)(n.code,{children:'"parking"'}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["For databases that support schemas, you can provide a schema name here to ingest all tables in the schema within the specified database.\nOracle Database and MySQL don\u2019t support this; therefore ",(0,a.jsx)(n.strong,{children:"DO NOT"})," enter a value here."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.ol,{start:"6",children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Commit your changes and create a Pull Request for review by the Data Platform team.\nYou should wait for it to be approved and deployed before moving onto the next step."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["See ",(0,a.jsx)(n.a,{href:"../getting-set-up/using-github#committing-your-changes-to-the-data-platform-project",children:"Committing changes"})," section of the ",(0,a.jsx)(n.strong,{children:"Using Github"})," guide.\nThe Data Platform team needs to approve any changes to the code that you make, so your change won't happen automatically."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Once you get confirmation that the code has been successfully deployed,\nyou will need to do the following before moving on to the next section:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Request that the ID of the security group of the Glue JDBC Connection (created in this module) is added to the source database's inbound security group rules by an appropriate engineer of the respective AWS account."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["You can find the security group ID of your connection by navigating to ",(0,a.jsx)(n.code,{children:"AWS Glue"})," in the AWS Console,\nthen clicking ",(0,a.jsx)(n.code,{children:"Connections"})," in the left-hand navigation bar and then searching for your connection.\nIt will have the same name that you set in the ",(0,a.jsx)(n.strong,{children:"name"})," input variable above."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Click on your connection and copy the ID next to ",(0,a.jsx)(n.code,{children:"Security groups"})," e.g. ",(0,a.jsx)(n.code,{children:"sg-05a4fc711d3e12345"}),".\nThis is the security group ID you need to provide to the engineer."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Test your connection and ensure it works:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Select your connection and click ",(0,a.jsx)(n.code,{children:"Test Connection"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Assign the relevant department IAM role.","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsxs)(n.strong,{children:["Note: If the data you are ingesting is not department specific, you should use the IAM role: ",(0,a.jsx)(n.code,{children:"dataplatform-prod-glue-role"}),"."]})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Lastly, click ",(0,a.jsx)(n.code,{children:"Test Connection"})," (this can take up to a minute to complete)."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"create-a-glue-job-and-crawler-to-ingest-all-database-tables",children:"Create a Glue job and Crawler to ingest all database tables"}),"\n",(0,a.jsx)(n.p,{children:"Once your Pull Request for setting up the JDBC Connection has been approved and deployed, you can continue with this section."}),"\n",(0,a.jsx)(n.p,{children:"Here you will create a Glue job which will use the JDBC connection you've just created to pull the database tables into S3.\nYou will also create a Crawler to read all the ingested tables from S3 and populate a Glue Catalog Database so that the\ndata can be queried in Athena or consumed by other Glue jobs for further processing."}),"\n",(0,a.jsx)(n.admonition,{type:"important",children:(0,a.jsxs)(n.p,{children:["If your data is ",(0,a.jsx)(n.strong,{children:"NOT"})," department specific, you will not be able to crawl the S3 output location to populate a Glue Catalog database and therefore you should ",(0,a.jsx)(n.strong,{children:"NOT"})," set any configurations in the ",(0,a.jsx)(n.code,{children:"crawler_details"})," input variable (delete this input variable if necessary)."]})}),"\n",(0,a.jsx)(n.h4,{id:"prototyping-your-glue-job",children:"Prototyping your Glue job"}),"\n",(0,a.jsxs)(n.p,{children:["You can prototype your Glue job and test ingesting a few tables by referring to and cloning an existing Glue job.\nYou can search for the ",(0,a.jsx)(n.code,{children:'"Academy Revenues & Benefits Housing Needs Database Ingestion"'})," Glue job in the list of jobs in the ",(0,a.jsx)(n.a,{href:"https://eu-west-2.console.aws.amazon.com/gluestudio/home?region=eu-west-2#/jobs",children:"AWS Console"}),"\nto use as an example. You can also refer to the ",(0,a.jsx)(n.a,{href:"/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/using-glue-studio",children:"Using Glue Studio"})," guide for guidance on prototyping your Glue job."]}),"\n",(0,a.jsxs)(n.p,{children:["To prototype your script you will need to manually set/ update all the Glue Job parameters and Connections in the ",(0,a.jsx)(n.code,{children:"Job Details"})," tab:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["The ",(0,a.jsx)(n.code,{children:"source_catalog_database"})," Glue Job parameter and the ",(0,a.jsx)(n.code,{children:"Connections"})," input variable should be the same as what you set for the ",(0,a.jsx)(n.strong,{children:"name"})," input variable in the previous section."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The example Glue job linked above will read all the tables and output them to a specified S3 location."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["It uses two helper functions which are imported from ",(0,a.jsx)(n.code,{children:"helpers.py"}),", these are:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"get_all_database_tables"}),": used to retrieve all the table names from the specified Glue Catalog Database"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"update_table_ingestion_details"}),": used to create a dataframe containing stats, including errors, on the ingestion process for each table"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"deploying-your-glue-job",children:"Deploying your Glue job"}),"\n",(0,a.jsx)(n.admonition,{type:"important",children:(0,a.jsx)(n.p,{children:"Before continuing with this section, ensure that you have deleted any data that was copied to S3 whilst prototyping your Glue job."})}),"\n",(0,a.jsx)(n.p,{children:"When you are ready to deploy your Glue job (and Crawler) to the Data Platform project, you can continue with the below steps.\nYour Glue job will copy all the tables from your source database to S3 which will then be crawled and populated in a Glue Catalog Database where the tables can be queried.\nSpark Web UI is used to monitior and debug the glue jobs. Every 30 seconds, AWS Glue flushes the Spark event logs to an S3 bucket titled Spark UI Bucket."}),"\n",(0,a.jsxs)(n.p,{children:["You will be using the existing ",(0,a.jsx)(n.a,{href:"/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/deploy-glue-jobs",children:"Glue job module"})," to deploy your ingestion Glue job and Crawler."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"The steps below only serve as complementary guidance and should be followed along with the official documentation for the Glue job module linked above."})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"You can add the Glue job module Terraform code to the same file you created/ updated in the previous section when setting up the Glue JDBC Connection."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Set the input variables for the Glue job and Crawler"})}),"\n",(0,a.jsxs)(n.p,{children:["The following ",(0,a.jsx)(n.strong,{children:"input variables"})," and ",(0,a.jsx)(n.strong,{children:"job parameters"})," need to be set:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Input variables"})," (required):"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"script_s3_object_key"})," (required): S3 object key of the script which will be used to ingest the database tables.\nSet this to: ",(0,a.jsx)(n.code,{children:"aws_s3_bucket_object.ingest_database_tables_via_jdbc_connection.key"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"jdbc_connections"})," (required): The list of connections used for this job, i.e. JDBC connection.\nThis will be ",(0,a.jsx)(n.code,{children:"[module.<NAME_OF_CONNECTION_MODULE>[0].jdbc_connection_name]"}),".\nSee step 4 in the section: ",(0,a.jsx)(n.a,{href:"#set-up-the-glue-jdbc-connection",children:"set up the glue JDBC connection"})," above for a reminder of the module name."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"For example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"[module.academy_lbhatestrbviews_database_ingestion[0].jdbc_connection_name]\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.em,{children:["Note: ensure there are surrounding square brackets (",(0,a.jsx)(n.code,{children:"[]"}),") around the value provided here"]})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"triggered_by_crawler"})," (optional): You can configure your job to run automatically once the Crawler created in the previous section has run successfully.","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["To add this trigger, set this input variable to:","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"module.<NAME_OF_CONNECTION_MODULE>[0].crawler_name\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"If you don't populate this variable then the Glue job and Crawler will need to be run manually in the AWS Console."}),"\n",(0,a.jsxs)(n.p,{children:["If you want it to run on a schedule then please refer to the ",(0,a.jsx)(n.strong,{children:'"Variables used for scheduling a Glue job"'})," section of ",(0,a.jsx)(n.a,{href:"/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/deploy-glue-jobs#variables-used-for-scheduling-a-glue-job",children:"this article"})," for an explanation on how to set the variables to do so."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"workflow_name"})," (optional): Workflow to add your Glue job (and Crawler to).\nThis is required if you set a ",(0,a.jsx)(n.strong,{children:"schedule"})," or set the input variable ",(0,a.jsx)(n.strong,{children:"triggered_by_crawler"})," above."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"To add your Glue job (and Crawler) to a workflow, set this input variable to:"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"module.<NAME_OF_CONNECTION_MODULE>[0].workflow_name\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Job parameters"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Note: For the following optional ",(0,a.jsx)(n.strong,{children:"job parameters"}),"; ",(0,a.jsx)(n.em,{children:'"--s3_ingestion_bucket_target"'})," and ",(0,a.jsx)(n.em,{children:'"--s3_ingestion_details_target"'}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"<ZONE>"})," refers to either: ",(0,a.jsx)(n.code,{children:"raw"})," or ",(0,a.jsx)(n.code,{children:"landing"})," S3 zones"]}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsxs)(n.strong,{children:["If the data you're ingesting is for a specific department then it should be ingested into that department's ",(0,a.jsx)(n.code,{children:"raw"})," zone, otherwise it should go into the ",(0,a.jsx)(n.code,{children:"landing"})," zone"]})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:'"--source_catalog_database"'})," (required): The Glue Catalog Database where your databases' table schemas are stored"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["This will be ",(0,a.jsx)(n.code,{children:"module.<NAME_OF_CONNECTION_MODULE>[0].ingestion_database_name"}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"For example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"module.academy_lbhatestrbviews_database_ingestion[0].ingestion_database_name\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:'"--s3_ingestion_bucket_target"'})," (required): The S3 location where the ingested tables should be stored"]}),"\n",(0,a.jsx)(n.p,{children:"For example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'"--s3_ingestion_bucket_target" = "s3://${module.<ZONE>_zone.bucket_id}/<YOUR-DEPARTMENT-NAME>/"\n'})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsxs)(n.em,{children:["Note: ensure that your department name and folder name is all ",(0,a.jsx)(n.strong,{children:"lowercase"})," with ",(0,a.jsx)(n.strong,{children:"words separated by hyphens"}),"\ne.g. ",(0,a.jsx)(n.code,{children:"housing-repairs"}),"."]})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:'"--s3_ingestion_details_target"'})," (required): The S3 location where the ingestion details should be stored"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.em,{children:["Note: in order for the Crawler to add your ingestion details to the Glue Catalog Database so that they can be analysed in Athena later,\nyou should set this parameter to have one additional folder level (e.g.",(0,a.jsx)(n.code,{children:"ingestion-details"}),") to what was set in ",(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"s3_ingestion_bucket_target"})})]}),"."]}),"\n",(0,a.jsx)(n.p,{children:"For example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'"--s3_ingestion_details_target" = "s3://${module.<ZONE>_zone.bucket_id}/<YOUR-DEPARTMENT-NAME>/ingestion-details/"\n'})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsxs)(n.em,{children:["Note: ensure that your department name is all ",(0,a.jsx)(n.strong,{children:"lowercase"})," with ",(0,a.jsx)(n.strong,{children:"words separated by underscores"}),"\ne.g. ",(0,a.jsx)(n.code,{children:"housing_repairs"}),"."]})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"crawler_details"}),":"]}),"\n",(0,a.jsx)(n.admonition,{type:"caution",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"If your data is department specific, you should set the following parameters."}),"\nOtherwise, if your data is ",(0,a.jsx)(n.strong,{children:"NOT"})," department specific, or ",(0,a.jsx)(n.strong,{children:"NOT"}),' "unrestricted" data, and is being written to the ',(0,a.jsx)(n.strong,{children:"landing"})," zone, you should ",(0,a.jsx)(n.strong,{children:"NOT"})," set any of the below parameters (deleting the entire ",(0,a.jsx)(n.code,{children:"crawler_details"})," configuration if present or working with a duplicated module block).\nIn that case, the data will need to be moved to ",(0,a.jsx)(n.strong,{children:"raw"})," zone, and a specific ",(0,a.jsx)(n.strong,{children:"department"})," before it can be crawled and queried in Athena."]})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:"database_name"})," (required): Glue database where results are written after being crawled"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"module.department_<YOUR_DEPARTMENT_NAME>.<S3_BUCKET_ZONE>_zone_catalog_database_name\n"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Where ",(0,a.jsx)(n.code,{children:"<S3_BUCKET_ZONE>"})," will be: ",(0,a.jsx)(n.code,{children:"raw"}),". The same zone you wrote the data to in S3."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:"s3_target_location"})," (required): This should be the same as ",(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:'"--s3_ingestion_bucket_target"'})})," set above"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:"configuration"})," (required): Set the ",(0,a.jsx)(n.code,{children:"TableLevelConfiguration"})," to 1 plus the number of directory levels in ",(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:'"--s3_ingestion_bucket_target"'})})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.em,{children:"table_prefix"})," (required): Set this to value of your choice or if the table prefix is not required set it to ",(0,a.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["For example: The value for ",(0,a.jsx)(n.code,{children:"TableLevelConfiguration"})," with an ",(0,a.jsx)(n.strong,{children:"s3_ingestion_bucket_target"})," of ",(0,a.jsx)(n.code,{children:'"s3://${module.raw_zone.bucket_id}/academy/"'})," will be ",(0,a.jsx)(n.code,{children:"3"})]}),"\n",(0,a.jsxs)(n.p,{children:["A complete example of ",(0,a.jsx)(n.strong,{children:"crawler_details"})," can be seen below:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'crawler_details = {\n    database_name      = module.department_academy.raw_zone_catalog_database_name \n    s3_target_location = "s3://${module.raw_zone.bucket_id}/academy/"\n    configuration = jsonencode({\n        Version = 1.0\n        Grouping = {\n            TableLevelConfiguration = 3\n        }\n    })\ntable_prefix      = null\n}\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"commit-your-changes-and-create-a-pull-request-for-review-by-the-data-platform-team",children:"Commit your changes and create a Pull Request for review by the Data Platform team"}),"\n",(0,a.jsx)(n.p,{children:"You can now submit your changes for review by the Data Platform team."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["See ",(0,a.jsx)(n.a,{href:"../getting-set-up/using-github#committing-your-changes-to-the-data-platform-project",children:"Committing changes"})," section of the ",(0,a.jsx)(n.strong,{children:"Using Github"})," guide.\nThe Data Platform team needs to approve any changes to the code that you make, so your change won't happen automatically.\nOnce your changes have been approved and deployed, the job will run at the next scheduled time (if scheduled)."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"running-the-ingestion-manually",children:"Running the ingestion manually"}),"\n",(0,a.jsx)(n.p,{children:"Once you have been notified that your Pull Request has been merged, you can run the ingestion manually from the AWS Console or wait until the scheduled time (if you've set one)."}),"\n",(0,a.jsx)(n.h3,{id:"complete-example-with-both-glue-jdbc-connection-and-glue-job-modules",children:"Complete example with both Glue JDBC Connection and Glue job modules"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'module "academy_mssql_database_ingestion" {\n    count = local.is_live_environment ? 1 : 0\n    tags  = module.tags.values\n\n    source = "../modules/database-ingestion-via-jdbc-connection"\n\n    name                        = "revenue-benefits-and-council-tax"\n    jdbc_connection_url         = "jdbc:sqlserver://10.120.23.22:1433;databaseName=LBHATestRBViews"\n    jdbc_connection_description = "JDBC connection to Academy Production Insights LBHATestRBViews database"\n    jdbc_connection_subnet_id   = local.subnet_ids_list[local.subnet_ids_random_index]\n    database_availability_zone  = "eu-west-2a"\n    database_secret_name        = "database-credentials/lbhatestrbviews-council-tax"\n    identifier_prefix           = local.short_identifier_prefix\n    vpc_id                      = data.aws_vpc.network.id\n}\n\nmodule "ingest_rev_bev_council_tax" {\n  count = local.is_live_environment ? 1 : 0\n  tags  = module.tags.values\n\n  source = "../modules/aws-glue-job"\n\n  job_name               = "${local.short_identifier_prefix}Revenue & Benefits and Council Tax Database Ingestion"\n  script_s3_object_key   = aws_s3_bucket_object.ingest_database_tables_via_jdbc_connection.key\n  environment            = var.environment\n  pydeequ_zip_key        = aws_s3_bucket_object.pydeequ.key\n  helper_module_key      = aws_s3_bucket_object.helpers.key\n  jdbc_connections       = [module.academy_mssql_database_ingestion[0].jdbc_connection_name]\n  glue_role_arn          = aws_iam_role.glue_role.arn\n  glue_temp_bucket_id    = module.glue_temp_storage.bucket_id\n  glue_scripts_bucket_id = module.glue_scripts.bucket_id\n  workflow_name          = module.academy_mssql_database_ingestion[0].workflow_name\n  triggered_by_crawler   = module.academy_mssql_database_ingestion[0].crawler_name\n  job_parameters = {\n    "--source_data_database"             = module.academy_mssql_database_ingestion[0].ingestion_database_name\n    "--s3_ingestion_bucket_target"       = "s3://${module.raw_zone.bucket_id}/academy/"\n    "--s3_ingestion_details_target"      = "s3://${module.raw_zone.bucket_id}/academy/ingestion-details/"\n    "--TempDir"                          = "s3://${module.glue_temp_storage.bucket_id}/"\n    "--extra-py-files"                   = "s3://${module.glue_scripts.bucket_id}/${aws_s3_bucket_object.helpers.key}"\n    "--extra-jars"                       = "s3://${module.glue_scripts.bucket_id}/jars/deequ-1.0.3.jar"\n    "--enable-continuous-cloudwatch-log" = "true"\n  }\n  crawler_details = {\n    database_name      = module.department_academy.raw_zone_catalog_database_name \n    s3_target_location = "s3://${module.raw_zone.bucket_id}/academy/"\n    configuration = jsonencode({\n      Version = 1.0\n      Grouping = {\n        TableLevelConfiguration = 3\n      }\n    })\n    table_prefix      = null\n  }\n}\n'})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var a=t(6540);const s={},i=a.createContext(s);function o(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);