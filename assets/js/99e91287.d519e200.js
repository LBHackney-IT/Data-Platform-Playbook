"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[9800],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>g});var o=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,o,r=function(e,t){if(null==e)return{};var a,o,r={},n=Object.keys(e);for(o=0;o<n.length;o++)a=n[o],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(o=0;o<n.length;o++)a=n[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=o.createContext({}),d=function(e){var t=o.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},m=function(e){var t=d(e.components);return o.createElement(l.Provider,{value:t},e.children)},p="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var a=e.components,r=e.mdxType,n=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),p=d(a),h=r,g=p["".concat(l,".").concat(h)]||p[h]||c[h]||n;return a?o.createElement(g,i(i({ref:t},m),{},{components:a})):o.createElement(g,i({ref:t},m))}));function g(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var n=a.length,i=new Array(n);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:r,i[1]=s;for(var d=2;d<n;d++)i[d]=a[d];return o.createElement.apply(null,i)}return o.createElement.apply(null,a)}h.displayName="MDXCreateElement"},922:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>n,metadata:()=>s,toc:()=>d});var o=a(7462),r=(a(7294),a(3905));const n={id:"introduction",title:"\ud83d\udcdaIntroduction",description:"The DAP\u21e8flow guide for data analysts and engineers, for developing and deploying Airflow DAGs, running data pipelines in the Data Analytics Platform (DAP).",layout:"playbook_js",tags:["dap-airflow"]},i="\ud83d\udcdaIntroduction",s={unversionedId:"dap-airflow/introduction",id:"dap-airflow/introduction",title:"\ud83d\udcdaIntroduction",description:"The DAP\u21e8flow guide for data analysts and engineers, for developing and deploying Airflow DAGs, running data pipelines in the Data Analytics Platform (DAP).",source:"@site/docs/dap-airflow/introduction.md",sourceDirName:"dap-airflow",slug:"/dap-airflow/introduction",permalink:"/Data-Platform-Playbook/dap-airflow/introduction",draft:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/dap-airflow/introduction.md",tags:[{label:"dap-airflow",permalink:"/Data-Platform-Playbook/tags/dap-airflow"}],version:"current",frontMatter:{id:"introduction",title:"\ud83d\udcdaIntroduction",description:"The DAP\u21e8flow guide for data analysts and engineers, for developing and deploying Airflow DAGs, running data pipelines in the Data Analytics Platform (DAP).",layout:"playbook_js",tags:["dap-airflow"]},sidebar:"docs",previous:{title:"VPC Peering Connection between Data Platform and Production APIs AWS accounts",permalink:"/Data-Platform-Playbook/docs/vpc-peering-connection-dataplatform-and-production-apis-account"},next:{title:"Begin here",permalink:"/Data-Platform-Playbook/dap-airflow/onboarding/begin"}},l={},d=[{value:"What is <strong>DAP\u21e8flow</strong>?",id:"what-is-dapflow",level:2},{value:"<strong>DAP\u21e8flow</strong> allows Data Analysts, in the simplest way possible, to develop and run data pipelines using their own service&#39;s data and create data products for their service and service users.",id:"dapflow-allows-data-analysts-in-the-simplest-way-possible-to-develop-and-run-data-pipelines-using-their-own-services-data-and-create-data-products-for-their-service-and-service-users",level:3},{value:"Building data pipelines used to be harder and more complex and time consuming.",id:"building-data-pipelines-used-to-be-harder-and-more-complex-and-time-consuming",level:4},{value:"How <strong>DAP\u21e8flow</strong> solved our problems",id:"how-dapflow-solved-our-problems",level:4},{value:"\ud83d\udcdaOnboarding",id:"onboarding",level:2},{value:"A series onboarding documents is available here, to help Data Analysts get started with <strong>DAP\u21e8flow</strong>",id:"a-series-onboarding-documents-is-available-here-to-help-data-analysts-get-started-with-dapflow",level:4},{value:"<strong><em>&quot;We</em> \u2661 <em>your feedback!&quot;</em></strong>",id:"we--your-feedback",level:4},{value:"<strong>Below here, is the full list of topics currently on offer...</strong>",id:"below-here-is-the-full-list-of-topics-currently-on-offer",level:4},{value:"Before you begin",id:"before-you-begin",level:3},{value:"How do I get onboard <strong>DAP\u21e8flow</strong>?",id:"how-do-i-get-onboard-dapflow",level:4},{value:"Welcome!",id:"welcome",level:3},{value:"What are my <strong>Service Terms<code>[]</code></strong>?",id:"what-are-my-service-terms",level:4},{value:"AWS Console access",id:"aws-console-access",level:3},{value:"How will I access the <strong><em>AWS Management Console</em></strong>?",id:"how-will-i-access-the-aws-management-console",level:4},{value:"AWS region",id:"aws-region",level:3},{value:"How will I ensure I am in the correct <strong>AWS region</strong>?",id:"how-will-i-ensure-i-am-in-the-correct-aws-region",level:4},{value:"Amazon Athena",id:"amazon-athena",level:3},{value:"How will I use <strong><em>Amazon Athena</em></strong> to access my database?",id:"how-will-i-use-amazon-athena-to-access-my-database",level:4},{value:"My current service data",id:"my-current-service-data",level:3},{value:"How will I access my <code>[service]</code>&#39;s current data from <strong><em>Amazon Athena</em></strong>?",id:"how-will-i-access-my-services-current-data-from-amazon-athena",level:4},{value:"My service data history",id:"my-service-data-history",level:3},{value:"How will I access my <code>[service]</code>&#39;s data history from <strong><em>Amazon Athena</em></strong>?",id:"how-will-i-access-my-services-data-history-from-amazon-athena",level:4},{value:"Query my service data",id:"query-my-service-data",level:3},{value:"How will I query and analyze my <code>[service]</code>&#39;s data with <strong><em>Amazon Athena</em></strong>?",id:"how-will-i-query-and-analyze-my-services-data-with-amazon-athena",level:4},{value:"Prototype simple transforms",id:"prototype-simple-transforms",level:3},{value:"How can I use <strong><em>Amazon Athena</em></strong> to prototype a simple table-join data transformation?",id:"how-can-i-use-amazon-athena-to-prototype-a-simple-table-join-data-transformation",level:4},{value:"Prototype legacy transforms",id:"prototype-legacy-transforms",level:3},{value:"How do I use <strong><em>Amazon Athena</em></strong> to prototype a data transformation from my <code>[legacy SQL query]</code>?",id:"how-do-i-use-amazon-athena-to-prototype-a-data-transformation-from-my-legacy-sql-query",level:4},{value:"GitHub access",id:"github-access",level:3},{value:"How do I set up my <strong><em>GitHub</em></strong> access for <strong>DAP\u21e8flow</strong>?",id:"how-do-i-set-up-my-github-access-for-dapflow",level:4},{value:"GitHub branching",id:"github-branching",level:3},{value:"How do I create <code>[transform branch]</code> as my new working branch of <strong>DAP\u21e8flow</strong>&#39;s repository?",id:"how-do-i-create-transform-branch-as-my-new-working-branch-of-dapflows-repository",level:4},{value:"Committing transforms",id:"committing-transforms",level:3},{value:"How do I commit my working <code>[transform SQL]</code> to <strong>DAP\u21e8flow</strong>&#39;s repository?",id:"how-do-i-commit-my-working-transform-sql-to-dapflows-repository",level:4},{value:"GitHub pull requests",id:"github-pull-requests",level:3},{value:"How do I raise a <em>&quot;pull request&quot;</em> to merge my <code>[transform branch]</code> into the <code>main</code> trunk of the <strong>DAP\u21e8flow</strong> repository?",id:"how-do-i-raise-a-pull-request-to-merge-my-transform-branch-into-the-main-trunk-of-the-dapflow-repository",level:4},{value:"\ud83d\udcdaComing soon...",id:"coming-soon",level:2},{value:"Merging branches",id:"merging-branches",level:3},{value:"How do i complete the merge of <code>[transform branch]</code> into the main trunk of <strong>DAP\u21e8flow</strong>&#39;s repository?",id:"how-do-i-complete-the-merge-of-transform-branch-into-the-main-trunk-of-dapflows-repository",level:4},{value:"Airflow",id:"airflow",level:3},{value:"How will I access my data transforms using <strong><em>Airflow</em></strong> on the web?",id:"how-will-i-access-my-data-transforms-using-airflow-on-the-web",level:4},{value:"Adding tables to the raw-zone",id:"adding-tables-to-the-raw-zone",level:3},{value:"How do i add a new table ingestion to my <code>[service raw-zone]</code> database?",id:"how-do-i-add-a-new-table-ingestion-to-my-service-raw-zone-database",level:4},{value:"\ud83d\udcdaSuggested for later...",id:"suggested-for-later",level:2},{value:"Migrating old prototype Athena SQL to the new <strong>DAP\u21e8flow</strong>",id:"migrating-old-prototype-athena-sql-to-the-new-dapflow",level:4},{value:"Migrating old production Spark SQL to the new <strong>DAP\u21e8flow</strong>",id:"migrating-old-production-spark-sql-to-the-new-dapflow",level:4},{value:"Refined-zone views",id:"refined-zone-views",level:4},{value:"External access to <strong>DAP\u21e8flow</strong> products",id:"external-access-to-dapflow-products",level:4},{value:"Removing tables from my <code>[service raw-zone]</code> database",id:"removing-tables-from-my-service-raw-zone-database",level:4},{value:"Removing products from the <code>[service refined-zone]</code> database",id:"removing-products-from-the-service-refined-zone-database",level:4}],m={toc:d},p="wrapper";function c(e){let{components:t,...n}=e;return(0,r.kt)(p,(0,o.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"introduction"},"\ud83d\udcdaIntroduction"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"DAP\u21e8flow",src:a(4673).Z,width:"1930",height:"347"}),"  "),(0,r.kt)("h2",{id:"what-is-dapflow"},"What is ",(0,r.kt)("strong",{parentName:"h2"},"DAP\u21e8flow"),"?"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"DAP\u21e8flow")," is an integration of ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Apache Airflow"))," with ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," built upon Hackney's ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Data Analytics Platform")),". "),(0,r.kt)("h3",{id:"dapflow-allows-data-analysts-in-the-simplest-way-possible-to-develop-and-run-data-pipelines-using-their-own-services-data-and-create-data-products-for-their-service-and-service-users"},(0,r.kt)("strong",{parentName:"h3"},"DAP\u21e8flow")," allows Data Analysts, in the simplest way possible, to develop and run data pipelines using their own service's data and create data products for their service and service users."),(0,r.kt)("h4",{id:"building-data-pipelines-used-to-be-harder-and-more-complex-and-time-consuming"},"Building data pipelines used to be harder and more complex and time consuming."),(0,r.kt)("p",null,"Data Analysts, after prototyping their SQL queries using ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," were required to convert ",(0,r.kt)("em",{parentName:"p"},"Athena SQL")," code to ",(0,r.kt)("em",{parentName:"p"},"Spark SQL"),", a different SQL dialect, then embed their code within an ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Glue"))," job which they had to deploy using ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Terraform")),".  "),(0,r.kt)("p",null,"Data Analysts were forced to query across multiple generations of the same data stored in the ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon S3"))," Data lake when all they actually wanted was just their current data. That meant they could not simply take legacy SQL queries and run them directly in ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena")),"."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("h5",{parentName:"li",id:"previously-too-hard--too-complex--too-time-consuming"},"PREVIOUSLY: Too hard + too complex = too time consuming..."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("img",{alt:"Fig. 1",src:a(6135).Z,width:"960",height:"540"}))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("h5",{parentName:"li",id:"firebreak-deciding-what-we-wanted-to-change"},"FIREBREAK: Deciding what we wanted to change..."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("img",{alt:"Fig. 2",src:a(3530).Z,width:"960",height:"540"}))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("h5",{parentName:"li",id:"outcome-dap-re-imagined-using-airflow--dapflow"},"OUTCOME: DAP re-imagined using ",(0,r.kt)("em",{parentName:"h5"},"Airflow")," = ",(0,r.kt)("strong",{parentName:"h5"},"DAP\u21e8flow"),"..."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("img",{alt:"Fig. 3",src:a(6069).Z,width:"960",height:"540"})))),(0,r.kt)("h4",{id:"how-dapflow-solved-our-problems"},"How ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow")," solved our problems"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Firstly, Data Analysts no longer need to convert and re-test their prototype SQL transforms to run in the separate and more complex ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Glue"))," run-time environment. "),(0,r.kt)("p",{parentName:"li"},"  Instead, ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Apache Airflow"))," can use exactly the same ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," to transform data in production with the outputs going directly into data products. So that Data Analysts' prototype SQL transform queries, that they spent time on testing until they were working, can simply be reused instead of being discarded.  "),(0,r.kt)("p",{parentName:"li"},"  ",(0,r.kt)("strong",{parentName:"p"},"That cuts development time by more than half while Data Analysts no longer need to context-switch between the two SQL dialects."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Secondly, Data Analysts no longer must adapt their legacy SQL queries to ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon S3")),"'s Data Lake partitioning architecture. "),(0,r.kt)("p",{parentName:"li"},"  Instead, ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Apache Airflow"))," is configured to generate views of the underlying table data to present Data Analysts with current-only ingested service data, both in readiness for prototyping and testing, and for when the working transforms are subsequently deployed, being automated and run by Airflow."),(0,r.kt)("p",{parentName:"li"},"  ",(0,r.kt)("strong",{parentName:"p"},"That further cuts development time while Data Analysts can very easily take the legacy SQL code from their service database system and run it directly on ",(0,r.kt)("em",{parentName:"strong"},"Amazon Athena")," with few changes.")),(0,r.kt)("p",{parentName:"li"},"  Data Analysts can also migrate their existing Athena SQL prototypes, previously adapted for the ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon S3")),"'s Data Lake partitioning architecture, because the same table history is available to them, although the table names will now be suffixed \"",(0,r.kt)("strong",{parentName:"p"},"_history"),'", which is more intuitive for new users.')),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Lastly, Data Analysts no longer need to use ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Terraform"))," for deploying their data pipeline jobs because ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Apache Airflow"))," simply takes care of that as soon as they commit their transform queries to ",(0,r.kt)("strong",{parentName:"p"},"DAP\u21e8flow"),"'s ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"GitHub"))," repository."))),(0,r.kt)("h2",{id:"onboarding"},"\ud83d\udcdaOnboarding"),(0,r.kt)("h4",{id:"a-series-onboarding-documents-is-available-here-to-help-data-analysts-get-started-with-dapflow"},"A series onboarding documents is available here, to help Data Analysts get started with ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow")),(0,r.kt)("p",null,"Anyone new to ",(0,r.kt)("strong",{parentName:"p"},"DAP\u21e8flow")," will start with ",(0,r.kt)("a",{parentName:"p",href:"../dap-airflow/onboarding/begin"},(0,r.kt)("strong",{parentName:"a"},"\ud83d\udcdaBefore you begin"))," followed by ",(0,r.kt)("a",{parentName:"p",href:"../dap-airflow/onboarding/welcome"},(0,r.kt)("strong",{parentName:"a"},"\ud83d\udcdaWelcome!")),"."),(0,r.kt)("p",null,"Thereafter, Data Analysts do not need to read every document in the order they are listed below, especially if they are already familiar with the ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"AWS Management Console"))," and have used ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," before."),(0,r.kt)("p",null,"Data Analysts are encouraged to think about what they need to do before deciding which document to read next. For example, if they have a ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"legacy SQL query"))," that they want to migrate to ",(0,r.kt)("strong",{parentName:"p"},"DAP\u21e8flow"),", they could jump straight to ",(0,r.kt)("a",{parentName:"p",href:"../dap-airflow/onboarding/prototype-legacy-transforms"},(0,r.kt)("strong",{parentName:"a"},"\ud83d\udcdaPrototype legacy transforms")),"."),(0,r.kt)("h4",{id:"we--your-feedback"},(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},'"We')," \u2661 ",(0,r.kt)("em",{parentName:"strong"},'your feedback!"'))),(0,r.kt)("p",null,"Your continuous feedback enables us to improve ",(0,r.kt)("strong",{parentName:"p"},"DAP\u21e8flow")," and our ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Data Analytics Platform"))," service. Survey links are provided at the end of each onboarding document."),(0,r.kt)("h4",{id:"below-here-is-the-full-list-of-topics-currently-on-offer"},(0,r.kt)("strong",{parentName:"h4"},"Below here, is the full list of topics currently on offer...")),(0,r.kt)("p",null,"And more topics will be added as they are ready. ",(0,r.kt)("a",{parentName:"p",href:"#coming-soon"},(0,r.kt)("strong",{parentName:"a"},"Skip to the end"))," to discover what's coming next!"),(0,r.kt)("h3",{id:"before-you-begin"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/begin"},"Before you begin")),(0,r.kt)("h4",{id:"how-do-i-get-onboard-dapflow"},"How do I get onboard ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow"),"?"),(0,r.kt)("h3",{id:"welcome"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/welcome"},"Welcome!")),(0,r.kt)("h4",{id:"what-are-my-service-terms"},"What are my ",(0,r.kt)("strong",{parentName:"h4"},"Service Terms",(0,r.kt)("inlineCode",{parentName:"strong"},"[]")),"?"),(0,r.kt)("h3",{id:"aws-console-access"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/access-the-AWS-Management-Console"},"AWS Console access")),(0,r.kt)("h4",{id:"how-will-i-access-the-aws-management-console"},"How will I access the ",(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},"AWS Management Console")),"?"),(0,r.kt)("h3",{id:"aws-region"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/access-the-AWS-region"},"AWS region")),(0,r.kt)("h4",{id:"how-will-i-ensure-i-am-in-the-correct-aws-region"},"How will I ensure I am in the correct ",(0,r.kt)("strong",{parentName:"h4"},"AWS region"),"?"),(0,r.kt)("h3",{id:"amazon-athena"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/access-my-Amazon-Athena-database"},"Amazon Athena")),(0,r.kt)("h4",{id:"how-will-i-use-amazon-athena-to-access-my-database"},"How will I use ",(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," to access my database?"),(0,r.kt)("h3",{id:"my-current-service-data"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/access-my-current-service-data"},"My current service data")),(0,r.kt)("h4",{id:"how-will-i-access-my-services-current-data-from-amazon-athena"},"How will I access my ",(0,r.kt)("inlineCode",{parentName:"h4"},"[service]"),"'s current data from ",(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena")),"?"),(0,r.kt)("h3",{id:"my-service-data-history"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/access-my-service-data-history"},"My service data history")),(0,r.kt)("h4",{id:"how-will-i-access-my-services-data-history-from-amazon-athena"},"How will I access my ",(0,r.kt)("inlineCode",{parentName:"h4"},"[service]"),"'s data history from ",(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena")),"?"),(0,r.kt)("h3",{id:"query-my-service-data"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/query-my-service-data"},"Query my service data")),(0,r.kt)("h4",{id:"how-will-i-query-and-analyze-my-services-data-with-amazon-athena"},"How will I query and analyze my ",(0,r.kt)("inlineCode",{parentName:"h4"},"[service]"),"'s data with ",(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena")),"?"),(0,r.kt)("h3",{id:"prototype-simple-transforms"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/prototype-simple-transforms"},"Prototype simple transforms")),(0,r.kt)("h4",{id:"how-can-i-use-amazon-athena-to-prototype-a-simple-table-join-data-transformation"},"How can I use ",(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," to prototype a simple table-join data transformation?"),(0,r.kt)("h3",{id:"prototype-legacy-transforms"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/prototype-legacy-transforms"},"Prototype legacy transforms")),(0,r.kt)("h4",{id:"how-do-i-use-amazon-athena-to-prototype-a-data-transformation-from-my-legacy-sql-query"},"How do I use ",(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," to prototype a data transformation from my ",(0,r.kt)("inlineCode",{parentName:"h4"},"[legacy SQL query]"),"?"),(0,r.kt)("h3",{id:"github-access"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/github-access"},"GitHub access")),(0,r.kt)("h4",{id:"how-do-i-set-up-my-github-access-for-dapflow"},"How do I set up my ",(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},"GitHub"))," access for ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow"),"?"),(0,r.kt)("h3",{id:"github-branching"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/github-branch"},"GitHub branching")),(0,r.kt)("h4",{id:"how-do-i-create-transform-branch-as-my-new-working-branch-of-dapflows-repository"},"How do I create ",(0,r.kt)("inlineCode",{parentName:"h4"},"[transform branch]")," as my new working branch of ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow"),"'s repository?"),(0,r.kt)("h3",{id:"committing-transforms"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/github-commit-transform"},"Committing transforms")),(0,r.kt)("h4",{id:"how-do-i-commit-my-working-transform-sql-to-dapflows-repository"},"How do I commit my working ",(0,r.kt)("inlineCode",{parentName:"h4"},"[transform SQL]")," to ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow"),"'s repository?"),(0,r.kt)("h3",{id:"github-pull-requests"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/github-pull-request"},"GitHub pull requests")),(0,r.kt)("h4",{id:"how-do-i-raise-a-pull-request-to-merge-my-transform-branch-into-the-main-trunk-of-the-dapflow-repository"},"How do I raise a ",(0,r.kt)("em",{parentName:"h4"},'"pull request"')," to merge my ",(0,r.kt)("inlineCode",{parentName:"h4"},"[transform branch]")," into the ",(0,r.kt)("inlineCode",{parentName:"h4"},"main")," trunk of the ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow")," repository?"),(0,r.kt)("h2",{id:"coming-soon"},"\ud83d\udcdaComing soon..."),(0,r.kt)("p",null,"The following guides are due for completion."),(0,r.kt)("h3",{id:"merging-branches"},"Merging branches"),(0,r.kt)("h4",{id:"how-do-i-complete-the-merge-of-transform-branch-into-the-main-trunk-of-dapflows-repository"},"How do i complete the merge of ",(0,r.kt)("inlineCode",{parentName:"h4"},"[transform branch]")," into the main trunk of ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow"),"'s repository?"),(0,r.kt)("h3",{id:"airflow"},"Airflow"),(0,r.kt)("h4",{id:"how-will-i-access-my-data-transforms-using-airflow-on-the-web"},"How will I access my data transforms using ",(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},"Airflow"))," on the web?"),(0,r.kt)("h3",{id:"adding-tables-to-the-raw-zone"},"Adding tables to the raw-zone"),(0,r.kt)("h4",{id:"how-do-i-add-a-new-table-ingestion-to-my-service-raw-zone-database"},"How do i add a new table ingestion to my ",(0,r.kt)("inlineCode",{parentName:"h4"},"[service raw-zone]")," database?"),(0,r.kt)("h2",{id:"suggested-for-later"},"\ud83d\udcdaSuggested for later..."),(0,r.kt)("p",null,"The following guides are on our backlog."),(0,r.kt)("h4",{id:"migrating-old-prototype-athena-sql-to-the-new-dapflow"},"Migrating old prototype Athena SQL to the new ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow")),(0,r.kt)("h4",{id:"migrating-old-production-spark-sql-to-the-new-dapflow"},"Migrating old production Spark SQL to the new ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow")),(0,r.kt)("h4",{id:"refined-zone-views"},"Refined-zone views"),(0,r.kt)("h4",{id:"external-access-to-dapflow-products"},"External access to ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow")," products"),(0,r.kt)("h4",{id:"removing-tables-from-my-service-raw-zone-database"},"Removing tables from my ",(0,r.kt)("inlineCode",{parentName:"h4"},"[service raw-zone]")," database"),(0,r.kt)("h4",{id:"removing-products-from-the-service-refined-zone-database"},"Removing products from the ",(0,r.kt)("inlineCode",{parentName:"h4"},"[service refined-zone]")," database"),(0,r.kt)("br",null),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow",src:a(4621).Z,width:"750",height:"800"}),"  "),(0,r.kt)("br",null),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow",src:a(2987).Z,width:"1812",height:"700"}),"  "),(0,r.kt)("br",null),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow",src:a(2530).Z,width:"768",height:"232"})))}c.isMDXComponent=!0},2987:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/AirflowLogo-b028904451fee565e7d778ca7187430f.png"},2530:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/AmazonAthenaLogo-e58d9d0c441123494e04bae050fdd85f.png"},4673:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/DAPairflowFLOWwide-dc996eb46ef834a0086f5996af589442.png"},6135:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/introduction-one-5396d6f7f5656b221d50e9101c0889e3.png"},6069:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/introduction-three-c368afe102c0aabfbf83d81fe6be1876.png"},3530:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/introduction-two-b2320445059b22995ae6826c17aa4862.png"},4621:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/worker_tap_valve_800_wht-c34645452f188ce11a1b52ad32c6543c.jpg"}}]);