"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[7693],{3905:(e,t,o)=>{o.d(t,{Zo:()=>d,kt:()=>m});var a=o(7294);function n(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function r(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,a)}return o}function i(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?r(Object(o),!0).forEach((function(t){n(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):r(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function l(e,t){if(null==e)return{};var o,a,n=function(e,t){if(null==e)return{};var o,a,n={},r=Object.keys(e);for(a=0;a<r.length;a++)o=r[a],t.indexOf(o)>=0||(n[o]=e[o]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)o=r[a],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(n[o]=e[o])}return n}var s=a.createContext({}),u=function(e){var t=a.useContext(s),o=t;return e&&(o="function"==typeof e?e(t):i(i({},t),e)),o},d=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},p="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var o=e.components,n=e.mdxType,r=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),p=u(o),h=n,m=p["".concat(s,".").concat(h)]||p[h]||g[h]||r;return o?a.createElement(m,i(i({ref:t},d),{},{components:o})):a.createElement(m,i({ref:t},d))}));function m(e,t){var o=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=o.length,i=new Array(r);i[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:n,i[1]=l;for(var u=2;u<r;u++)i[u]=o[u];return a.createElement.apply(null,i)}return a.createElement.apply(null,o)}h.displayName="MDXCreateElement"},7994:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>g,frontMatter:()=>r,metadata:()=>l,toc:()=>u});var a=o(7462),n=(o(7294),o(3905));const r={id:"using-glue-studio",title:"Using Glue Studio",description:"Using AWS Glue Studio to create ETL processes.",layout:"playbook_js",tags:["playbook"]},i=void 0,l={unversionedId:"playbook/transforming-data/using-aws-glue/using-glue-studio",id:"playbook/transforming-data/using-aws-glue/using-glue-studio",title:"Using Glue Studio",description:"Using AWS Glue Studio to create ETL processes.",source:"@site/docs/playbook/transforming-data/using-aws-glue/001-using-glue-studio.md",sourceDirName:"playbook/transforming-data/using-aws-glue",slug:"/playbook/transforming-data/using-aws-glue/using-glue-studio",permalink:"/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/using-glue-studio",draft:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/playbook/transforming-data/using-aws-glue/001-using-glue-studio.md",tags:[{label:"playbook",permalink:"/Data-Platform-Playbook/tags/playbook"}],version:"current",sidebarPosition:1,frontMatter:{id:"using-glue-studio",title:"Using Glue Studio",description:"Using AWS Glue Studio to create ETL processes.",layout:"playbook_js",tags:["playbook"]},sidebar:"docs",previous:{title:"Guide to unit testing on the Data Platform",permalink:"/Data-Platform-Playbook/playbook/transforming-data/guides-to-testing-in-the-platform/unit-testing-guide"},next:{title:"Deploying Glue jobs to the Data Platform",permalink:"/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/deploy-glue-jobs"}},s={},u=[{value:"Using AWS Glue Studio to create ETL processes",id:"using-aws-glue-studio-to-create-etl-processes",level:2},{value:"Creating a new Glue job",id:"creating-a-new-glue-job",level:2},{value:"Note: Exporting Data",id:"note-exporting-data",level:3},{value:"Clone and edit an existing Glue job",id:"clone-and-edit-an-existing-glue-job",level:2},{value:"Pre-filter your source data for a glue job",id:"pre-filter-your-source-data-for-a-glue-job",level:2},{value:"Monitoring a glue job run",id:"monitoring-a-glue-job-run",level:2},{value:"Continuous logging",id:"continuous-logging",level:3},{value:"Enabling continuous logging",id:"enabling-continuous-logging",level:4},{value:"Viewing logs with continuous logging enabled",id:"viewing-logs-with-continuous-logging-enabled",level:4},{value:"When a glue job errors",id:"when-a-glue-job-errors",level:3},{value:"Viewing error logs",id:"viewing-error-logs",level:4},{value:"Finding the error in the error logs",id:"finding-the-error-in-the-error-logs",level:4},{value:"Receive email notifications when Glue jobs fail",id:"receive-email-notifications-when-glue-jobs-fail",level:2}],d={toc:u},p="wrapper";function g(e){let{components:t,...r}=e;return(0,n.kt)(p,(0,a.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h2",{id:"using-aws-glue-studio-to-create-etl-processes"},"Using AWS Glue Studio to create ETL processes"),(0,n.kt)("admonition",{type:"important"},(0,n.kt)("p",{parentName:"admonition"},"The aim of this guide is to help you get started in creating and testing your Glue jobs."),(0,n.kt)("p",{parentName:"admonition"},"Once your Glue job is ready and working as expected, you can refer to ",(0,n.kt)("a",{parentName:"p",href:"./deploy-glue-jobs"},"this guide"),"\nto deploy your Glue job to the Data Platform Production environment. You should keep your original Glue job (which was created in Glue Studio) should you need to make further improvements to it in the future\nand refer to the note at the bottom of ",(0,n.kt)("a",{parentName:"p",href:"./deploy-glue-jobs#1-add-your-script-to-the-data-platform-project-using-the-github-ui"},"section 1")," of ",(0,n.kt)("strong",{parentName:"p"},"Deploying Glue jobs to the Data Platform"),", to re-deploy the new version to the Data Platform Production environment.")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/ug/what-is-glue-studio.html"},"AWS Glue Studio")," is a new graphical interface that makes it easy to create, run, and monitor extract, transform, and load (ETL) jobs in AWS Glue."),(0,n.kt)("p",null,"To use AWS Glue Studio the job needs to be created manually. This guide includes the steps needed to create an editable job in Glue Studio. Templates may have already been created and existing jobs can be duplicated to make it easier to get started with Glue Studio. Dynamically created scripts using ",(0,n.kt)("a",{parentName:"p",href:"https://www.terraform.io/"},"terraform")," (the infrastructure as code tool Hackney use) will not have the ability to use the visual editor."),(0,n.kt)("p",null,"Note: If a job has already been created manually you can select the job using the radio button and in the job ",(0,n.kt)("em",{parentName:"p"},"Actions")," mennu, select ",(0,n.kt)("strong",{parentName:"p"},"Clone job"),"."),(0,n.kt)("h2",{id:"creating-a-new-glue-job"},"Creating a new Glue job"),(0,n.kt)("p",null,"Note: The instructions below assume an S3 Data Source and Target Location."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"Log in to AWS Management Console."),(0,n.kt)("li",{parentName:"ol"},"Select the ",(0,n.kt)("strong",{parentName:"li"},"AWS Glue")," service from the services menu (or search field)."),(0,n.kt)("li",{parentName:"ol"},"From the AWS Glue menu ETL section, select ",(0,n.kt)("strong",{parentName:"li"},"AWS Glue Studio"),"."),(0,n.kt)("li",{parentName:"ol"},"From the AWS Glue Studio main page, choose the ",(0,n.kt)("strong",{parentName:"li"},"Create and manage jobs")," option."),(0,n.kt)("li",{parentName:"ol"},"Within the ",(0,n.kt)("em",{parentName:"li"},"Create Job")," section, select ",(0,n.kt)("strong",{parentName:"li"},"Source and target added to the graph")," and select desired Source and Target values."),(0,n.kt)("li",{parentName:"ol"},"Click the ",(0,n.kt)("strong",{parentName:"li"},"Create")," button."),(0,n.kt)("li",{parentName:"ol"},"In the Visual editor that is now displayed, click on the ",(0,n.kt)("em",{parentName:"li"},"Data Source - S3")," box and in the ",(0,n.kt)("em",{parentName:"li"},"Data source properties - S3")," tab to set ",(0,n.kt)("em",{parentName:"li"},"S3 source type")," to ",(0,n.kt)("strong",{parentName:"li"},"S3 location")," and set the ",(0,n.kt)("em",{parentName:"li"},"S3 URL")," for the source data. To reduce the time your job takes to run, you can follow ",(0,n.kt)("a",{parentName:"li",href:"#pre-filter-your-source-data-for-a-glue-job"},"these steps")," to pre-filter the source data passed to your transformation step."),(0,n.kt)("li",{parentName:"ol"},"Also ",(0,n.kt)("em",{parentName:"li"},"Data Target - S3")," box and in the ",(0,n.kt)("em",{parentName:"li"},"Data target properties - S3")," tab set the ",(0,n.kt)("em",{parentName:"li"},"S3 URL")," for the data target, usually your department folder in the Data Platform account.\n",(0,n.kt)("em",{parentName:"li"},"NB: You can add additional folders at this point for your new data within your department folder. Each department has an unrestricted directory within each zone where unrestricted datasets can be stored, in order to add further cross-department insights and maintain datasets. There is also an unrestricted department which allows users to maintain datasets which don't have access restrictions across each zone"),(0,n.kt)("strong",{parentName:"li"},"Data source and data target (amongst other operations) must be set to be able to save the job. You can also apply ",(0,n.kt)("em",{parentName:"strong"},"Transformations")," specific to your job via the Visual tab. See the ",(0,n.kt)("a",{parentName:"strong",href:"https://docs.aws.amazon.com/glue/latest/ug/edit-nodes-chapter.html"},"AWS Glue Studio Documentation")),"."),(0,n.kt)("li",{parentName:"ol"},"To complete the set up you need to select the ",(0,n.kt)("strong",{parentName:"li"},"Job details")," tab."),(0,n.kt)("li",{parentName:"ol"},"Complete the ",(0,n.kt)("em",{parentName:"li"},"Name")," and optional ",(0,n.kt)("em",{parentName:"li"},"Description")," fields. You may use this job as a template for repeat use, so a generic name to use as a template might be useful to begin with."),(0,n.kt)("li",{parentName:"ol"},"Select ",(0,n.kt)("strong",{parentName:"li"},"dataplatform-{environment}-glue-role")," where environment is either 'stg' or 'prod'; as the ",(0,n.kt)("em",{parentName:"li"},"IAM Role")," for the job."),(0,n.kt)("li",{parentName:"ol"},"The remaining standard fields default values are usually fine to use. The programming language Glue Studio creates can be Scala or Python."),(0,n.kt)("li",{parentName:"ol"},"In ",(0,n.kt)("em",{parentName:"li"},"Advanced properties")," check the ",(0,n.kt)("em",{parentName:"li"},"Script filename")," refers to the task being carried out."),(0,n.kt)("li",{parentName:"ol"},"Set the ",(0,n.kt)("em",{parentName:"li"},"Script path")," to the central scripts S3 bucket: ",(0,n.kt)("inlineCode",{parentName:"li"},"s3://dataplatform-{environment}-glue-scripts/custom/")," where environment is either 'stg' or 'prod' - you can create new folders or specify existing folders in the S3 bucket like this: ",(0,n.kt)("inlineCode",{parentName:"li"},"s3://dataplatform-{environment}-glue-scripts/custom/YOUR_FOLDER_NAME/"),"."),(0,n.kt)("li",{parentName:"ol"},"Set the ",(0,n.kt)("em",{parentName:"li"},"Temporary path")," to the central temp storage S3 bucket: ",(0,n.kt)("inlineCode",{parentName:"li"},"s3://dataplatform-{environment}-glue-temp-storage/")," where environment is either 'stg' or 'prod'."),(0,n.kt)("li",{parentName:"ol"},"In ",(0,n.kt)("em",{parentName:"li"},"Security configuration")," select the appropriate security configuration for your target bucket location (for example for the Raw Zone, use ",(0,n.kt)("strong",{parentName:"li"},"glue-job-security-configuration-to-raw"),")."),(0,n.kt)("li",{parentName:"ol"},"Ensure the ",(0,n.kt)("em",{parentName:"li"},"Server-side encryption")," option is ",(0,n.kt)("strong",{parentName:"li"},"not checked"),", so that it uses the buckets default encryption configuration."),(0,n.kt)("li",{parentName:"ol"},"In the ",(0,n.kt)("em",{parentName:"li"},"Tags")," section, add the key ",(0,n.kt)("inlineCode",{parentName:"li"},"PlatformDepartment")," and set the value as the name of your department.\nThis should be the same name which was used to set up the department on the ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/terraform/core/05-departments.tf"},"Data Platform"),"."),(0,n.kt)("li",{parentName:"ol"},"Add any further libraries or parameters you need in the ",(0,n.kt)("em",{parentName:"li"},"Advanced properties")," section.")),(0,n.kt)("h3",{id:"note-exporting-data"},"Note: Exporting Data"),(0,n.kt)("p",null,"When exporting data from your Glue Job it is important that you follow the Data Platforms partitioning stratagy. To do this, exports must be exported with the partitions import_year, import_month, import_day & import_date at a minimum. To accomplish this using AWS Glue Studio use the ",(0,n.kt)("inlineCode",{parentName:"p"},"Add a partition")," key option at the bottom of the Data target node. It is important to further note, that inorder to add these attributes as partitions, they must exist within the dataset as fields."),(0,n.kt)("h2",{id:"clone-and-edit-an-existing-glue-job"},"Clone and edit an existing Glue job"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"In the ",(0,n.kt)("em",{parentName:"li"},"Your jobs")," sections, select the radio button of the job you would like to clone."),(0,n.kt)("li",{parentName:"ol"},"In the ",(0,n.kt)("em",{parentName:"li"},"Actions")," dropdown, select ",(0,n.kt)("em",{parentName:"li"},"Clone job"),". If there is an information banner with a message ",(0,n.kt)("em",{parentName:"li"},"Your job was created outside of Glue Studio and has no DAG available")," (see screenshot below) there will not be a Visual tab and therefore you will not have an interface to edit your job. Instead you will need to edit your job in the python job script. To do this navigate to the ",(0,n.kt)("em",{parentName:"li"},"Script")," tab. If you prefer to use the interface, then select another job which has a Visual tab or create a new job as instructed in the ",(0,n.kt)("em",{parentName:"li"},"Creating a new Glue job")," section above.\n",(0,n.kt)("img",{parentName:"li",src:"https://user-images.githubusercontent.com/46002877/114702251-bcac0500-9d1b-11eb-8475-814523fd58dc.png",alt:"image"})),(0,n.kt)("li",{parentName:"ol"},"In the Job Details tab, update the ",(0,n.kt)("em",{parentName:"li"},"Name")," for your new job and check the ",(0,n.kt)("em",{parentName:"li"},"Creating a new Glue job")," seciton above to ensure the configuration suits your needs."),(0,n.kt)("li",{parentName:"ol"},"To save your job, select the ",(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("em",{parentName:"strong"},"Save"))," button.")),(0,n.kt)("h2",{id:"pre-filter-your-source-data-for-a-glue-job"},"Pre-filter your source data for a glue job"),(0,n.kt)("p",null,"By default, AWS glue will load all source data for a job before running the transformation steps.\nIf the transformation is only interested in a subset of the data partitions, you can use a pushdown predicate to tell the glue job only to consider these partitions.\nIf you have a large data set, this could have substantial performance benefits for your glue jobs (i.e. it will reduce the time it takes them to run)."),(0,n.kt)("p",null,"Follow these steps to set this in Glue Studio."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"Navigate to the visual tab when viewing your glue job in glue studio."),(0,n.kt)("li",{parentName:"ol"},'Select a "Data source" node.'),(0,n.kt)("li",{parentName:"ol"},'Staying in the "Data source properties - S3" tab, you should see the field "Partition predicate - optional", enter your predicate expression here.')),(0,n.kt)("p",null,"The predicate must be Spark SQL syntax; Glue Studio provides an example of a predicate in the UI.\nHere is an example that only keeps records where the import_date (a standard partition used within the Data Platform) is within the last seven days."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"  to_date(import_date, 'yyyyMMdd') >= date_sub(current_date, 7)\n")),(0,n.kt)("p",null,"For further reading around pushdown predicates and partitioned data in Glue jobs, AWS have some ",(0,n.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html"},"documentation")," on their use in AWS Glue ETL's and a ",(0,n.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/work-with-partitioned-data-in-aws-glue/"},"blog post about working with partitioned data"),"."),(0,n.kt)("h2",{id:"monitoring-a-glue-job-run"},"Monitoring a glue job run"),(0,n.kt)("p",null,"We recommend that you enable continuous logging on your glue jobs, you can read more about this in ",(0,n.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/dg/monitor-continuous-logging.html"},"AWS's documentation"),".\nEnabling continuous logging will allow you to ",(0,n.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/dg/monitor-continuous-logging-view.html"},"view logs")," after your glue job has started. "),(0,n.kt)("h3",{id:"continuous-logging"},"Continuous logging"),(0,n.kt)("h4",{id:"enabling-continuous-logging"},"Enabling continuous logging"),(0,n.kt)("p",null,'To enable continous logging you need to check the "Continuous logging" box in the "Advanced properties" section of the job details.\n',(0,n.kt)("img",{alt:"continous looging checkbox",src:o(2302).Z,width:"804",height:"164"})),(0,n.kt)("h4",{id:"viewing-logs-with-continuous-logging-enabled"},"Viewing logs with continuous logging enabled"),(0,n.kt)("p",null,"To view the logs, navigate to the ",(0,n.kt)("inlineCode",{parentName:"p"},"Runs"),' tab and click on "All logs" under "Cloudwatch logs" in the run details, highlighted in the image below.\n',(0,n.kt)("img",{alt:"glue job run details",src:o(7431).Z,width:"2594",height:"956"}),"\nThis will take you to the list of logs streams for that job run.\nThere will be one for each executor of the job, including the driver, most of the time the driver logs will hold the most helpful information.\nThe driver logs are in the stream postfixed by -driver, like in the screenshot below.\n",(0,n.kt)("img",{alt:"list of log streams for all logs",src:o(7250).Z,width:"2261",height:"608"}),"\nYou can click on this log stream to see the logs. "),(0,n.kt)("h3",{id:"when-a-glue-job-errors"},"When a glue job errors"),(0,n.kt)("p",null,"Even without continuous logging you can still view the full error output after a glue job has failed.\nOn the run details page it will only show a truncated version of the error message, often hiding the underlying error."),(0,n.kt)("h4",{id:"viewing-error-logs"},"Viewing error logs"),(0,n.kt)("p",null,'To view the error logs click on "Error logs" under "Cloudwatch logs" in the run details, highlighted in the ',(0,n.kt)("a",{parentName:"p",href:"#viewing-logs-with-continuous-logging-enabled"},"image above"),".\nThis will take you to a list of logs streams for that job run.\nThere will be one for each executor of the job, including the driver, most of the time the driver logs will hold the most helpful information.\nYou can open up the logs for the driver by selecting the log stream which isn't postfixed by a second identifier.\nThe driver log stream is highlighted in the sreenshot below.\n",(0,n.kt)("img",{alt:"list of log streams for errors",src:o(2202).Z,width:"2290",height:"492"}),"\nYou may need to expand the log stream column size in order to see this.\nClick on the driver logs to see the full error output and stack trace."),(0,n.kt)("h4",{id:"finding-the-error-in-the-error-logs"},"Finding the error in the error logs"),(0,n.kt)("p",null,'There will be a lot of logs, most will be be for information, marked "INFO", you should looks at the ones marked with "ERROR".\nThere may still be a couple marked "ERROR" so you should consider them all when looking for the error that caused your glue job to fail.'),(0,n.kt)("p",null,"For an example, if you ",(0,n.kt)("a",{parentName:"p",href:"https://hackney.awsapps.com/start#/"},"login to the pre-production AWS account")," then navigate to ",(0,n.kt)("a",{parentName:"p",href:"https://eu-west-2.console.aws.amazon.com/cloudwatch/home?region=eu-west-2#logsV2:log-groups/log-group/$252Faws-glue$252Fjobs$252Fdataplatform-stg-config-to-refined-role$252Fdataplatform-stg-glue-parking$252Ferror/log-events/jr_e7713ac7cf766da842d6e2e84479d508bda6013b28dc409344a77e6af796aa5b_attempt_1"},"these logs"),'.\nLooking from the bottom of the logs you first come accross an ERROR log that is mentioning an S3 Access Denied error (you can click the little arrow on the left to expand the message).\nThis is an error with pushing the logs and isn\'t the reason the glue job failed.\nIf you then scroll up a bit to the next ERROR, it contains a message that mentions "Error from Python:Traceback " and then follows on to give the stacktrace for the SQL error "DataType varchar is not supported.(line 54, pos 45)".\nThis is the error which has failed the glue job.'),(0,n.kt)("h2",{id:"receive-email-notifications-when-glue-jobs-fail"},"Receive email notifications when Glue jobs fail"),(0,n.kt)("p",null,"Each time a Glue job fails, an email notification with details of the error message is sent to the respective department, and their subscribed members."),(0,n.kt)("p",null,"The message will include:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Name of the Glue job"),(0,n.kt)("li",{parentName:"ul"},"Error message"),(0,n.kt)("li",{parentName:"ul"},"Time of failure"),(0,n.kt)("li",{parentName:"ul"},"Job start time"),(0,n.kt)("li",{parentName:"ul"},"Job end time"),(0,n.kt)("li",{parentName:"ul"},"Job last modified time"),(0,n.kt)("li",{parentName:"ul"},"A link to log in to Hackney SSO and view the Job run details")),(0,n.kt)("p",null,"In order to receive email notifications, you will need to ensure that you are subscribed to receive emails from your department's ",(0,n.kt)("a",{parentName:"p",href:"https://groups.google.com/my-groups"},"Google group")," and that you have confirmed your subscription to receive AWS Notifications when prompted."),(0,n.kt)("p",null,"Additionally, Spark Web UI is used to monitior and debug the glue jobs. Every 30 seconds, AWS Glue flushes the Spark event logs to an S3 bucket titled Spark UI Bucket."),(0,n.kt)("admonition",{type:"important"},(0,n.kt)("p",{parentName:"admonition"},"Ensure the ",(0,n.kt)("strong",{parentName:"p"},"PlatformDepartment")," tag is correctly set in the ",(0,n.kt)("em",{parentName:"p"},"Advanced details")," section in the Glue job's ",(0,n.kt)("em",{parentName:"p"},"Job Details")," (see ",(0,n.kt)("a",{parentName:"p",href:"#creating-a-new-glue-job"},"Creating a new Glue job")," section above).")))}g.isMDXComponent=!0},7250:(e,t,o)=>{o.d(t,{Z:()=>a});const a=o.p+"assets/images/glue_all_log_streams-ee80964cac77c35a7868df990d21acb0.png"},2202:(e,t,o)=>{o.d(t,{Z:()=>a});const a=o.p+"assets/images/glue_error_log_streams-31776ca3709521c433aa33ff00843d33.png"},7431:(e,t,o)=>{o.d(t,{Z:()=>a});const a=o.p+"assets/images/glue_run_details-520527963baa9712f891b41dff95b501.png"},2302:(e,t,o)=>{o.d(t,{Z:()=>a});const a=o.p+"assets/images/glue_studio_cont_log_checkbox-8dd5bb41f25b1dc618ffd207b8615093.png"}}]);