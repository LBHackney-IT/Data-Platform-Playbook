"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[8346],{3905:function(e,t,a){a.d(t,{Zo:function(){return u},kt:function(){return c}});var o=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,o,n=function(e,t){if(null==e)return{};var a,o,n={},i=Object.keys(e);for(o=0;o<i.length;o++)a=i[o],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)a=i[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=o.createContext({}),d=function(e){var t=o.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},u=function(e){var t=d(e.components);return o.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),h=d(a),c=n,m=h["".concat(l,".").concat(c)]||h[c]||p[c]||i;return a?o.createElement(m,r(r({ref:t},u),{},{components:a})):o.createElement(m,r({ref:t},u))}));function c(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,r=new Array(i);r[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:n,r[1]=s;for(var d=2;d<i;d++)r[d]=a[d];return o.createElement.apply(null,r)}return o.createElement.apply(null,a)}h.displayName="MDXCreateElement"},3024:function(e,t,a){a.r(t),a.d(t,{assets:function(){return u},contentTitle:function(){return l},default:function(){return c},frontMatter:function(){return s},metadata:function(){return d},toc:function(){return p}});var o=a(3117),n=a(102),i=(a(7294),a(3905)),r=["components"],s={title:"Optimizing Glue jobs",description:"Elements for optimizing glue jobs",layout:"playbook_js",tags:["playbook"]},l=void 0,d={unversionedId:"playbook/transforming-data/using-aws-glue/optimizing-glue-jobs",id:"playbook/transforming-data/using-aws-glue/optimizing-glue-jobs",title:"Optimizing Glue jobs",description:"Elements for optimizing glue jobs",source:"@site/docs/playbook/transforming-data/using-aws-glue/optimizing-glue-jobs.md",sourceDirName:"playbook/transforming-data/using-aws-glue",slug:"/playbook/transforming-data/using-aws-glue/optimizing-glue-jobs",permalink:"/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/optimizing-glue-jobs",draft:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/playbook/transforming-data/using-aws-glue/optimizing-glue-jobs.md",tags:[{label:"playbook",permalink:"/Data-Platform-Playbook/tags/playbook"}],version:"current",frontMatter:{title:"Optimizing Glue jobs",description:"Elements for optimizing glue jobs",layout:"playbook_js",tags:["playbook"]},sidebar:"docs",previous:{title:"Prototyping glue jobs in a notebook",permalink:"/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/using-sagemaker"},next:{title:"Scheduling Liberator Glue Jobs",permalink:"/Data-Platform-Playbook/playbook/transforming-data/using-aws-glue/practical-examples/scheduling-liberator-glue-jobs"}},u={},p=[{value:"Filtering data before creating the dataframe",id:"filtering-data-before-creating-the-dataframe",level:2},{value:"Job bookmarks",id:"job-bookmarks",level:2},{value:"How it works",id:"how-it-works",level:3},{value:"Pros",id:"pros",level:3},{value:"Cons",id:"cons",level:3},{value:"Scenarios when not to use it",id:"scenarios-when-not-to-use-it",level:3},{value:"How to use it in a job",id:"how-to-use-it-in-a-job",level:3},{value:"External doc about job bookmarks",id:"external-doc-about-job-bookmarks",level:3},{value:"Pushdown predicates",id:"pushdown-predicates",level:2},{value:"Pushdown predicate based on the current date + a few days buffer",id:"pushdown-predicate-based-on-the-current-date--a-few-days-buffer",level:3},{value:"Pros",id:"pros-1",level:4},{value:"Cons",id:"cons-1",level:4},{value:"Scenarios when not to use it",id:"scenarios-when-not-to-use-it-1",level:4},{value:"How to use it in a job",id:"how-to-use-it-in-a-job-1",level:4},{value:"Pushdown predicate based on the last date partition from the Glue catalogue",id:"pushdown-predicate-based-on-the-last-date-partition-from-the-glue-catalogue",level:3},{value:"Pros",id:"pros-2",level:4},{value:"Cons",id:"cons-2",level:4},{value:"Scenarios when not to use it",id:"scenarios-when-not-to-use-it-2",level:4},{value:"How to use it in a job",id:"how-to-use-it-in-a-job-2",level:4},{value:"External documentation about pushdown predicates",id:"external-documentation-about-pushdown-predicates",level:3},{value:"Filtering data after creating the dataframe",id:"filtering-data-after-creating-the-dataframe",level:2},{value:"Pros",id:"pros-3",level:3},{value:"Cons",id:"cons-3",level:3},{value:"How to use it in a job",id:"how-to-use-it-in-a-job-3",level:3},{value:"Conclusion",id:"conclusion",level:2}],h={toc:p};function c(e){var t=e.components,s=(0,n.Z)(e,r);return(0,i.kt)("wrapper",(0,o.Z)({},h,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"This article proposes a few principles to follow so that Glue jobs don\u2019t run unnecessarily slowly."),(0,i.kt)("h1",{id:"making-sure-the-job-processes-the-minimum-amount-of-data"},"Making sure the job processes the minimum amount of data"),(0,i.kt)("p",null,"Most of the time, we only are interested in the latest partition of the source data.\nWhen working from the ",(0,i.kt)("em",{parentName:"p"},"Trusted zone"),", most of the time, only the latest data is available so there is nothing to do. But when working from the refined or the raw zone, historical partitions exist that we need to discard. This section describes several ways to do this."),(0,i.kt)("p",null,"As shown on the picture below, the typical job first loads some data from S3 using ",(0,i.kt)("inlineCode",{parentName:"p"},"Execution_context.get_dataframe")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"glueContext.create_dynamic_frame.from_catalog"),", and then filters it down to only keep the latest records using ",(0,i.kt)("inlineCode",{parentName:"p"},"df.get_latest_partition")," or\n",(0,i.kt)("inlineCode",{parentName:"p"},"df.get_latest_partition_optimized"),"."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Typical steps when loading and processing data from S3",src:a(5907).Z,width:"2170",height:"1216"})),(0,i.kt)("p",null,"There are opportunities to filter data at both stages: before creating the dataframe and afterwards. Options are described below."),(0,i.kt)("h2",{id:"filtering-data-before-creating-the-dataframe"},"Filtering data before creating the dataframe"),(0,i.kt)("p",null,"In this section we\u2019ll explore job bookmarks and pushdown predicates."),(0,i.kt)("h2",{id:"job-bookmarks"},"Job bookmarks"),(0,i.kt)("h3",{id:"how-it-works"},"How it works"),(0,i.kt)("p",null,"Job Bookmark is a Glue feature that operates at file level. It completely ignores partitions.\nWith Bookmark on, the Glue job will only load files that have changed or have been created in the source bucket/folder since the last successful run. It will result in a smaller dataframe."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"center"},(0,i.kt)("img",{alt:"Loading and processing data from S3 using Glue job bookmarks",src:a(1444).Z,width:"2196",height:"1214"})))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"center"},(0,i.kt)("em",{parentName:"td"},"In this example, since the last job run, one additional file has been created on the 4/10/2022 and 2 on the 5/10/2022. These 3 files are in different partitions but the bookmarks ignores this fact. The 3 files will get loaded into the same dataframe and processed in the next job run."))))),(0,i.kt)("h3",{id:"pros"},"Pros"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Glue built-in feature."),(0,i.kt)("li",{parentName:"ul"},"Rewind, reset, disable the bookmark in the Glue console without touching the script."),(0,i.kt)("li",{parentName:"ul"},"The bookmark does not rely on crawlers, partitions or catalogue."),(0,i.kt)("li",{parentName:"ul"},"Fine grained filtering: if a new file comes in the middle of the day while others have been processed a few hours earlier, you can run your job and only process the new one. You cannot do that if filtering at partition level.")),(0,i.kt)("h3",{id:"cons"},"Cons"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"If an old file changes in the source bucket, it will be processed, whatever partition it is in. To prevent this, additional precautions may be taken (extra filtering using SQL or pushdown predicate)."),(0,i.kt)("li",{parentName:"ul"},"Job bookmarks are not very transparent. It is difficult to know what was the last file processed. "),(0,i.kt)("li",{parentName:"ul"},"Users may not know how, or not have permissions to, reset or rewind the bookmark (it used to be only accessible in the legacy pages). ")),(0,i.kt)("h3",{id:"scenarios-when-not-to-use-it"},"Scenarios when not to use it"),(0,i.kt)("p",null,"Bookmarks are not very convenient for a test job that is meant to process several times the same data. Not great if you have several data sources with different filtering requirements: you can choose to use the bookmark or not for each source (using the transformation_ctx in the loading block), but you cannot rewind or reset the bookmark for only one source."),(0,i.kt)("h3",{id:"how-to-use-it-in-a-job"},"How to use it in a job"),(0,i.kt)("p",null,"Enabling bookmarks requires 2 steps."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"In the job parameters or in Terraform: Use the standard job parameter ",(0,i.kt)("inlineCode",{parentName:"li"},"bookmark=enable")," (It is disabled by default in Glue console and in our Job terraform module).")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Enabling job bookmarks in Terraform",src:a(4013).Z,width:"2272",height:"1306"})),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"In the job script: for incremental data sources that need bookmarking, set the ",(0,i.kt)("inlineCode",{parentName:"li"},"transformation_ctx")," to a unique string value when creating the data frame. For data sources that don't change and need to be processed each time, don't set a ",(0,i.kt)("inlineCode",{parentName:"li"},"transformation_ctx")," and the bookmark won't apply.")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Setting the transformation_ctx for job bookmarks",src:a(9691).Z,width:"1066",height:"230"})),(0,i.kt)("h3",{id:"external-doc-about-job-bookmarks"},"External doc about job bookmarks"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html"},"https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://medium.com/analytics-vidhya/implementing-glue-etl-job-with-job-bookmarks-b76a8ba38dc8"},"https://medium.com/analytics-vidhya/implementing-glue-etl-job-with-job-bookmarks-b76a8ba38dc8"),"\nDatasets with different update cycles: ",(0,i.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/process-data-with-varying-data-ingestion-frequencies-using-aws-glue-job-bookmarks/"},"https://aws.amazon.com/blogs/big-data/process-data-with-varying-data-ingestion-frequencies-using-aws-glue-job-bookmarks/")),(0,i.kt)("h2",{id:"pushdown-predicates"},"Pushdown predicates"),(0,i.kt)("p",null,"When using a pushdown predicate, Glue will only load partitions (S3 folders) meeting the predicate.\nThis example assumes import_date is a partition key: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"df = glueContext.create_dynamic_frame.from_catalog(\n            name_space=database_name,\n            table_name=table_name,\n            push_down_predicate = \"import_date=='20221001'\")\n")),(0,i.kt)("p",null,"This statement results in a smaller dataframe."),(0,i.kt)("p",null,"How to create the pushdown predicate in a script? Using the DP helpers, there are 2 ways to set the date dynamically:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Using the current date and adding a few days buffer before this date (i.e. loading everything in the last n days)"),(0,i.kt)("li",{parentName:"ul"},"Using the latest partition date by checking the Glue catalogue (i.e. loading the latest written data, whatever its age is)")),(0,i.kt)("p",null,"These 2 approaches and their pros/cons are described below."),(0,i.kt)("h3",{id:"pushdown-predicate-based-on-the-current-date--a-few-days-buffer"},"Pushdown predicate based on the current date + a few days buffer"),(0,i.kt)("p",null,"This methos loads the current day's partition + the n previous ones."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"center"},(0,i.kt)("img",{alt:"Loading and processing data from S3 using a pushdown predicate with a 1 day buffer",src:a(5122).Z,width:"2398",height:"1352"})))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"center"},(0,i.kt)("em",{parentName:"td"},"In this example, we have 3 partitions for 3 different import_dates. The job runs on the 5/10/2022. Because of the pushdown predicate wih buffer, it will load and process data from the same day's partition, + 1 previous day."))))),(0,i.kt)("h4",{id:"pros-1"},"Pros"),(0,i.kt)("p",null,"This approach gives you a security buffer when you're not sure which is the latest non-empty partition. For instance, if a job runs every day except from the weekend, a 2 days buffer will ensure you always load some data, even on a Monday morning. A 1 day buffer is also useful if you\u2019re not sure if the source data is produced before or after midnight."),(0,i.kt)("h4",{id:"cons-1"},"Cons"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"This method can be expensive (i.e. load more data than needed) if you want a large buffer."),(0,i.kt)("li",{parentName:"ul"},"You can miss data if there is a longer gap than expected in the catalogue")),(0,i.kt)("h4",{id:"scenarios-when-not-to-use-it-1"},"Scenarios when not to use it"),(0,i.kt)("p",null,"This is not suitable if the data source comes very irregularly, because you may not know which size of buffer to use."),(0,i.kt)("h4",{id:"how-to-use-it-in-a-job-1"},"How to use it in a job"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Import the helper function called ",(0,i.kt)("inlineCode",{parentName:"li"},"create_pushdown_predicate"),"."),(0,i.kt)("li",{parentName:"ol"},"Call the ",(0,i.kt)("inlineCode",{parentName:"li"},"create_pushdown_predicate()")," method in the ",(0,i.kt)("inlineCode",{parentName:"li"},"push_down_predicate")," option of the ",(0,i.kt)("inlineCode",{parentName:"li"},"createDataFrame")," block. Pass the name of the partition column as the first argument and the number of days before the current date as the second argument. For instance, to load the data written in the last 7 days, write:")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Write a pushdown predicate with a 7 days buffer",src:a(5535).Z,width:"1742",height:"218"})),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Warning"),": a buffer size of 0 means that you\u2019re loading the full dataset."),(0,i.kt)("ol",{start:3},(0,i.kt)("li",{parentName:"ol"},"Later in your script, you can use ",(0,i.kt)("inlineCode",{parentName:"li"},"get_latest_partitions()")," on the resulting dataframe to only keep one day's worth of data.")),(0,i.kt)("h3",{id:"pushdown-predicate-based-on-the-last-date-partition-from-the-glue-catalogue"},"Pushdown predicate based on the last date partition from the Glue catalogue"),(0,i.kt)("p",null,"With this method, a helper queries the Glue catalogue with boto3 to get the latest partition value as a string, i.e. '20221005' (this string can also be returned). It then creates a pushdown predicate to load only this partition."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"center"},(0,i.kt)("img",{alt:"Loading and processing data from S3 using a pushdown predicate fetching the latest date value from the Glue catalogue",src:a(8944).Z,width:"2380",height:"1326"})))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"center"},(0,i.kt)("em",{parentName:"td"},"In this example, we have 3 partitions for 3, 4, and 5/10/2022. The job runs on the 7/10/2022. Because of the pushdown predicate with latest partition date, it will identify that the latest partition was on the 5/10/2022. It will load and process this partition only."))))),(0,i.kt)("h4",{id:"pros-2"},"Pros"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"This approach never loads more than one day\u2019s worth of data, so it is cheap."),(0,i.kt)("li",{parentName:"ul"},"It works even if you have no idea when source data was last produced"),(0,i.kt)("li",{parentName:"ul"},"You don't need a GetLatestPartitions query after loading your dataframe")),(0,i.kt)("h4",{id:"cons-2"},"Cons"),(0,i.kt)("p",null,"This approach relies on the Glue catalogue being up-to-date and not containing empty partitions. If data is deleted, we want the corresponding partition to be removed from the catalogue. If crawlers are used to update the catalogue, they must be set up with the non-standard option as below::"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Crawler option to delete empty partitions",src:a(2927).Z,width:"1544",height:"294"})),(0,i.kt)("h4",{id:"scenarios-when-not-to-use-it-2"},"Scenarios when not to use it"),(0,i.kt)("p",null,"This is not suitable if the catalogue contains deprecated partitions."),(0,i.kt)("h4",{id:"how-to-use-it-in-a-job-2"},"How to use it in a job"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Import the DP helper method ",(0,i.kt)("inlineCode",{parentName:"li"},"create_pushdown_predicate_for_latest_available_partition")),(0,i.kt)("li",{parentName:"ol"},"Call the method in the ",(0,i.kt)("inlineCode",{parentName:"li"},"push_down_predicate")," option of the ",(0,i.kt)("inlineCode",{parentName:"li"},"createDataFrame")," block (the example below uses the ",(0,i.kt)("inlineCode",{parentName:"li"},"execution_context")," to create the data frame but the same can be achieved using ",(0,i.kt)("inlineCode",{parentName:"li"},"create_dynamic_frame.from_catalogue"),")")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Write a pushdown predicate based on the latest partition from the Glue catalogue",src:a(7481).Z,width:"2076",height:"340"})),(0,i.kt)("h3",{id:"external-documentation-about-pushdown-predicates"},"External documentation about pushdown predicates"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html"},"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/helpers/helpers.py"},"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/helpers/helpers.py")),(0,i.kt)("h2",{id:"filtering-data-after-creating-the-dataframe"},"Filtering data after creating the dataframe"),(0,i.kt)("p",null,"After you have loaded data into a dataframe, you can use a pySpark query that only keeps the latest day from all the loaded data.\nThis will work whatever approach you have used to create the dataframe. It does not rely on the Glue catalogue and on registered partitions, but only on the loaded data content. This method has been used in nearly every job in the early Data Platform."),(0,i.kt)("h3",{id:"pros-3"},"Pros"),(0,i.kt)("p",null,"Certitude you only are processing one day\u2019s worth of data"),(0,i.kt)("h3",{id:"cons-3"},"Cons"),(0,i.kt)("p",null,"Can be very expensive if you have loaded many partitions in your dataframe."),(0,i.kt)("h3",{id:"how-to-use-it-in-a-job-3"},"How to use it in a job"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Import the helper function. Several versions of the helper exist:")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"get_latest_partitions")," is the initial version, it uses a ",(0,i.kt)("inlineCode",{parentName:"li"},"where()")," clause and the standard columns ",(0,i.kt)("inlineCode",{parentName:"li"},"import_year"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"import_month")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"inport_day"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"get_latest_partitions_optimized")," is a quicker version using ",(0,i.kt)("inlineCode",{parentName:"li"},"filter()")," instead of ",(0,i.kt)("inlineCode",{parentName:"li"},"where()")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"get_latest_snapshot_optimized")," is the same function as above but uses the partition name ",(0,i.kt)("inlineCode",{parentName:"li"},"snapshot_date")," instead of ",(0,i.kt)("inlineCode",{parentName:"li"},"import_date"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"get_latest_rows_by_date")," is the version to use if the partition name is not standard ",(0,i.kt)("inlineCode",{parentName:"li"},"import_date")," or ",(0,i.kt)("inlineCode",{parentName:"li"},"snapshot_date"),". This function lets you pass the partition name as a parameter.")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Call the method after having loaded the data into a dataframe. It requires a Spark dataframe, not a Glue Dynamic Frame, so you must convert your dynamic frame if necessary.")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Using get_latest_snapshot after loading the data",src:a(4609).Z,width:"1770",height:"514"})),(0,i.kt)("h2",{id:"conclusion"},"Conclusion"),(0,i.kt)("p",null,"We've covered different approaches to make sure the job only processes the data it needs. Many jobs use both filtering before and after loading data into the dataframe. Normally, using a pushdown predicate based on the lasted partition from the Glue catalogue can be used on its own and doesn't require further filtering. Also remember that working from the Trusted zone is he best way to only get the latest data, without needing to filter out older partitions!"))}c.isMDXComponent=!0},2927:function(e,t,a){t.Z=a.p+"assets/images/crawler-option-to-delete-empty-partitions-60a0607a196bb39244aebcf3b4220cbc.png"},4013:function(e,t,a){t.Z=a.p+"assets/images/enabling-bookmark-in-terraform-9b409b4bacc9f37e00d97c0c49581d99.png"},1444:function(e,t,a){t.Z=a.p+"assets/images/loading-processing-steps-with-bookmarks-828cae06cd5f21f76a6a3875b82d7a94.png"},8944:function(e,t,a){t.Z=a.p+"assets/images/loading-processing-steps-with-pushdown-predicate-boto3-73aa4a73834c6a0b4b32c3efd3a6a48e.png"},5122:function(e,t,a){t.Z=a.p+"assets/images/loading-processing-steps-with-pushdown-predicate-buffer-d9559136d9d3a5a8a67037f874491d14.png"},5907:function(e,t,a){t.Z=a.p+"assets/images/loading-processing-steps-291c3e1485892f42ac9a224377dc3611.png"},9691:function(e,t,a){t.Z=a.p+"assets/images/setting-transformation-ctx-for-job-bookmarks-b3dd325cdaf4ba47054e2ec95ebf2b1c.png"},4609:function(e,t,a){t.Z=a.p+"assets/images/using_get_latest_snapshot_after_loading_data-7d44332b1e50e22c6c7ab1b1d8587200.png"},7481:function(e,t,a){t.Z=a.p+"assets/images/write-pushdown-predicate-based-on-the-latest-partition-from-glue-catalogue-8bca65fae0f29be139bedb1eb26f9258.png"},5535:function(e,t,a){t.Z=a.p+"assets/images/write-pushdown-predicate-with-7-days-buffer-7bdd7528f44ca8c93e92a63fa8cc0f74.png"}}]);