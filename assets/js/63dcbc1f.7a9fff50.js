"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[1563],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return u}});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,d=r(e,["components","mdxType","originalType","parentName"]),h=p(a),u=o,m=h["".concat(l,".").concat(u)]||h[u]||c[u]||i;return a?n.createElement(m,s(s({ref:t},d),{},{components:a})):n.createElement(m,s({ref:t},d))}));function u(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,s=new Array(i);s[0]=h;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r.mdxType="string"==typeof e?e:o,s[1]=r;for(var p=2;p<i;p++)s[p]=a[p];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},7262:function(e,t,a){a.r(t),a.d(t,{assets:function(){return d},contentTitle:function(){return l},default:function(){return u},frontMatter:function(){return r},metadata:function(){return p},toc:function(){return c}});var n=a(3117),o=a(102),i=(a(7294),a(3905)),s=["components"],r={title:"Tascomi data ingestion",description:"Description of the ingestion and refinement pipeline for Tascomi planning data",layout:"playbook_js",tags:["playbook"]},l=void 0,p={unversionedId:"docs/tascomi-ingestion",id:"docs/tascomi-ingestion",title:"Tascomi data ingestion",description:"Description of the ingestion and refinement pipeline for Tascomi planning data",source:"@site/docs/docs/tascomi-ingestion.md",sourceDirName:"docs",slug:"/docs/tascomi-ingestion",permalink:"/Data-Platform-Playbook/docs/tascomi-ingestion",draft:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/docs/tascomi-ingestion.md",tags:[{label:"playbook",permalink:"/Data-Platform-Playbook/tags/playbook"}],version:"current",frontMatter:{title:"Tascomi data ingestion",description:"Description of the ingestion and refinement pipeline for Tascomi planning data",layout:"playbook_js",tags:["playbook"]},sidebar:"docs",previous:{title:"Redshift - Creating users, databases and exposing data from Glue",permalink:"/Data-Platform-Playbook/docs/redshift"},next:{title:"VPC Peering Connection between Data Platform and Production APIs AWS accounts",permalink:"/Data-Platform-Playbook/docs/vpc-peering-connection-dataplatform-and-production-apis-account"}},d={},c=[{value:"Details of individual steps",id:"details-of-individual-steps",level:2},{value:"Step 1 - Ingestion",id:"step-1---ingestion",level:3},{value:"1.1 - Initial full ingestion",id:"11---initial-full-ingestion",level:4},{value:"1.2 - Daily ingestion of latest updated records",id:"12---daily-ingestion-of-latest-updated-records",level:4},{value:"Step 2 - Daily parsing of the json increments",id:"step-2---daily-parsing-of-the-json-increments",level:3},{value:"Step 3 - Daily refinement of the parsed increments",id:"step-3---daily-refinement-of-the-parsed-increments",level:3},{value:"Step 4 - Creation of the daily snapshot",id:"step-4---creation-of-the-daily-snapshot",level:3},{value:"Full workflow and scheduling",id:"full-workflow-and-scheduling",level:2},{value:"Structure of the S3 buckets and Glue tables",id:"structure-of-the-s3-buckets-and-glue-tables",level:2},{value:"How to add a table to the pipeline",id:"how-to-add-a-table-to-the-pipeline",level:2},{value:"Test the endpoint",id:"test-the-endpoint",level:3},{value:"Create and check out a new branch in the repository",id:"create-and-check-out-a-new-branch-in-the-repository",level:3},{value:"Add the table to the column type dictionary",id:"add-the-table-to-the-column-type-dictionary",level:3},{value:"Add the table to the Terraform script.",id:"add-the-table-to-the-terraform-script",level:3},{value:"Add data quality tests in the relevant scripts",id:"add-data-quality-tests-in-the-relevant-scripts",level:3},{value:"Commit your changes in the new branch and open a pull request",id:"commit-your-changes-in-the-new-branch-and-open-a-pull-request",level:3},{value:"How to reset all refined Tascomi data",id:"how-to-reset-all-refined-tascomi-data",level:2},{value:"Reset the ingested increments:",id:"reset-the-ingested-increments",level:3},{value:"Reset the parsed increments:",id:"reset-the-parsed-increments",level:3},{value:"Reset the refined increments:",id:"reset-the-refined-increments",level:3},{value:"Reset the refined snapshot:",id:"reset-the-refined-snapshot",level:3},{value:"How to rewind to a past state and recreate the snapshots from there",id:"how-to-rewind-to-a-past-state-and-recreate-the-snapshots-from-there",level:2},{value:"Delete recent data in S3 and crawl",id:"delete-recent-data-in-s3-and-crawl",level:3},{value:"Rewind the job bookmark to the last day everything was fine",id:"rewind-the-job-bookmark-to-the-last-day-everything-was-fine",level:3},{value:"Extra steps needed depending on the scenario",id:"extra-steps-needed-depending-on-the-scenario",level:3},{value:"Run the daily snapshot job and crawl",id:"run-the-daily-snapshot-job-and-crawl",level:3}],h={toc:c};function u(e){var t=e.components,r=(0,o.Z)(e,s);return(0,i.kt)("wrapper",(0,n.Z)({},h,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"This section describes how Tascomi Planning data gets ingested and refined in the data platform. The process relies on ",(0,i.kt)("a",{parentName:"p",href:"https://hackney-planning.tascomi.com/rest/v1/documentation.html?public_key=dd95bcd473f46a4325a4021d54500c7d#available-resources"},"Tascomi API")," and is composed of the following steps:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"An initial full ingestion from Tascomi API (only once, happened in October 2021)"),(0,i.kt)("li",{parentName:"ul"},"A daily call to the Tascomi API to get latest updated records (increment)"),(0,i.kt)("li",{parentName:"ul"},"Parsing of the json data increment returned by the API"),(0,i.kt)("li",{parentName:"ul"},"Refinement of the parsed data to recast all columns to the right data type"),(0,i.kt)("li",{parentName:"ul"},"Creation of a full snapshot by applying the daily increment to the previous snapshot")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Tascomi data ingestion process",src:a(566).Z,width:"3470",height:"1620"})),(0,i.kt)("h2",{id:"details-of-individual-steps"},"Details of individual steps"),(0,i.kt)("h3",{id:"step-1---ingestion"},"Step 1 - Ingestion"),(0,i.kt)("p",null,"This ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/jobs/planning/tascomi_api_ingestion.py"},"process")," queries one API endpoint (e.g. the applications endpoint) and writes the data into a table of the same name. This process writes into the raw zone bucket, with the 'api_response' prefix. The data is partitioned by ",(0,i.kt)("inlineCode",{parentName:"p"},"import_date"),"."),(0,i.kt)("h4",{id:"11---initial-full-ingestion"},"1.1 - Initial full ingestion"),(0,i.kt)("p",null,"This initial run imported the full Tascomi tables"),(0,i.kt)("h4",{id:"12---daily-ingestion-of-latest-updated-records"},"1.2 - Daily ingestion of latest updated records"),(0,i.kt)("p",null,"The subsequent runs only ingest the records updated since the last import. The process relies on the ",(0,i.kt)("inlineCode",{parentName:"p"},"last_updated")," column that is present on all Tascomi tables."),(0,i.kt)("h3",{id:"step-2---daily-parsing-of-the-json-increments"},"Step 2 - Daily parsing of the json increments"),(0,i.kt)("p",null,"This ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/jobs/planning/tascomi_parse_tables_increments.py"},"process")," uses job bookmarking to only process new increments. It also uses a pushdown predicate to only load the last 5 daily prtitions (it is quicker than loading the full dataset).\nIt processes all tables in a loop. For each table, the large json blob containing all the fields is exploded into separate textual columns."),(0,i.kt)("p",null,"This process writes into the raw zone bucket, with the 'planning/tascomi/parsed' prefix. The data is partitioned by ",(0,i.kt)("inlineCode",{parentName:"p"},"import_date"),"."),(0,i.kt)("h3",{id:"step-3---daily-refinement-of-the-parsed-increments"},"Step 3 - Daily refinement of the parsed increments"),(0,i.kt)("p",null,"This ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/jobs/planning/tascomi_recast_tables_increments.py"},"process")," uses job bookmarking to only process new increments.\nIt processes all tables in a loop. For each table, the text columns are converted into correct data types (dates, boolean etc.). It uses a ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/jobs/planning/tascomi-column-type-dictionary.json"},"column type dictionary")," saved in S3 in a separate json file. This dictionary was created semi-automatically with FME (an ETL tool used in the Data and Insight team), by converting the list of columns described in the ",(0,i.kt)("a",{parentName:"p",href:"https://hackney-planning.tascomi.com/rest/v1/documentation.html?public_key=dd95bcd473f46a4325a4021d54500c7d#available-resources"},"API endpoints documentation"),"."),(0,i.kt)("p",null,"This process writes into the refined zone bucket, with the 'planning/tascomi/increment' prefix. The data is partitioned by ",(0,i.kt)("inlineCode",{parentName:"p"},"import_date"),"."),(0,i.kt)("h3",{id:"step-4---creation-of-the-daily-snapshot"},"Step 4 - Creation of the daily snapshot"),(0,i.kt)("p",null,"This ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/jobs/planning/tascomi_create_daily_snapshot.py"},"process")," combines the latest snapshot and all increments created since that day, to create a new snapshot. It uses pushdown predicate and job bookmarking to only process new increments. Like the 2 previous steps, it processes all tables in a loop. If several days increments need to be applied, the process first ensures that no duplicate records are present, by only keeping the latest updated one (for instance, if a planning application has changed status 2 times, it only keeps the record with the latest status). To apply the increments to the previous snapshot, we just replace pre-existing records with the newer version, using the unique id. A new column 'snapshot_date' is created and set to the current date."),(0,i.kt)("p",null,"This process writes into the refined zone bucket, with the 'planning/tascomi/snapshot' prefix. The data is partitioned by ",(0,i.kt)("inlineCode",{parentName:"p"},"snapshot_date"),"."),(0,i.kt)("h2",{id:"full-workflow-and-scheduling"},"Full workflow and scheduling"),(0,i.kt)("p",null,"The full workflow is defined in the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/terraform/etl/24-aws-glue-tascomi-data.tf"},"glue-tascomi-data terraform script"),".\nIt defines a list of tables that needs updating everyday, and a list of static tables that are only updated weekly (these are the static tables like application types). The schedule is as follows:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"3am GMT: as many jobs as tables to update are triggered. Each job queries one API endpoint for latest updated records. That's 25 jobs on Sundays (including static tables), about half of that on other days."),(0,i.kt)("li",{parentName:"ul"},"4am GMT: a crawler crawls the API results bucket",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"the previous crawler triggers the ",(0,i.kt)("strong",{parentName:"li"},"parsing")," job and the crawling of its results"),(0,i.kt)("li",{parentName:"ul"},"the previous crawler triggers the ",(0,i.kt)("strong",{parentName:"li"},"recasting")," job and the crawling of its results"),(0,i.kt)("li",{parentName:"ul"},"the previous crawler triggers the ",(0,i.kt)("strong",{parentName:"li"},"daily snapshot creation")," job and the crawling of its results"))),(0,i.kt)("li",{parentName:"ul"},"5am GMT: the API results bucket gets crawled again - this is in case the ingestion had not finished at 4am when the first crawling happened, and some tables were missed. The same sequence as above will repeat, each job being triggered by the crawler of the previous job. However, each job will usually finish early, as it is bookmarked and won't usually find any new data to process.")),(0,i.kt)("h2",{id:"structure-of-the-s3-buckets-and-glue-tables"},"Structure of the S3 buckets and Glue tables"),(0,i.kt)("p",null,"The data created along the process (initial full load, increments and snapshots) is stored in S3 in the raw and refined zones, with one folder per table."),(0,i.kt)("p",null,"The ready-for-use data is in the refined zone bucket with the prefix /planning/tascomi/snapshot. The corresponding tables in the Glue catalog are simply called applications, appeals, etc. To get the latest data, the query must refer to the snapshot_date latest partition, for example"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'select * from "dataplatform-prod-tascomi-refined-zone"."applications" where snapshot_date = (select max(snapshot_date) from "dataplatform-prod-tascomi-refined-zone"."applications")\n')),(0,i.kt)("p",null,"The refined increments are in the Refined zone Planning bucket, in the ",(0,i.kt)("inlineCode",{parentName:"p"},"increments")," area. The tables are prefixed with ",(0,i.kt)("inlineCode",{parentName:"p"},"increment_"),". To count the increment loaded on a specific day, you could use:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'select count(*) from "dataplatform-prod-tascomi-refined-zone"."increment_applications" where import_date = \'20211208\'\n')),(0,i.kt)("p",null,"The parsed increments are in the Raw zone Planning bucket, in the ",(0,i.kt)("inlineCode",{parentName:"p"},"parsed")," area. The tables are not prefixed, and partitioned by ",(0,i.kt)("inlineCode",{parentName:"p"},"import_date")," with. To count the increment loaded on a specific day, you could use:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'select count(*) from "dataplatform-prod-tascomi-raw-zone"."applications" where import_date = \'20211208\'\n')),(0,i.kt)("p",null,"The raw data returned by the API is in the Raw zone Planning bucket, in the ",(0,i.kt)("inlineCode",{parentName:"p"},"api_response")," area. The tables are prefixed with ",(0,i.kt)("inlineCode",{parentName:"p"},"api_response_"),", and partitioned by ",(0,i.kt)("inlineCode",{parentName:"p"},"import_date")," with. To count the increment loaded on a specific day, you could use:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'select count(*) from "dataplatform-prod-tascomi-raw-zone"."api_response_applications" where import_date = \'20211208\'\n')),(0,i.kt)("h2",{id:"how-to-add-a-table-to-the-pipeline"},"How to add a table to the pipeline"),(0,i.kt)("p",null,"Follow these steps to start ingesting data from a new endpoint available from the API. "),(0,i.kt)("h3",{id:"test-the-endpoint"},"Test the endpoint"),(0,i.kt)("p",null,"You can use a Jupyter notebook on your local install to check that the endpoint is returning what you expect. It is hard to test with Postman because of the time-dependent token that the Tascomi API is using for authentication."),(0,i.kt)("h3",{id:"create-and-check-out-a-new-branch-in-the-repository"},"Create and check out a new branch in the repository"),(0,i.kt)("p",null,"All the changes below should be commited to this branch first."),(0,i.kt)("h3",{id:"add-the-table-to-the-column-type-dictionary"},"Add the table to the ",(0,i.kt)("a",{parentName:"h3",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/jobs/planning/tascomi-column-type-dictionary.json"},"column type dictionary")),(0,i.kt)("p",null,"This json dictionary supports the 'recast increment' step that converts string columns into their cortect data types. It looks like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'        "long": {\n            "applications": [\n                "site_address_x",\n                "site_address_y",\n                "tree_location_x",\n                "tree_location_y"\n            ],\n            "emails": [\n                "last_updated",\n                "submit_date",\n                "ceased_date"\n            ],\n            "enforcements": [\n                "complaint_location_x",\n                "complaint_location_y"\n            ],\n            "dtf_locations": ["parent_uprn","uprn","usrn"],\n            "users": ["mileage_rate"]\n        },\n        "double": {\n            "applications": [\n                "affordable_housing_balancing_sum",\n                "height_of_proposed_development",\n                "proposed_building_dimensions_breadth",\n                "proposed_building_dimensions_eaves",\n                "proposed_building_dimensions_length",\n                "proposed_building_dimensions_ridge",\n                "proposed_building_distance_from_proposal",\n                "proposed_building_overall_ground_area",\n                "proposed_fish_tank_cage_depth",\n                "proposed_fish_tank_cage_height",\n                "proposed_fish_tank_cage_length",\n                "proposed_fish_tank_cage_width"\n            ],\n            "appeals": ["appeal_location_x","appeal_location_y"]\n        }\n')),(0,i.kt)("p",null,"To amend the catalogue semi-automatically you need to do the following (TODO: create a Python script to replace the FME process)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Open the ",(0,i.kt)("a",{parentName:"li",href:"https://hackney-planning.tascomi.com/rest/v1/documentation.html?public_key=dd95bcd473f46a4325a4021d54500c7d#available-resources"},"Tascomi API resources page"),". Navigate to the table you're adding, select and copy its content."),(0,i.kt)("li",{parentName:"ul"},"Open the ",(0,i.kt)("a",{parentName:"li",href:"https://docs.google.com/spreadsheets/d/1ZZwWHSoudBgN9j0jV6ZrNZKgXYMOm7ObWTWLT3Xg8Rw/edit?usp=sharing"},"Tascomi column dictionary Google Sheet"),", create a new tab for the new table and paste the content you copied in the previous step. Only keep 2 columns: field and type."),(0,i.kt)("li",{parentName:"ul"},"Launch FME desktop, open the Tascomi Dictionary workspace, refresh the feature types in the reader to see the new tab of the Google Sheet. Run the workspace for the new tab. You'll get fragments of json that you can copy and paste into the proper dictionary.")),(0,i.kt)("h3",{id:"add-the-table-to-the-terraform-script"},"Add the table to the ",(0,i.kt)("a",{parentName:"h3",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/terraform/etl/24-aws-glue-tascomi-data.tf"},"Terraform script"),"."),(0,i.kt)("p",null,"Decide wether the new table should be ingested daily (in this case append it to the ",(0,i.kt)("inlineCode",{parentName:"p"},"tascomi_table_names")," list) or weekly (in this case appen it to the ",(0,i.kt)("inlineCode",{parentName:"p"},"tascomi_static_tables")," list)."),(0,i.kt)("h3",{id:"add-data-quality-tests-in-the-relevant-scripts"},"Add data quality tests in the relevant scripts"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://playbook.hackney.gov.uk/Data-Platform-Playbook/playbook/transforming-data/guides-to-testing-in-the-platform/data-quality-testing-guide"},"Quality testing with PyDeequ")," is parameterised inside each relevant script. At the moment, only the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/scripts/jobs/planning/tascomi_parse_tables_increments.py"},"parse table increment script")," has tests implemented. For your new table to be quality-checked each day, you need to open this script and append a line in this section near the top: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"dq_params = {'appeals': {'unique': ['id', 'import_date'], 'complete': 'id'},\n             'applications': {'unique': ['id', 'import_date'], 'complete': 'application_reference_number'},\n             'appeal_decision': {'unique': ['id', 'import_date'], 'complete': 'id'}\n             }\n")),(0,i.kt)("p",null,"The last line means that, for the job to complete successfully, in the appeal_decision table increment, the combination (id, import_date) should be unique, and the id field should be complete.\nThis is the only script you need to amend at present, but it would be useful to add quality testing to other bits of the process."),(0,i.kt)("h3",{id:"commit-your-changes-in-the-new-branch-and-open-a-pull-request"},"Commit your changes in the new branch and open a pull request"),(0,i.kt)("p",null,"Unit tests will run automatically when you push. At the moment, tests are implemented for all bits of the process except from the 'parse table increments' one.  "),(0,i.kt)("h2",{id:"how-to-reset-all-refined-tascomi-data"},"How to reset all refined Tascomi data"),(0,i.kt)("p",null,"If you suspect a problem in the increments or snapshots, you can delete and recreate them in their respective buckets."),(0,i.kt)("h3",{id:"reset-the-ingested-increments"},"Reset the ingested increments:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In S3 raw zone 'api",(0,i.kt)("em",{parentName:"li"},"response' bucket, in each table repository, delete the data up to the last date you want to keep. _Do not delete the initial full load!")),(0,i.kt)("li",{parentName:"ul"},"Run the api_response crawler"),(0,i.kt)("li",{parentName:"ul"},"Run the ingestion job"),(0,i.kt)("li",{parentName:"ul"},"Run the api_response crawler again.")),(0,i.kt)("p",null,"As a result you should see in S3 a new partition with today's date. It contains all records updated since the last day you kept in the bucket."),(0,i.kt)("h3",{id:"reset-the-parsed-increments"},"Reset the parsed increments:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In S3 raw zone, empty the 'parsed' bucket"),(0,i.kt)("li",{parentName:"ul"},"Reset the job bookmark (In Glue, > job view > select the job and click on actions)"),(0,i.kt)("li",{parentName:"ul"},"Remove the pushdown predicate: open the job script and edit the line that sets the pushdown predicae to 0 days, then save:")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"pushdown_predicate = create_pushdown_predicate(partitionDateColumn='import_date', daysBuffer=0)")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Run the job"),(0,i.kt)("li",{parentName:"ul"},"Set back the pushdown predicate to its initial value, then save the script:")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"pushdown_predicate = create_pushdown_predicate(partitionDateColumn='import_date', daysBuffer=5)")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Run the parsed bucket crawler")),(0,i.kt)("h3",{id:"reset-the-refined-increments"},"Reset the refined increments:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In S3 refined zone, empty the 'increments' bucket"),(0,i.kt)("li",{parentName:"ul"},"Reset the job bookmark (In Glue, > job view > select the job and click on actions)"),(0,i.kt)("li",{parentName:"ul"},"Remove the pushdown predicate: open the job script and edit the line that sets the pushdown predicae to 0 days, then save:")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"pushdown_predicate = create_pushdown_predicate(partitionDateColumn='import_date', daysBuffer=0)")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Run the job"),(0,i.kt)("li",{parentName:"ul"},"Set back the pushdown predicate to its initial value, then save:")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"pushdown_predicate = create_pushdown_predicate(partitionDateColumn='import_date', daysBuffer=5)")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Run the refined increment crawler")),(0,i.kt)("h3",{id:"reset-the-refined-snapshot"},"Reset the refined snapshot:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In S3 refined zone, empty the 'snapshot' bucket"),(0,i.kt)("li",{parentName:"ul"},"Delete all the snapshot tables in the Glue catalogue"),(0,i.kt)("li",{parentName:"ul"},"Reset the job bookmark (In Glue, > job view > select the job and click on actions)"),(0,i.kt)("li",{parentName:"ul"},"Remove the pushdown predicate: open the job script and edit the line that sets the pushdown predicae to 0 days, then save:")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"pushdown_predicate = create_pushdown_predicate(partitionDateColumn='snapshot_date', daysBuffer=0)")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Run the job"),(0,i.kt)("li",{parentName:"ul"},"Set back the pushdown predicate to its initial value:")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"pushdown_predicate = create_pushdown_predicate(partitionDateColumn='snapshot_date', daysBuffer=5)")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Run the refined snapshot crawler.")),(0,i.kt)("p",null,"As a resut you should only have today's snapshot in the snapshot bucket."),(0,i.kt)("h2",{id:"how-to-rewind-to-a-past-state-and-recreate-the-snapshots-from-there"},"How to rewind to a past state and recreate the snapshots from there"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"This method uses the AWS CLI"),"\nFollow these steps if a problem occured at a recent date and you don't want to reset the full history of snapshots. You will reset the bookmarks to a date prior to the problem, delete all the snapshots since that date in S3, and run the snapshot job again."),(0,i.kt)("h3",{id:"delete-recent-data-in-s3-and-crawl"},"Delete recent data in S3 and crawl"),(0,i.kt)("p",null,"Say something wrong happened on 23/01/2022 and we are the 25th. You need to delete the snapshots dated 20220123, 20220124 and 20220125.\nDo this in AWS CLI using:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"aws s3 rm s3://dataplatform-prod-refined-zone/planning/tascomi/snapshot --recursive --exclude '*' --include '*20220123*'\n")),(0,i.kt)("p",null,"and then"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"aws s3 rm s3://dataplatform-prod-refined-zone/planning/tascomi/snapshot --recursive --exclude '*' --include '*20220124*'\n")),(0,i.kt)("p",null,"and then"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"aws s3 rm s3://dataplatform-prod-refined-zone/planning/tascomi/snapshot --recursive --exclude '*' --include '*20220125*'\n")),(0,i.kt)("p",null,"If you also want to delete the refined increments, you can go one level up: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"aws s3 rm s3://dataplatform-prod-refined-zone/planning/tascomi/ --recursive --exclude '*' --include '*20220124*'\n")),(0,i.kt)("p",null,"etc."),(0,i.kt)("p",null,"If you have AWS Vault configured with a profile called preprod, the command becomes: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"aws-vault exec preprod -- aws glue reset-job-bookmark --job-name 'prod tascomi_create_daily_snapshot_planning' --run-id jr_e6d6c7e66b27ff27929b3f46555ecdcd9f9e068675eaafaf231f4d338d04db33\n")),(0,i.kt)("p",null,"Don't forget to run the refined snapshot crawler so the Glue catalogue sees the recent changes."),(0,i.kt)("h3",{id:"rewind-the-job-bookmark-to-the-last-day-everything-was-fine"},"Rewind the job bookmark to the last day everything was fine"),(0,i.kt)("p",null,"We are using ",(0,i.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/cli/latest/reference/glue/reset-job-bookmark.html"},"this method")," and running in the CLI:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"get-job-bookmark\n--job-name <value>\n[--run-id <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n")),(0,i.kt)("p",null,"You'll find the job-name and the run-id of the last successful run in the 'jobs' section of the Glue console. An example of full command is:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"aws glue reset-job-bookmark --job-name 'prod tascomi_create_daily_snapshot_planning' --run-id jr_e6d6c7e66b27ff27929b3f46555ecdcd9f9e068675eaafaf231f4d338d04db33\n")),(0,i.kt)("h3",{id:"extra-steps-needed-depending-on-the-scenario"},"Extra steps needed depending on the scenario"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"If you're going more than 5 days back, the pushdown predicate menas you won't be loading any older snapshot. You need to allow a larger daysbuffer in the pushdown predicate before running the job. Save the script."),(0,i.kt)("li",{parentName:"ul"},"If you are going back to a point when one of the snapshots didn't exist (because the endpoint had not been used yet), you need to delete this snapshot table in the Glue catalogue before running the job.")),(0,i.kt)("h3",{id:"run-the-daily-snapshot-job-and-crawl"},"Run the daily snapshot job and crawl"),(0,i.kt)("p",null,"Today's snapshot will be created. There won't be any snapshot between this one and the last day everything was fine. Don't forget to run the crawler to see the data in Athena."))}u.isMDXComponent=!0},566:function(e,t,a){t.Z=a.p+"assets/images/tascomi-ingestion-pipeline-e34ac24b998aac993115d0d9822a3ed5.png"}}]);