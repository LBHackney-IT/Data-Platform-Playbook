"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[8872],{3905:function(e,t,a){a.d(t,{Zo:function(){return l},kt:function(){return p}});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var d=n.createContext({}),c=function(e){var t=n.useContext(d),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},l=function(e){var t=c(e.components);return n.createElement(d.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},f=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,d=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),f=c(a),p=o,m=f["".concat(d,".").concat(p)]||f[p]||u[p]||r;return a?n.createElement(m,i(i({ref:t},l),{},{components:a})):n.createElement(m,i({ref:t},l))}));function p(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=f;var s={};for(var d in t)hasOwnProperty.call(t,d)&&(s[d]=t[d]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var c=2;c<r;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}f.displayName="MDXCreateElement"},1452:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return s},contentTitle:function(){return d},metadata:function(){return c},toc:function(){return l},default:function(){return f}});var n=a(7462),o=a(3366),r=(a(7294),a(3905)),i=["components"],s={title:"Ingesting Academy data onto the Data Platform",description:"Overview of how Academy data is ingested onto the Data Platform from MS SQL databases and distributed to Housing Benefits & Needs and Revenues Departments",tags:null,layout:"layout"},d=void 0,c={unversionedId:"docs/academy-ingestion",id:"docs/academy-ingestion",isDocsHomePage:!1,title:"Ingesting Academy data onto the Data Platform",description:"Overview of how Academy data is ingested onto the Data Platform from MS SQL databases and distributed to Housing Benefits & Needs and Revenues Departments",source:"@site/docs/docs/academy-ingestion.md",sourceDirName:"docs",slug:"/docs/academy-ingestion",permalink:"/Data-Platform-Playbook/docs/academy-ingestion",editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/docs/academy-ingestion.md",tags:[],version:"current",frontMatter:{title:"Ingesting Academy data onto the Data Platform",description:"Overview of how Academy data is ingested onto the Data Platform from MS SQL databases and distributed to Housing Benefits & Needs and Revenues Departments",tags:null,layout:"layout"},sidebar:"docs",previous:{title:"Querying the Data Platform using SQL within AWS Athena",permalink:"/Data-Platform-Playbook/playbook/querying-and-analysing-data/querying-data-using-sql"},next:{title:"Auto-adjusting AWS Budget Alerts",permalink:"/Data-Platform-Playbook/docs/auto-adjusting-aws-budget"}},l=[{value:"Architecture",id:"architecture",children:[]}],u={toc:l};function f(e){var t=e.components,s=(0,o.Z)(e,i);return(0,r.kt)("wrapper",(0,n.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"This section covers the overview of how Academy data is ingested onto the Data Platform from an MS SQL database and distributed to Housing Benefits & Needs and Revenues Departments"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Ingesting Academy Data",src:a(6636).Z})),(0,r.kt)("h2",{id:"architecture"},"Architecture"),(0,r.kt)("p",null,"A ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/dg/connection-defining.html"},"JDBC connection")," (Glue Connection) is used to get access and connect to the Academy's MS SQL server database which contains Benefits & Housing Needs and Revenues data. "),(0,r.kt)("p",null,"The JDBC connection is used in both the AWS Glue crawler and AWS Glue job (seen in the diagram) to extract data from the SQL views and tables."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"JDBC is a low-level API for making database connections and handling SQL queries and responses. ")),(0,r.kt)("p",null,"With the established connection, the Glue Crawler crawls the source database and populates a Glue Catalog database\nwith the tables' structures; such as table schemas and other metadata of the specified tables from the MS SQL database."),(0,r.kt)("p",null,"The tables' schema and metadata are then read from the Glue Catalog Database, and multiple concurrent Glue jobs pull the data from the MS SQL Server source database via the JDBC Connection (ingestion of tables are divided up to improve efficiency), and then the data is written to a landing zone S3 bucket. "),(0,r.kt)("p",null,"Once the data has been ingested into the landing zone S3 bucket, it is then crawled and uploaded to a landing zone Glue Catalog database where another Glue job then copies it over to the respective Benefits & Housing Needs and Revenues raw zone department areas in S3 and Glue Catalog databases."),(0,r.kt)("p",null,"The departments' data then becomes available for querying in Athena and/or for further processing and transforming in Glue jobs to move the data to refined and trusted zones and ultimately pulled into BI tools such as Qlik and Google Data Studio."))}f.isMDXComponent=!0},6636:function(e,t,a){t.Z=a.p+"assets/images/academy-data-ingestion-process-0bf06ff51b78f5709ca23fe811c00868.png"}}]);