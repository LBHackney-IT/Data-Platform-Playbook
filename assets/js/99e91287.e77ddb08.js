"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[9800],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>u});var o=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,o,r=function(e,t){if(null==e)return{};var a,o,r={},n=Object.keys(e);for(o=0;o<n.length;o++)a=n[o],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(o=0;o<n.length;o++)a=n[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=o.createContext({}),d=function(e){var t=o.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=d(e.components);return o.createElement(l.Provider,{value:t},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var a=e.components,r=e.mdxType,n=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=d(a),h=r,u=m["".concat(l,".").concat(h)]||m[h]||c[h]||n;return a?o.createElement(u,i(i({ref:t},p),{},{components:a})):o.createElement(u,i({ref:t},p))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var n=a.length,i=new Array(n);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[m]="string"==typeof e?e:r,i[1]=s;for(var d=2;d<n;d++)i[d]=a[d];return o.createElement.apply(null,i)}return o.createElement.apply(null,a)}h.displayName="MDXCreateElement"},922:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>n,metadata:()=>s,toc:()=>d});var o=a(7462),r=(a(7294),a(3905));const n={id:"introduction",title:"Introduction",description:"The DAP\u21e8flow guide for data analysts and engineers, for developing and deploying Airflow DAGs, running data pipelines in the Data Analytics Platform (DAP).",layout:"playbook_js",tags:["dap-airflow"]},i="Introduction",s={unversionedId:"dap-airflow/introduction",id:"dap-airflow/introduction",title:"Introduction",description:"The DAP\u21e8flow guide for data analysts and engineers, for developing and deploying Airflow DAGs, running data pipelines in the Data Analytics Platform (DAP).",source:"@site/docs/dap-airflow/introduction.md",sourceDirName:"dap-airflow",slug:"/dap-airflow/introduction",permalink:"/Data-Platform-Playbook/dap-airflow/introduction",draft:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/dap-airflow/introduction.md",tags:[{label:"dap-airflow",permalink:"/Data-Platform-Playbook/tags/dap-airflow"}],version:"current",frontMatter:{id:"introduction",title:"Introduction",description:"The DAP\u21e8flow guide for data analysts and engineers, for developing and deploying Airflow DAGs, running data pipelines in the Data Analytics Platform (DAP).",layout:"playbook_js",tags:["dap-airflow"]},sidebar:"docs",previous:{title:"VPC Peering Connection between Data Platform and Production APIs AWS accounts",permalink:"/Data-Platform-Playbook/docs/vpc-peering-connection-dataplatform-and-production-apis-account"},next:{title:"Before you begin",permalink:"/Data-Platform-Playbook/dap-airflow/onboarding/begin"}},l={},d=[{value:"What is <strong>DAP\u21e8flow</strong>?",id:"what-is-dapflow",level:2},{value:"It allows Data Analysts, in the simplest way possible, to develop and run data pipelines using their own service&#39;s data and create data products for their service and service users.",id:"it-allows-data-analysts-in-the-simplest-way-possible-to-develop-and-run-data-pipelines-using-their-own-services-data-and-create-data-products-for-their-service-and-service-users",level:4},{value:"How <strong>DAP\u21e8flow</strong> solves these problems",id:"how-dapflow-solves-these-problems",level:4},{value:"\ud83d\udcdaOnboarding",id:"onboarding",level:2},{value:"A series onboarding documents is available here, to help Data Analysts get started with <strong>DAP\u21e8flow</strong>",id:"a-series-onboarding-documents-is-available-here-to-help-data-analysts-get-started-with-dapflow",level:4},{value:"<strong><em>&quot;We</em> \u2661 <em>your feedback!&quot;</em></strong>",id:"we--your-feedback",level:4},{value:"Here below, is the full list of topics currently on offer...",id:"here-below-is-the-full-list-of-topics-currently-on-offer",level:4},{value:"Before you begin",id:"before-you-begin",level:3},{value:"What must happen before I can begin DAP\u21e8flow?",id:"what-must-happen-before-i-can-begin-dapflow",level:4},{value:"AWS Console access",id:"aws-console-access",level:3},{value:"How will I access the AWS Management Console?",id:"how-will-i-access-the-aws-management-console",level:4},{value:"AWS region",id:"aws-region",level:3},{value:"How will I ensure I am in the correct AWS region?",id:"how-will-i-ensure-i-am-in-the-correct-aws-region",level:4},{value:"Amazon Athena",id:"amazon-athena",level:3},{value:"How will I access my database from Amazon Athena?",id:"how-will-i-access-my-database-from-amazon-athena",level:4},{value:"My current service data",id:"my-current-service-data",level:3},{value:"How will I access my current service data from Amazon Athena?",id:"how-will-i-access-my-current-service-data-from-amazon-athena",level:4},{value:"My service data history",id:"my-service-data-history",level:3},{value:"How will I access my service data history from Amazon Athena?",id:"how-will-i-access-my-service-data-history-from-amazon-athena",level:4},{value:"Query my service data",id:"query-my-service-data",level:3},{value:"How will I query and analyze my service data with Amazon Athena?",id:"how-will-i-query-and-analyze-my-service-data-with-amazon-athena",level:4},{value:"Prototype simple transforms",id:"prototype-simple-transforms",level:3},{value:"How can I use Amazon Athena to prototype a simple table-join data transformation?",id:"how-can-i-use-amazon-athena-to-prototype-a-simple-table-join-data-transformation",level:4},{value:"Prototype legacy transforms",id:"prototype-legacy-transforms",level:3},{value:"How do I use Amazon Athena to prototype a data transformation from a <code>[legacy SQL query]</code>?",id:"how-do-i-use-amazon-athena-to-prototype-a-data-transformation-from-a-legacy-sql-query",level:4},{value:"Topics arriving here soon...",id:"topics-arriving-here-soon",level:2},{value:"Setting up Github",id:"setting-up-github",level:3},{value:"How do I set up my Github access for <strong>DAP\u21e8flow</strong>?",id:"how-do-i-set-up-my-github-access-for-dapflow",level:4},{value:"Github branching",id:"github-branching",level:3},{value:"How do I add a new development branch to <strong>DAP\u21e8flow</strong>&#39;s <code>[dap-airflow]</code> repository?",id:"how-do-i-add-a-new-development-branch-to-dapflows-dap-airflow-repository",level:4},{value:"Committing transforms",id:"committing-transforms",level:3},{value:"How do I commit my working transform query to <strong>DAP\u21e8flow</strong>&#39;s <code>[dap-airflow]</code> repository?",id:"how-do-i-commit-my-working-transform-query-to-dapflows-dap-airflow-repository",level:4},{value:"Raising pull requests",id:"raising-pull-requests",level:3},{value:"How do I raise a Pull Request to merge my development branch into the main trunk of <strong>DAP\u21e8flow</strong>&#39;s <code>[dap-airflow]</code> repository?",id:"how-do-i-raise-a-pull-request-to-merge-my-development-branch-into-the-main-trunk-of-dapflows-dap-airflow-repository",level:4},{value:"Merging branches",id:"merging-branches",level:3},{value:"How do i complete the merge of my development branch into the main trunk of <strong>DAP\u21e8flow</strong>&#39;s <code>[dap-airflow]</code> repository?",id:"how-do-i-complete-the-merge-of-my-development-branch-into-the-main-trunk-of-dapflows-dap-airflow-repository",level:4},{value:"Adding tables to the raw-zone",id:"adding-tables-to-the-raw-zone",level:3},{value:"How do i add a new table ingestion to my service&#39;s raw-zone database?",id:"how-do-i-add-a-new-table-ingestion-to-my-services-raw-zone-database",level:4},{value:"Topics suggested for the later...",id:"topics-suggested-for-the-later",level:2},{value:"Migrating old Athena prototype SQL to the new <strong>DAP\u21e8flow</strong>",id:"migrating-old-athena-prototype-sql-to-the-new-dapflow",level:4},{value:"Refined-zone views",id:"refined-zone-views",level:4},{value:"External access to <strong>DAP\u21e8flow</strong> products",id:"external-access-to-dapflow-products",level:4},{value:"Removing tables from the raw-zone",id:"removing-tables-from-the-raw-zone",level:4},{value:"Removing products from the refined-zone",id:"removing-products-from-the-refined-zone",level:4}],p={toc:d},m="wrapper";function c(e){let{components:t,...n}=e;return(0,r.kt)(m,(0,o.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"DAP\u21e8flow",src:a(3196).Z,width:"930",height:"347"}),"  "),(0,r.kt)("h2",{id:"what-is-dapflow"},"What is ",(0,r.kt)("strong",{parentName:"h2"},"DAP\u21e8flow"),"?"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"DAP\u21e8flow")," is an integation of ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Apache Airflow"))," with ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," built upon Hackney's ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Data Analytics Platform")),". "),(0,r.kt)("h4",{id:"it-allows-data-analysts-in-the-simplest-way-possible-to-develop-and-run-data-pipelines-using-their-own-services-data-and-create-data-products-for-their-service-and-service-users"},"It allows Data Analysts, in the simplest way possible, to develop and run data pipelines using their own service's data and create data products for their service and service users."),(0,r.kt)("p",null,"Building data pipelines used to be harder and more complex and time consuming. Data Analysts after prototyping their SQL queries using ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," were required to migrate Athena's SQL code to Spark SQL, a different SQL dialect, then embed their code within an ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Glue"))," job. They also had to negotiate querying across multiple generations of the same data stored in the ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon S3"))," Data lake. That meant they could not simply take a legacy SQL query and run it in ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," or create an ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Glue"))," product directly from it."),(0,r.kt)("h4",{id:"how-dapflow-solves-these-problems"},"How ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow")," solves these problems"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Firstly, Data Analysts no longer need to use the more complex ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Glue"))," because data pipelines built using ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Apache Airflow"))," can use ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," to generate the data transformation products using the same prototype SQL code.  "),(0,r.kt)("p",{parentName:"li"},"  ",(0,r.kt)("strong",{parentName:"p"},"This cuts development time by more than half while Data Analysts no longer need to context-switch between the two SQL dialects."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Secondly, Data Analysts no longer need to adapt their SQL queries to ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon S3")),"'s Data Lake partitioning architecture, because ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Apache Airflow"))," is used to generate views of the underlying table data which presents Data Analysts with only current ingested service data, in readiness for prototyping and also later on when the automated transforms are run by Airflow."),(0,r.kt)("p",{parentName:"li"},"  ",(0,r.kt)("strong",{parentName:"p"},"This further cuts development time while Data Analysts can very easily take the legacy SQL code from their service database system and run it directly on ",(0,r.kt)("em",{parentName:"strong"},"Amazon Athena")," with few changes.")),(0,r.kt)("p",{parentName:"li"},"  Data Analysts can also migrate their existing Athena SQL prototypes, previously adapted for the ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon S3")),"'s Data Lake partitioning architecture, because the table history is still available to them, although the table names will now be suffixed \"",(0,r.kt)("strong",{parentName:"p"},"_history"),'".'))),(0,r.kt)("h2",{id:"onboarding"},"\ud83d\udcdaOnboarding"),(0,r.kt)("h4",{id:"a-series-onboarding-documents-is-available-here-to-help-data-analysts-get-started-with-dapflow"},"A series onboarding documents is available here, to help Data Analysts get started with ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow")),(0,r.kt)("p",null,"Anyone new to ",(0,r.kt)("strong",{parentName:"p"},"DAP\u21e8flow")," will need read ",(0,r.kt)("a",{parentName:"p",href:"../dap-airflow/onboarding/begin"},(0,r.kt)("strong",{parentName:"a"},"\ud83d\udcdaBefore you begin")),"."),(0,r.kt)("p",null,"Thereafter, Data Analysts do not need to read every documents in the order they are listed below, especially if they are already familiar with the ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"AWS Management Console"))," and have used ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Amazon Athena"))," before."),(0,r.kt)("p",null,"Data Analysts are encouraged to think about what they need to do before deciding which document to read next. For example, if they have a ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"legacy SQL query"))," that they want to migrate to ",(0,r.kt)("strong",{parentName:"p"},"DAP\u21e8flow"),", they might jump straight to ",(0,r.kt)("a",{parentName:"p",href:"../dap-airflow/onboarding/prototype-legacy-transforms"},(0,r.kt)("strong",{parentName:"a"},"\ud83d\udcdaPrototype legacy transforms")),"."),(0,r.kt)("h4",{id:"we--your-feedback"},(0,r.kt)("strong",{parentName:"h4"},(0,r.kt)("em",{parentName:"strong"},'"We')," \u2661 ",(0,r.kt)("em",{parentName:"strong"},'your feedback!"'))),(0,r.kt)("p",null,"Your continuous feedback enables us to improve ",(0,r.kt)("strong",{parentName:"p"},"DAP\u21e8flow")," and our ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Data Analytics Platform"))," service. Survey links are provided at the end of each onboarding document."),(0,r.kt)("h4",{id:"here-below-is-the-full-list-of-topics-currently-on-offer"},"Here below, is the full list of topics currently on offer..."),(0,r.kt)("p",null,"Further documents will be added as they are developed. ",(0,r.kt)("a",{parentName:"p",href:"#topics-arriving-here-soon"},(0,r.kt)("strong",{parentName:"a"},"Jump to the end"))," to discover what is coming next!"),(0,r.kt)("h3",{id:"before-you-begin"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/begin"},"Before you begin")),(0,r.kt)("h4",{id:"what-must-happen-before-i-can-begin-dapflow"},"What must happen before I can begin DAP\u21e8flow?"),(0,r.kt)("h3",{id:"aws-console-access"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/access-the-AWS-Management-Console"},"AWS Console access")),(0,r.kt)("h4",{id:"how-will-i-access-the-aws-management-console"},"How will I access the AWS Management Console?"),(0,r.kt)("h3",{id:"aws-region"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/access-the-AWS-region"},"AWS region")),(0,r.kt)("h4",{id:"how-will-i-ensure-i-am-in-the-correct-aws-region"},"How will I ensure I am in the correct AWS region?"),(0,r.kt)("h3",{id:"amazon-athena"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/access-my-Amazon-Athena-database"},"Amazon Athena")),(0,r.kt)("h4",{id:"how-will-i-access-my-database-from-amazon-athena"},"How will I access my database from Amazon Athena?"),(0,r.kt)("h3",{id:"my-current-service-data"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/access-my-current-service-data"},"My current service data")),(0,r.kt)("h4",{id:"how-will-i-access-my-current-service-data-from-amazon-athena"},"How will I access my current service data from Amazon Athena?"),(0,r.kt)("h3",{id:"my-service-data-history"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/access-my-service-data-history"},"My service data history")),(0,r.kt)("h4",{id:"how-will-i-access-my-service-data-history-from-amazon-athena"},"How will I access my service data history from Amazon Athena?"),(0,r.kt)("h3",{id:"query-my-service-data"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/query-my-service-data"},"Query my service data")),(0,r.kt)("h4",{id:"how-will-i-query-and-analyze-my-service-data-with-amazon-athena"},"How will I query and analyze my service data with Amazon Athena?"),(0,r.kt)("h3",{id:"prototype-simple-transforms"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/prototype-simple-transforms"},"Prototype simple transforms")),(0,r.kt)("h4",{id:"how-can-i-use-amazon-athena-to-prototype-a-simple-table-join-data-transformation"},"How can I use Amazon Athena to prototype a simple table-join data transformation?"),(0,r.kt)("h3",{id:"prototype-legacy-transforms"},(0,r.kt)("a",{parentName:"h3",href:"../dap-airflow/onboarding/prototype-legacy-transforms"},"Prototype legacy transforms")),(0,r.kt)("h4",{id:"how-do-i-use-amazon-athena-to-prototype-a-data-transformation-from-a-legacy-sql-query"},"How do I use Amazon Athena to prototype a data transformation from a ",(0,r.kt)("inlineCode",{parentName:"h4"},"[legacy SQL query]"),"?"),(0,r.kt)("h2",{id:"topics-arriving-here-soon"},"Topics arriving here soon..."),(0,r.kt)("h3",{id:"setting-up-github"},"Setting up Github"),(0,r.kt)("h4",{id:"how-do-i-set-up-my-github-access-for-dapflow"},"How do I set up my Github access for ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow"),"?"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://docs.google.com/document/d/1gEjFshKJYYV5w9IbM1Rj0u2tj_BwSWpOLGV8aGwId1A/edit?usp=drive_link"},"DPF-185 DOCUMENTATION / 4.0 Set up Github access for DAP Airflow"),(0,r.kt)("br",{parentName:"p"}),"\n","If you haven\u2019t done so already, get yourself a GitHub account. This document will tell you how to get set up for automating and deploying your DAP Airflow transforms."),(0,r.kt)("h3",{id:"github-branching"},"Github branching"),(0,r.kt)("h4",{id:"how-do-i-add-a-new-development-branch-to-dapflows-dap-airflow-repository"},"How do I add a new development branch to ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow"),"'s ",(0,r.kt)("inlineCode",{parentName:"h4"},"[dap-airflow]")," repository?"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://docs.google.com/document/d/1g6s14JK-9LBM8HT-F6-T-rGq7TxnWHD37FJi04Fh41Q/edit?usp=drive_link"},"DPF-185 DOCUMENTATION / 4.1 Add a new development branch to the DAP Airflow repository ")),(0,r.kt)("h3",{id:"committing-transforms"},"Committing transforms"),(0,r.kt)("h4",{id:"how-do-i-commit-my-working-transform-query-to-dapflows-dap-airflow-repository"},"How do I commit my working transform query to ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow"),"'s ",(0,r.kt)("inlineCode",{parentName:"h4"},"[dap-airflow]")," repository?"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://docs.google.com/document/d/18TL2ep1laWzHU9MW-XvC_N1S-gJx9SmPh9M2DC9XNQ4/edit?usp=drive_link"},"DPF-185 DOCUMENTATION / 4.2  Commit a working transform query to the DAP Airflow repository")),(0,r.kt)("h3",{id:"raising-pull-requests"},"Raising pull requests"),(0,r.kt)("h4",{id:"how-do-i-raise-a-pull-request-to-merge-my-development-branch-into-the-main-trunk-of-dapflows-dap-airflow-repository"},"How do I raise a Pull Request to merge my development branch into the main trunk of ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow"),"'s ",(0,r.kt)("inlineCode",{parentName:"h4"},"[dap-airflow]")," repository?"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://docs.google.com/document/d/1LJjJobb2FVLoadUNCl3R7w9FedTFo8IJWHne-Zw9l-M/edit?usp=drive_link"},"DPF-146 DOCUMENTATION 4.3 Raise a Pull Request to merge the development branch into the main trunk of the DAP Airflow repository")),(0,r.kt)("h3",{id:"merging-branches"},"Merging branches"),(0,r.kt)("h4",{id:"how-do-i-complete-the-merge-of-my-development-branch-into-the-main-trunk-of-dapflows-dap-airflow-repository"},"How do i complete the merge of my development branch into the main trunk of ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow"),"'s ",(0,r.kt)("inlineCode",{parentName:"h4"},"[dap-airflow]")," repository?"),(0,r.kt)("h3",{id:"adding-tables-to-the-raw-zone"},"Adding tables to the raw-zone"),(0,r.kt)("h4",{id:"how-do-i-add-a-new-table-ingestion-to-my-services-raw-zone-database"},"How do i add a new table ingestion to my service's raw-zone database?"),(0,r.kt)("h2",{id:"topics-suggested-for-the-later"},"Topics suggested for the later..."),(0,r.kt)("h4",{id:"migrating-old-athena-prototype-sql-to-the-new-dapflow"},"Migrating old Athena prototype SQL to the new ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow")),(0,r.kt)("h4",{id:"refined-zone-views"},"Refined-zone views"),(0,r.kt)("h4",{id:"external-access-to-dapflow-products"},"External access to ",(0,r.kt)("strong",{parentName:"h4"},"DAP\u21e8flow")," products"),(0,r.kt)("h4",{id:"removing-tables-from-the-raw-zone"},"Removing tables from the raw-zone"),(0,r.kt)("h4",{id:"removing-products-from-the-refined-zone"},"Removing products from the refined-zone"),(0,r.kt)("br",null),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow",src:a(4621).Z,width:"750",height:"800"}),"  "),(0,r.kt)("br",null),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow",src:a(2987).Z,width:"1812",height:"700"}),"  "),(0,r.kt)("br",null),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Apache Airflow",src:a(2530).Z,width:"768",height:"232"})))}c.isMDXComponent=!0},2987:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/AirflowLogo-b028904451fee565e7d778ca7187430f.png"},2530:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/AmazonAthenaLogo-e58d9d0c441123494e04bae050fdd85f.png"},3196:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/DAPairflowFLOW-4becb07acf8bfbb5784ddeee42631832.png"},4621:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/worker_tap_valve_800_wht-c34645452f188ce11a1b52ad32c6543c.jpg"}}]);