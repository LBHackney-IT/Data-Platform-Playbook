"use strict";(self.webpackChunkdata_platform_playbook=self.webpackChunkdata_platform_playbook||[]).push([[6814],{9980:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var a=t(4848),i=t(8453);const o={id:"creating-a-lambda-function-step-by-step",title:"How to create a Lambda function for data ingestion on the Data Analytics Platform",description:"Ingesting data using AWS Lambda [step-by-step]",layout:"playbook_js",tags:["playbook"]},s="How to create a Lambda function for data ingestion on the Data Analytics Platform",r={id:"playbook/ingesting-data/creating-a-lambda-function-step-by-step",title:"How to create a Lambda function for data ingestion on the Data Analytics Platform",description:"Ingesting data using AWS Lambda [step-by-step]",source:"@site/docs/playbook/ingesting-data/013-creating-a-lambda-function-step-by-step.md",sourceDirName:"playbook/ingesting-data",slug:"/playbook/ingesting-data/creating-a-lambda-function-step-by-step",permalink:"/Data-Platform-Playbook/playbook/ingesting-data/creating-a-lambda-function-step-by-step",draft:!1,unlisted:!1,editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/playbook/ingesting-data/013-creating-a-lambda-function-step-by-step.md",tags:[{inline:!0,label:"playbook",permalink:"/Data-Platform-Playbook/tags/playbook"}],version:"current",sidebarPosition:13,frontMatter:{id:"creating-a-lambda-function-step-by-step",title:"How to create a Lambda function for data ingestion on the Data Analytics Platform",description:"Ingesting data using AWS Lambda [step-by-step]",layout:"playbook_js",tags:["playbook"]},sidebar:"docs",previous:{title:"Using Watermarks to Record AWS GLue Job States Between Runs",permalink:"/Data-Platform-Playbook/playbook/ingesting-data/using-watermarks-to-record-job-states"},next:{title:"CSV files to Glue Catalog automation",permalink:"/Data-Platform-Playbook/playbook/ingesting-data/CSV-files-to-Glue-Catalog-automation"}},l={},d=[{value:"Check the feasibility of a Lambda function approach",id:"check-the-feasibility-of-a-lambda-function-approach",level:3},{value:"Outline what you need your Lambda function to do",id:"outline-what-you-need-your-lambda-function-to-do",level:3},{value:"Requesting the correct permissions set",id:"requesting-the-correct-permissions-set",level:3},{value:"Python packages and Lambda layers",id:"python-packages-and-lambda-layers",level:3},{value:"Creating a Lambda Function",id:"creating-a-lambda-function",level:3}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"how-to-create-a-lambda-function-for-data-ingestion-on-the-data-analytics-platform",children:"How to create a Lambda function for data ingestion on the Data Analytics Platform"})}),"\n",(0,a.jsx)(n.h3,{id:"check-the-feasibility-of-a-lambda-function-approach",children:"Check the feasibility of a Lambda function approach"}),"\n",(0,a.jsx)(n.p,{children:"Before starting, it is advisable to discuss with the DAP team that a Lambda function is the most appropriate approach to take for your data ingestion task."}),"\n",(0,a.jsx)(n.p,{children:"Here are some factors to consider before proceeding:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"Execution Time"})}),"\n",(0,a.jsx)(n.p,{children:"Lambda functions have execution time limits (currently 15 minutes per invocation). Ensure that your workload can execute within this time limit. If not, have a discussion with the DAP team to identify a more suitable approach."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"Supported Runtimes and dependencies"})}),"\n",(0,a.jsx)(n.p,{children:"Lambda supports multiple programming languages including Python. Certain Python packages may need to be imported. This can be achieved through Lambda layers."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"Monitoring"})}),"\n",(0,a.jsxs)(n.p,{children:["Lambda integrates with ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html",children:"AWS CloudWatch"})," for logging and monitoring; ensure that the permissions set includes the ability to write logs for debugging later on."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"outline-what-you-need-your-lambda-function-to-do",children:"Outline what you need your Lambda function to do"}),"\n",(0,a.jsxs)(n.p,{children:["On the Data Analytics Platform, we typically use ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/lambda/latest/dg/welcome.html",children:"AWS Lambda"})," to perform simple ingestion and data processing tasks which do not require the resource-intensive infrastructure of an ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html",children:"AWS Glue job"}),", for example:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Making an ",(0,a.jsx)(n.em,{children:"API call"})," and writing the response to the landing zone, including a crawler execution"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.em,{children:"Reading"})," in data from a ",(0,a.jsx)(n.em,{children:"Google Sheet"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.em,{children:"Moving data"})," from one zone to another zone, with a simple refinement"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.em,{children:"Event-driven"})," processing, for example once new data arrives in S3, a Lambda is triggered to perform a task immediately"]}),"\n",(0,a.jsxs)(n.li,{children:["Near ",(0,a.jsx)(n.em,{children:"Real-time processing"})," for example, when a new person is added to a permissions group, a Lambda is triggered to grant this permission"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"If your process involves reading in very large datasets, applying complex algorithms using Spark or and undertaking data transformations with multiple datasets, then this might be better suited to a different approach such as AWS Glue."}),"\n",(0,a.jsx)(n.h3,{id:"requesting-the-correct-permissions-set",children:"Requesting the correct permissions set"}),"\n",(0,a.jsx)(n.p,{children:"You will need to provide the DAP team with the following information so that the correct roles and policies can be set up for your Lambda. At the time of writing it is advisable to set up a short meeting with the DAP team to discuss your requirements :"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["The department or service area that the process is for e.g. ",(0,a.jsx)(n.em,{children:"Housing"})]}),"\n",(0,a.jsxs)(n.li,{children:["Where data will be ingested / read from e.g. ",(0,a.jsx)(n.em,{children:"an external API; housing-zone-raw database"})]}),"\n",(0,a.jsxs)(n.li,{children:["Where data needs to be written to e.g. ",(0,a.jsx)(n.em,{children:"landing zone, refined zone"})]}),"\n",(0,a.jsxs)(n.li,{children:["The kind of actions that needed e.g. writing data to S3 ",(0,a.jsx)(n.em,{children:"s3:PutObject"})]}),"\n",(0,a.jsxs)(n.li,{children:["Whether ",(0,a.jsx)(n.em,{children:"logging"})," is to be enabled"]}),"\n",(0,a.jsxs)(n.li,{children:["Whether secrets needs to be accessed from ",(0,a.jsx)(n.em,{children:"AWS Secret Manager"})," (and what the secrets are called)"]}),"\n",(0,a.jsxs)(n.li,{children:["Any other services needed for your Lambda task e.g. ",(0,a.jsx)(n.em,{children:"AWS Glue Crawler"})," ",(0,a.jsx)(n.em,{children:"StartCrawler"})," action"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"python-packages-and-lambda-layers",children:"Python packages and Lambda layers"}),"\n",(0,a.jsx)(n.p,{children:"Python packages can be imported in the usual way within your script, but will need to be made available to the Lambda function, usually by including within a zip file."}),"\n",(0,a.jsxs)(n.p,{children:["An alternative method is to use ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html",children:"Lambda layers"})," which are where a package (or data) can be stored within a zip file and made available to all organisational users on the AWS environment (our Data Analytics Platform). Some common packages such as pandas are already ",(0,a.jsx)(n.a,{href:"https://aws-sdk-pandas.readthedocs.io/en/stable/layers.html",children:"made available by AWS"})," (for specific Python versions), but other packages can be added to the DAP using this ",(0,a.jsx)(n.a,{href:"https://docs.google.com/document/d/17d9YiCmZRYb2eH2ATkMcPBm97IajdkP9ieY8XtzSxyQ/edit#heading=h.zao8k5xrsbng",children:"method"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"The advantages of using Lambda layers is that the overall deployment package will be smaller due to fewer dependencies as well as avoiding Lambdas importing the same packages in isolation. Having a smaller deployment package means that code can be tested and debugged more easily in the AWS Lambda console as it won\u2019t need to be zipped up."}),"\n",(0,a.jsx)(n.h3,{id:"creating-a-lambda-function",children:"Creating a Lambda Function"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"Introduction"})}),"\n",(0,a.jsxs)(n.p,{children:["There is a good example of a simple Lambda function on the AWS website ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html",children:"here"}),". This guide demonstrates how to use the AWS Console to build, deploy and test a simple Lambda function. It is recommended to read this first before trying to build your own Lambda function."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"Developing your own Lambda function"})}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Create and checkout a new git branch, giving it a name that is prefixed with the Jira card ID e.g. ",(0,a.jsx)(n.code,{children:"di-001-my-amazing-api-ingestion-for-housing"}),". This will ensure that any code commits and deployments are linked to the Jira agile workflow."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Ensure that you have the required roles and policies ",(0,a.jsx)(n.a,{href:"#requesting-the-correct-permissions-set",children:"set up"}),". If these are new roles and / or policies then they will likely be set up on a Terraform script by the DAP team. If you are using an existing role then you can create an empty Terraform file in the ",(0,a.jsx)(n.code,{children:"Terraform > ETL"})," folder. Name the file in the same format as existing files by adding a prefix with the next numerical value e.g. ",(0,a.jsx)(n.code,{children:"53-aws-lambda-my-amazing-ingestion.tf"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(8251).A+"",width:"888",height:"1268"})}),"\n",(0,a.jsxs)(n.ol,{start:"3",children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Once you are ready to start developing your Lambda function you will need to navigate to the ",(0,a.jsx)(n.code,{children:"lambdas folder"})," in your ",(0,a.jsx)(n.a,{href:"https://playbook.hackney.gov.uk/Data-Platform-Playbook/playbook/getting-set-up/local-pyspark-environment",children:"local environment"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Create a new folder and give it a name that describes what it is doing e.g. ",(0,a.jsx)(n.code,{children:"my-amazing-api-ingestion-for-housing"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(4404).A+"",width:"734",height:"1338"})}),"\n",(0,a.jsxs)(n.ol,{start:"5",children:["\n",(0,a.jsxs)(n.li,{children:["Create a new empty Python file called ",(0,a.jsx)(n.code,{children:"main.py"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Below is an adapted ",(0,a.jsx)(n.a,{href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/lambdas/govnotify_api_ingestion_repairs/main.py",children:"example"})," from the Data Platform GitHub repository. The script includes some commonly used functions that perform certain actions such as getting a secret from ",(0,a.jsx)(n.em,{children:"AWS Secrets Manager"}),", ",(0,a.jsx)(n.em,{children:"adding date partitions"})," to a file path and ",(0,a.jsx)(n.em,{children:"uploading a JSON string to an S3 bucket"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["Using this as a template, replace the script with your package imports, functions and code. Your main code needs to sit within the ",(0,a.jsx)(n.code,{children:"lambda_handler(event, context)"})," function which is what the Lambda job will look for when it is deployed."]}),"\n",(0,a.jsx)(n.p,{children:"The environment variables will be fully configured within the Terraform script that is created later on. For now, add the environment variables needed."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'from datetime import datetime\nimport json\nimport logging\nfrom os import getenv\n\nimport boto3\nfrom botocore.exceptions import ClientError\nfrom notifications_python_client.notifications import NotificationsAPIClient\n\n# Set up logging to track key stages of the Lambda job\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# custom functions (add any custom functions here)\ndef get_api_secret(api_secret_name, region_name):\n    session = boto3.session.Session()\n    client = session.client(service_name="secretsmanager", region_name=region_name)\n    try:\n        get_secret_value_response = client.get_secret_value(SecretId=api_secret_name)\n    except ClientError as e:\n        raise e\n    return get_secret_value_response["SecretString"]\n\ndef upload_to_s3(s3_bucket_name, s3_client, file_content, file_name):\n    try:\n        s3_client.put_object(Bucket=s3_bucket_name, Key=file_name, Body=file_content)\n        logger.info(f"Uploaded {file_name} to S3")\n    except Exception as e:\n        logger.error(f"Error uploading {file_name} to S3: {str(e)}")\n\ndef prepare_json(response):\n    return json.dumps(response).encode(\'utf-8\')\n\ndef add_date_partition_key_to_s3_prefix(s3_prefix):\n    t = datetime.today()\n    partition_key = f"import_year={t.strftime(\'%Y\')}/import_month={t.strftime(\'%m\')}/import_day={t.strftime(\'%d\')}/import_date={t.strftime(\'%Y%m%d\')}/"\n    return f"{s3_prefix}{partition_key}"\n\n# main lambda_handler function (main script goes here)\ndef lambda_handler(event, context):\n    logger.info(f"Set up S3 client...")\n\n    # setting up clients needed for the task\n    s3_client = boto3.client(\'s3\')\n    glue_client = boto3.client(\'glue\')\n    \n    # environment variables\n    api_secret_name = getenv("API_SECRET_NAME")\n    region_name = getenv("AWS_REGION")\n    output_s3_bucket = getenv("TARGET_S3_BUCKET")\n    output_folder = getenv("TARGET_S3_FOLDER")\n    crawler = getenv("CRAWLER_NAME")\n    \n    # main script\n    api_secret_string = get_api_secret(api_secret_name, region_name)\n    api_secret_json = json.loads(api_secret_string)\n    api_key = api_secret_json.get("api_key_live")\n    client = NotificationsAPIClient(api_key)\n\n    response = client.get_all_notifications(include_jobs=True)\n    output_folder_json = add_date_partition_key_to_s3_prefix(f\'{output_folder}notifications/json/\')\n    json_str = prepare_json(response=response)\n    \n    # upload the json string to S3\n    upload_to_s3(output_s3_bucket, s3_client, json_str, f\'{output_folder_json}notifications.json\')\n    \n    # crawl all the parquet data in S3\n    glue_client.start_crawler(Name=f\'{crawler}\')\n\nif __name__ == "__main__":\n    lambda_handler("event", "lambda_context")\n    \n'})}),"\n",(0,a.jsxs)(n.ol,{start:"6",children:["\n",(0,a.jsx)(n.li,{children:"Adding resources to the Terraform script"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["At the very beginning of your Terraform script (this is the script we created in ",(0,a.jsx)(n.em,{children:"step 2, 53-aws-lambda-my-amazing-ingestion.tf"}),") ensure that the following block has been included. This will ensure that the Lambda is initially only deployed to pre-prod."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"locals {\n amazing_api_ingestion_resource_count = local.is_live_environment && !local.is_production_environment ? 1 : 0\n}\n"})}),"\n",(0,a.jsxs)(n.p,{children:["After the policy blocks within the Terraform script, add a ",(0,a.jsx)(n.a,{href:"https://github.com/LBHackney-IT/Data-Platform/tree/main/terraform/modules/aws-lambda",children:"module block"})," for the main Lambda resource (which has been custom developed on the DAP). as well other resource blocks such as for an ",(0,a.jsx)(n.a,{href:"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudwatch_event_target",children:"EventBridge target"})," or an ",(0,a.jsx)(n.a,{href:"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/glue_crawler",children:"AWS Glue Crawler"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["Within the layers list within the module block, any AWS supplied or custom Lambda layers can be added (see image below). A full example can be seen ",(0,a.jsx)(n.a,{href:"https://github.com/LBHackney-IT/Data-Platform/blob/main/terraform/etl/48-lambda-gov-notify-ingestion.tf",children:"here"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'module "gov-notify-ingestion-housing-repairs" {\n count                          = local.create_govnotify_resource_count\n source                         = "../modules/aws-lambda"\n tags                           = module.tags.values\n lambda_name                    = "govnotify_api_ingestion_repairs"\n lambda_role_arn                = aws_iam_role.housing_gov_notify_ingestion[0].arn\n identifier_prefix              = local.short_identifier_prefix\n handler                        = "main.lambda_handler"\n lambda_artefact_storage_bucket = module.lambda_artefact_storage_data_source.bucket_id\n s3_key                         = "govnotify_api_ingestion_repairs.zip"\n lambda_source_dir              = "../../lambdas/govnotify_api_ingestion_repairs"\n lambda_output_path             = "../../lambdas/govnotify_api_ingestion_repairs.zip"\n runtime                        = "python3.9"\n environment_variables          = {\n\n   API_SECRET_NAME  = "housing/gov-notify_live_api_key"\n   TARGET_S3_BUCKET = module.landing_zone_data_source.bucket_id\n   TARGET_S3_FOLDER = "housing/govnotify/damp_and_mould/"\n   CRAWLER_NAME     = "${local.short_identifier_prefix}GovNotify Housing Repairs Landing Zone"\n }\n layers = [\n   "arn:aws:lambda:eu-west-2:336392948345:layer:AWSSDKPandas-Python39:13",\n   "arn:aws:lambda:eu-west-2:${data.aws_caller_identity.data_platform.account_id}:layer:notifications-python-client-9-0-0-layer:1",\n   "arn:aws:lambda:eu-west-2:${data.aws_caller_identity.data_platform.account_id}:layer:urllib3-1-26-18-layer:1"\n ]\n}\n'})}),"\n",(0,a.jsxs)(n.ol,{start:"7",children:["\n",(0,a.jsx)(n.li,{children:"Deploying and testing"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Stage, commit and push your new files to the repository. Check the results of the standard tests in the GitHub repository."}),"\n",(0,a.jsxs)(n.ol,{start:"8",children:["\n",(0,a.jsx)(n.li,{children:"Final deployment"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Once you are satisfied that your Lambda is working as intended, change the locals block in your existing Terraform script (53-aws-lambda-my-amazing-ingestion.tf) to the following, which will deploy to both pre-prod and production environments."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"locals {\n amazing_api_ingestion_resource_count = local.is_live_environment ? 1 : 0\n}\n"})}),"\n",(0,a.jsx)(n.p,{children:"Ensure that any secrets used are available in the production environment AWS Secrets Manager."}),"\n",(0,a.jsxs)(n.p,{children:["Well done, you have successfully created a Lambda job! ","\ud83c\udf89"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},4404:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/new-lambda-folder-03c0687004a4abea66cf91c295e96594.png"},8251:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/new-terraform-file-507659f1f692e0ec9964c6dad1189087.png"},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);